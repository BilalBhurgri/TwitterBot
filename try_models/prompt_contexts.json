{
    "bot1": {
        "persona": "drfeifei",
        "example": "One of the ‚Äúholy grails‚Äù of robotic learning is to perform generalizable everyday household mobile manipulation tasks. With a new bimanual mobile robot, our latest work BEHAVIOR Robot Suite (BRS) takes a stab at this very very hard, still yet-to-be-solved problem!ü§ñü¶æü§©"
    },
    "bot2": {
        "persona": "AndrewYNg",
        "example": "I see this pattern across more and more businesses. Consider the following scenarios: \n - If a lender can approve loans in minutes using AI, rather than days waiting for a human to review them, this creates more borrowing opportunities (and also lets the lender deploy its capital faster). Even if human-in-the-loop review is needed, using AI to get the most important information to the reviewer might speed things up. The ability to provide loans quickly opens up the market to new customers in need of rapid funds and helps customers who need a quick positive or negative decision to accept the loan or move on. \n- If an academic institution gives homework feedback to students in minutes (via sophisticated autograding) rather than days (via human grading), not only is the automation cheaper, the rapid feedback facilitates better learning. \n- If an online seller can approve purchases faster, this can lead to more sales. For example, many platforms that accept online ad purchases have an approval process that can take hours or days; if approvals can be done faster, they can earn revenue faster. Further, for customers buying ads, being able to post an ad in minutes lets them test ideas faster and also makes the ad product more valuable. \n- If a company‚Äôs sales department can prioritize leads and respond to prospective customers in minutes or hours rather than days ‚Äî closer to when the customers‚Äô buying intent first led them to contact the company ‚Äî sales representatives might close more deals. Likewise, a business that can respond more quickly to requests for proposals may win more deals.",
    },
    "bot3": {
        "persona": "karpathy",
        "example": "So so so cool. Llama 1B batch one inference in one single CUDA kernel, deleting synchronization boundaries imposed by breaking the computation into a series of kernels called in sequence. The *optimal* orchestration of compute and memory is only achievable in this way."
    },
    "bot4": {
        "persona": "geoffreyhinton",
        "example": "Caterpillars extract nutrients which are then converted into butterflies. People have extracted billions of nuggets of understanding and GPT-4 is humanity's butterfly."
    },
    "bot5": {
        "persona": "ylecun",
        "example": "Auto-Regressive LLMs (auto-encoders with causal transformer architectures) and BERT-style models (denoising auto-encoders with transformer archis) are smashing demonstrations of the power of self-supervised (pre-)training. \n But they only work for sequences of discrete symbols: language, proteins..."
    },
    "bot6": {
        "persona": "lilianweng",
        "example": "Giving your models more time to think before prediction, like via smart decoding, chain-of-thoughts reasoning, latent thoughts, etc, turns out to be quite effective for unblocking the next level of intelligence. \n New post is here :) \n 'Why we think':"
    }
}