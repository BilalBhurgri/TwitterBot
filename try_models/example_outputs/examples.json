{
    "good_formal_example": "üßµ New defense drop for LLMs: Adversarial Suffix Filtering (ASF) üö®\n\nJailbreaks via adversarial suffixes‚Äîthose sneaky token strings that hijack model behavior‚Äîhave been a persistent threat, even in black-box scenarios.\n\nASF introduces a lightweight, model-agnostic pipeline to tackle this. It preprocesses inputs, detects, and filters out adversarial suffixes without needing access to the model's internals. Think of it as a sanitizer that preserves the original prompt's intent while stripping malicious additions.\n\nKey takeaways:\n\n- Reduces attack success rates to below 4% across various models.\n- Maintains model performance on benign prompts.\n- Operates effectively in both black-box and white-box settings.\n\nThis approach offers a practical layer of defense, especially crucial as LLMs become more integrated into real-world applications. For those deploying LLMs in the wild, ASF provides a scalable solution to enhance security without compromising functionality.\n\nPaper: https://arxiv.org/pdf/2505.09602\n\n#AI #LLMSecurity #PromptInjection #AdversarialDefense",
    "playful_example": "LLMs still getting dunked on by 1 weird string of tokens? üíÄ Meet ASF: Adversarial Suffix Filtering.\n\nIt's like a spam filter but for jailbreaks. You give it a prompt + sketchy suffix ‚Üí it snips the bad bits, keeps the good stuff. No need to peek inside the model. Zero access. Just vibes (and a clever detection pipeline).\n\nüõ°Ô∏è Cuts attack success rates to <4%\nüß† Keeps model outputs mostly unchanged\nüï∂Ô∏è Works even in black-box settings\n\nPlug-and-play defense that doesn't break the model or the bank. If you're shipping LLMs, you might want this in your stack.\n\nPaper: https://arxiv.org/pdf/2505.09602\n#LLMs #AI #jailbreak #adversarialprompting #MLsecurity"
}