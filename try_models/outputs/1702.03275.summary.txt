This paper introduces Batch Renormalization, a modification of batch normalization that addresses the issues arising from minibatch dependency in batch-normed models. Batch normalization, which normalizes activations using minibatch statistics, leads to differences between training and inference phases. This discrepancy causes problems, particularly with non-i.i.d. minibatches and small sizes, where the model may overfit to the minibatch distribution. The authors propose Batch Renormalization, which ensures that activations during training depend only on individual examples, matching those during inference. This approach avoids the discrepancies by treating the correction parameters as fixed during training, allowing the model to generalize better. Experiments show that Batch Renormalization improves performance on small minibatches and non-i.i.d. cases, achieving comparable results to batchnorm on i.i.d. data. The method is efficient, requiring no additional computational cost, and maintains the benefits of batch normalization while reducing its limitations.