This paper proposes Latent Diffusion Models (LDMs), which improve high-resolution image synthesis by training diffusion models in a latent space derived from an autoencoder, reducing computational demands and enabling efficient generation. Key findings include that LDMs achieve competitive performance on multiple tasks while lowering computational costs, offer better perceptual and semantic compression, and enable scalable, high-fidelity image synthesis. The models are trained using a two-stage approach with perceptual compression and conditional conditioning, demonstrating superior results in image generation and super-resolution tasks compared to pixel-based diffusion models. They also support text-to-image and layout-to-image synthesis, and the authors released pretrained models for broader applicability.