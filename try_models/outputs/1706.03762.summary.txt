The paper introduces the Transformer, a novel sequence transduction model that replaces recurrent layers with self-attention mechanisms, achieving significant speedups and improved performance in machine translation tasks. It demonstrates that self-attention enables efficient parallel processing, reduces computational complexity, and enhances model effectiveness by allowing global dependencies between input and output. The model's success is validated through extensive experiments on translation tasks, showing superior BLEU scores compared to prior methods, with the biggest model reaching a BLEU score of 28.4 on English-German and 41.0 on English-French, trained for 12 hours on 8 P100 GPUs. The Transformer also excels in other tasks like English constituency parsing, highlighting its versatility beyond translation.