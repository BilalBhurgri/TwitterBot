This paper investigates why larger language models (LLMs) exhibit different in-context learning (ICL) behaviors compared to smaller ones. Through theoretical analysis of simplified models, including linear regression and sparse parity classification, the authors show that smaller models focus on essential hidden features, while larger models incorporate more features, including less relevant or noisy ones. This leads to smaller models being more robust to noise and label disturbances during ICL, whereas larger models are more susceptible to distractions, resulting in poorer ICL performance. Empirical experiments on NLP tasks confirm these findings, demonstrating that larger LLMs are more vulnerable to noise and less robust in ICL than smaller models, highlighting the importance of understanding these mechanisms for safer and more effective deployment of LLMs.