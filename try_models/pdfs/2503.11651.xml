<?xml version="1.0" encoding="UTF-8"?><TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VGGT: Visual Geometry Grounded Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-03-14">14 Mar 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Meta AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minghao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Meta AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nikita</forename><surname>Karaev</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Meta AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Meta AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Novotny</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Meta AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VGGT: Visual Geometry Grounded Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-03-14">14 Mar 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">422489C9B59295A0BF684E7CA58AC0DD</idno>
					<idno type="arXiv">arXiv:2503.11651v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-05-15T10:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Frame Attention Global Attention Camera Head DPT Point maps Tracks Cameras Depth maps √ó ùêø times Input Concat</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>‚Ä¶</head><p>Figure <ref type="figure">1</ref>. VGGT is a large feed-forward transformer with minimal 3D-inductive biases trained on a trove of 3D-annotated data. It accepts up to hundreds of images and predicts cameras, point maps, depth maps, and point tracks for all images at once in less than a second, which often outperforms optimization-based alternatives without further processing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We consider the problem of estimating the 3D attributes of a scene, captured in a set of images, utilizing a feedforward neural network. Traditionally, 3D reconstruction has been approached with visual-geometry methods, utilizing iterative optimization techniques like Bundle Adjustment (BA) <ref type="bibr" target="#b44">[45]</ref>. Machine learning has often played an important complementary role, addressing tasks that cannot be solved by geometry alone, such as feature matching and monocular depth prediction. The integration has become increasingly tight, and now state-of-the-art Structure-from-Motion (SfM) methods like VGGSfM <ref type="bibr" target="#b124">[125]</ref> combine machine learning and visual geometry end-to-end via differentiable BA. Even so, visual geometry still plays a major role in 3D reconstruction, which increases complexity and computational cost.</p><p>As networks become ever more powerful, we ask if, finally, 3D tasks can be solved directly by a neural network, eschewing geometry post-processing almost entirely. Recent contributions like DUSt3R <ref type="bibr" target="#b128">[129]</ref> and its evolution MASt3R <ref type="bibr">[62]</ref> have shown promising results in this direction, but these networks can only process two images at once and rely on post-processing to reconstruct more images, fusing pairwise reconstructions.</p><p>In this paper, we take a further step towards removing the need to optimize 3D geometry in post-processing. We do so by introducing Visual Geometry Grounded Transformer (VGGT), a feed-forward neural network that performs 3D reconstruction from one, a few, or even hundreds of input views of a scene. VGGT predicts a full set of 3D attributes, including camera parameters, depth maps, point maps, and 3D point tracks. It does so in a single forward pass, in seconds. Remarkably, it often outperforms optimization-based alternatives even without further processing. This is a substantial departure from DUSt3R, MASt3R, or VGGSfM, which still require costly iterative post-optimization to obtain usable results.</p><p>We also show that it is unnecessary to design a special network for 3D reconstruction. Instead, VGGT is based on a fairly standard large transformer <ref type="bibr" target="#b118">[119]</ref>, with no particular 3D or other inductive biases (except for alternating between frame-wise and global attention), but trained on a large number of publicly available datasets with 3D annotations. VGGT is thus built in the same mold as large models for natural language processing and computer vision, such as GPTs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b147">148]</ref>, CLIP <ref type="bibr" target="#b85">[86]</ref>, DINO <ref type="bibr">[10,</ref><ref type="bibr" target="#b77">78]</ref>, and Stable Diffusion <ref type="bibr" target="#b33">[34]</ref>. These have emerged as versatile backbones that can be fine-tuned to solve new, specific tasks. Similarly, we show that the features computed by VGGT can significantly enhance downstream tasks like point tracking in dynamic videos, and novel view synthesis.</p><p>There are several recent examples of large 3D neural networks, including DepthAnything <ref type="bibr" target="#b141">[142]</ref>, MoGe <ref type="bibr" target="#b127">[128]</ref>, and LRM <ref type="bibr" target="#b48">[49]</ref>. However, these models only focus on a single 3D task, such as monocular depth estimation or novel view synthesis. In contrast, VGGT uses a shared backbone to predict all 3D quantities of interest together. We demonstrate that learning to predict these interrelated 3D attributes enhances overall accuracy despite potential redundancies. At the same time, we show that, during inference, we can derive the point maps from separately predicted depth and camera parameters, obtaining better accuracy compared to directly using the dedicated point map head.</p><p>To summarize, we make the following contributions: (1) We introduce VGGT, a large feed-forward transformer that, given one, a few, or even hundreds of images of a scene, can predict all its key 3D attributes, including camera intrinsics and extrinsics, point maps, depth maps, and 3D point tracks, in seconds. (2) We demonstrate that VGGT's predictions are directly usable, being highly competitive and usually better than those of state-of-the-art methods that use slow post-processing optimization techniques. <ref type="bibr" target="#b2">(3)</ref> We also show that, when further combined with BA post-processing, VGGT achieves state-of-the-art results across the board, even when compared to methods that specialize in a subset of 3D tasks, often improving quality substantially.</p><p>We make our code and models publicly available at <ref type="url" target="https://github.com/facebookresearch/vggt">https://github.com/facebookresearch/vggt</ref>. We believe that this will facilitate further research in this direction and benefit the computer vision community by providing a new foundation for fast, reliable, and versatile 3D reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Structure from Motion is a classic computer vision problem <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b79">80]</ref> that involves estimating camera parameters and reconstructing sparse point clouds from a set of images of a static scene captured from different viewpoints. The traditional SfM pipeline [2, <ref type="bibr" target="#b35">36,</ref><ref type="bibr">70,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b133">134]</ref> consists of multiple stages, including image matching, triangulation, and bundle adjustment. COLMAP <ref type="bibr" target="#b93">[94]</ref> is the most popular framework based on the traditional pipeline. In recent years, deep learning has improved many components of the SfM pipeline, with keypoint detection <ref type="bibr" target="#b20">[21,</ref><ref type="bibr">31,</ref><ref type="bibr" target="#b115">116,</ref><ref type="bibr" target="#b148">149]</ref> and image matching <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr">99]</ref> being two primary areas of focus. Recent methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b108">109,</ref><ref type="bibr" target="#b111">112,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b117">118,</ref><ref type="bibr" target="#b121">122,</ref><ref type="bibr" target="#b124">125,</ref><ref type="bibr" target="#b130">131,</ref><ref type="bibr" target="#b159">160]</ref> explored end-to-end differentiable SfM, where VGGSfM <ref type="bibr" target="#b124">[125]</ref> started to outperform traditional algorithms on challenging phototourism scenarios.</p><p>Multi-view Stereo aims to densely reconstruct the geometry of a scene from multiple overlapping images, typically assuming known camera parameters, which are often estimated with SfM. MVS methods can be divided into three categories: traditional handcrafted <ref type="bibr">[38,</ref><ref type="bibr">39,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b129">130]</ref>, global optimization <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b132">133,</ref><ref type="bibr" target="#b146">147]</ref>, and learning-based methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr">72,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b144">145,</ref><ref type="bibr" target="#b156">157]</ref>. As in SfM, learning-based MVS approaches have recently seen a lot of progress. Here, DUSt3R <ref type="bibr" target="#b128">[129]</ref> and MASt3R [62] directly estimate aligned dense point clouds from a pair of views, similar to MVS but without requiring camera parameters. Some concurrent works <ref type="bibr" target="#b110">[111,</ref><ref type="bibr" target="#b126">127,</ref><ref type="bibr" target="#b140">141,</ref><ref type="bibr" target="#b155">156]</ref> explore replacing DUSt3R's test-time optimization with neural networks, though these attempts achieve only suboptimal or comparable performance to DUSt3R. Instead, VGGT outperforms DUSt3R and MASt3R by a large margin.</p><p>Tracking-Any-Point was first introduced in Particle Video <ref type="bibr" target="#b90">[91]</ref> and revived by PIPs [44] during the deep learning era, aiming to track points of interest across video sequences including dynamic motions. Given a video and some 2D query points, the task is to predict 2D correspondences of these points in all other frames. TAP-Vid <ref type="bibr" target="#b22">[23]</ref> proposed three benchmarks for this task and a simple baseline method later improved in TAPIR <ref type="bibr" target="#b23">[24]</ref>. CoTracker <ref type="bibr" target="#b54">[55,</ref><ref type="bibr">56]</ref> utilized correlations between different points to track through occlusions, while DOT <ref type="bibr" target="#b59">[60]</ref> enabled dense tracking through occlusions. Recently, TAPTR <ref type="bibr" target="#b62">[63]</ref> proposed an end-to-end transformer for this task, and LocoTrack <ref type="bibr" target="#b12">[13]</ref> extended commonly used pointwise features to nearby regions. All of these methods are specialized point trackers.</p><p>Here, we demonstrate that VGGT's features yield state-ofthe-art tracking performance when coupled with existing point trackers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We introduce VGGT, a large transformer that ingests a set of images as input and produces a variety of 3D quantities as output. We start by introducing the problem in Sec. 3.1, followed by our architecture in Sec. 3.2 and its prediction heads in Sec. 3.3, and finally the training setup in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem definition and notation</head><p>The input is a sequence (I i ) N i=1 of N RGB images I i ‚àà R 3√óH√óW , observing the same 3D scene. VGGT's transformer is a function that maps this sequence to a corresponding set of 3D annotations, one per frame:</p><formula xml:id="formula_0">f (I i ) N i=1 = (g i , D i , P i , T i ) N i=1 .<label>(1)</label></formula><p>The transformer thus maps each image I i to its camera parameters g i ‚àà R 9 (intrinsics and extrinsics), its depth map D i ‚àà R H√óW , its point map P i ‚àà R 3√óH√óW , and a grid T i ‚àà R C√óH√óW of C-dimensional features for point tracking. We explain next how these are defined. For the camera parameters g i , we use the parametrization from <ref type="bibr" target="#b124">[125]</ref> and set g = [q, t, f ] which is the concatenation of the rotation quaternion q ‚àà R 4 , the translation vector t ‚àà R 3 , and the field of view f ‚àà R 2 . We assume that the camera's principal point is at the image center, which is common in SfM frameworks <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b124">125]</ref>.</p><p>We denote the domain of the image I i with I(I i ) = {1, . . . , H} √ó {1, . . . , W }, i.e., the set of pixel locations. The depth map D i associates each pixel location y ‚àà I(I i ) with its corresponding depth value D i (y) ‚àà R + , as observed from the i-th camera. Likewise, the point map P i associates each pixel with its corresponding 3D scene point P i (y) ‚àà R 3 . Importantly, like in DUSt3R <ref type="bibr" target="#b128">[129]</ref>, the point maps are viewpoint invariant, meaning that the 3D points P i (y) are defined in the coordinate system of the first camera g 1 , which we take as the world reference frame.</p><p>Finally, for keypoint tracking, we follow track-anypoint methods such as <ref type="bibr">[25,</ref><ref type="bibr" target="#b56">57]</ref>. Namely, given a fixed query image point y q in the query image I q , the network outputs a track T ‚ãÜ (y q ) = (y i ) N i=1 formed by the corresponding 2D points y i ‚àà R 2 in all images I i .</p><p>Note that the transformer f above does not output the tracks directly but instead features T i ‚àà R C√óH√óW , which are used for tracking. The tracking is delegated to a separate module, described in Sec. 3.3, which implements a function T ((y j ) M j=1 ,</p><formula xml:id="formula_1">(T i ) N i=1 ) = ((≈∑ j,i ) N i=1 ) M j=1 .</formula><p>It ingests the query point y q and the dense tracking features T i output by the transformer f and then computes the track. The two networks f and T are trained jointly end-to-end.</p><p>Order of Predictions. The order of the images in the input sequence is arbitrary, except that the first image is chosen as the reference frame. The network architecture is designed to be permutation equivariant for all but the first frame.</p><p>Over-complete Predictions. Notably, not all quantities predicted by VGGT are independent. For example, as shown by DUSt3R <ref type="bibr" target="#b128">[129]</ref>, the camera parameters g can be inferred from the invariant point map P , for instance, by solving the Perspective-n-Point (PnP) problem <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b60">61]</ref>. As shown in the top row, our method successfully predicts the geometric structure of an oil painting, while DUSt3R predicts a slightly distorted plane. In the second row, our method correctly recovers a 3D scene from two images with no overlap, while DUSt3R fails. The third row provides a challenging example with repeated textures, while our prediction is still high-quality. We do not include examples with more than 32 frames, as DUSt3R runs out of memory beyond this limit.</p><p>Furthermore, the depth maps can be deduced from the point map and the camera parameters. However, as we show in Sec. 4.5, tasking VGGT with explicitly predicting all aforementioned quantities during training brings substantial performance gains, even when these are related by closed-form relationships. Meanwhile, during inference, it is observed that combining independently estimated depth maps and camera parameters produces more accurate 3D points compared to directly employing a specialized point map branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Backbone</head><p>Following recent works in 3D deep learning <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b128">129,</ref><ref type="bibr" target="#b131">132]</ref>, we design a simple architecture with minimal 3D inductive biases, letting the model learn from ample quantities of 3D-annotated data. In particular, we implement the model f as a large transformer <ref type="bibr" target="#b118">[119]</ref>. To this end, each input image I is initially patchified into a set of K tokens 1 t I ‚àà R K√óC through DINO <ref type="bibr" target="#b77">[78]</ref>. The combined set of image tokens from all frames, i.e., t I = ‚à™ N i=1 {t I i }, is subsequently processed through the main network structure, alternating frame-wise and global self-attention layers.</p><p>Alternating-Attention. We slightly adjust the standard transformer design by introducing Alternating-Attention 1 The number of tokens depends on the image resolution.</p><p>(AA), making the transformer focus within each frame and globally in an alternate fashion. Specifically, frame-wise self-attention attends to the tokens t I k within each frame separately, and global self-attention attends to the tokens t I across all frames jointly. This strikes a balance between integrating information across different images and normalizing the activations for the tokens within each image. By default, we employ L = 24 layers of global and frame-wise attention. In Sec. 4, we demonstrate that our AA architecture brings significant performance gains. Note that our architecture does not employ any cross-attention layers, only self-attention ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Prediction heads</head><p>Here, we describe how f predicts the camera parameters, depth maps, point maps, and point tracks. First, for each input image I i , we augment the corresponding image tokens t I i with an additional camera token t g i ‚àà R 1√óC ‚Ä≤ and four register tokens <ref type="bibr" target="#b18">[19]</ref> </p><formula xml:id="formula_2">t R i ‚àà R 4√óC ‚Ä≤ . The concatenation of (t I i , t g i , t R i j) N i=1 is then passed to the AA transformer, yield- ing output tokens ( tI i , tg i , tR i ) N i=1 .</formula><p>Here, the camera token and register tokens of the first frame (t g 1 := tg , t R 1 := tR ) are set to a different set of learnable tokens t g , t R than those of all other frames (t which are also learnable. This allows the model to distinguish the first frame from the rest, and to represent the 3D predictions in the coordinate frame of the first camera. Note that the refined camera and register tokens now become frame-specific--this is because our AA transformer contains frame-wise self-attention layers that allow the transformer to match the camera and register tokens with the corresponding tokens from the same image. Following common practice, the output register tokens tR i are discarded while tI i , tg i are used for prediction.</p><formula xml:id="formula_3">g i := t g , t R i := t R , i ‚àà [2, . . . , N ]),</formula><p>Coordinate Frame. As noted above, we predict cameras, point maps, and depth maps in the coordinate frame of the first camera g 1 . As such, the camera extrinsics output for the first camera are set to the identity, i.e., the first rotation quaternion is q 1 = [0, 0, 0, 1] and the first translation vector is t 1 = [0, 0, 0]. Recall that the special camera and register tokens t g 1 := t g , t R 1 := t R allow the transformer to identify the first camera.</p><p>Camera Predictions. The camera parameters (ƒù i ) N i=1 are predicted from the output camera tokens ( tg i ) N i=1 using four additional self-attention layers followed by a linear layer. This forms the camera head that predicts the camera intrin-sics and extrinsics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Predictions. The output image tokens tI</head><p>i are used to predict the dense outputs, i.e., the depth maps D i , point maps P i , and tracking features T i . More specifically, tI i are first converted to dense feature maps F i ‚àà R C ‚Ä≤‚Ä≤ √óH√óW with a DPT layer <ref type="bibr">[87]</ref>. Each F i is then mapped with a 3 √ó 3 convolutional layer to the corresponding depth and point maps D i and P i . Additionally, the DPT head also outputs dense features T i ‚àà R C√óH√óW , which serve as input to the tracking head. We also predict the aleatoric uncertainty <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b75">76]</ref> </p><formula xml:id="formula_4">Œ£ D i ‚àà R H√óW + and Œ£ P i ‚àà R H√óW +</formula><p>for each depth and point map, respectively. As described in Sec. 3.4, the uncertainty maps are used in the loss and, after training, are proportional to the model's confidence in the predictions.</p><p>Tracking. In order to implement the tracking module T , we use the CoTracker2 architecture <ref type="bibr" target="#b56">[57]</ref>, which takes the dense tracking features T i as input. More specifically, given a query point y j in a query image I q (during training, we always set q = 1, but any other image can be potentially used as a query), the tracking head T predicts the set of 2D points</p><formula xml:id="formula_5">T ((y j ) M j=1 , (T i ) N i=1 ) = ((≈∑ j,i ) N i=1</formula><p>) M j=1 in all images I i that correspond to the same 3D point as y. To do so, the feature map T q of the query image is first bilinearly sampled at the query point y j to obtain its feature. This feature is then correlated with all other feature maps T i , i Ã∏ = q to obtain a set of correlation maps. These maps are then processed by self-attention layers to predict the final 2D points ≈∑i , which are all in correspondence with y j . Note that, similar to VG-GSfM <ref type="bibr" target="#b124">[125]</ref>, our tracker does not assume any temporal ordering of the input frames and, hence, can be applied to any set of input images, not just videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>Training Losses. We train the VGGT model f end-to-end using a multi-task loss:</p><formula xml:id="formula_6">L = L camera + L depth + L pmap + ŒªL track .</formula><p>(2)</p><p>We found that the camera (L camera ), depth (L depth ), and point-map (L pmap ) losses have similar ranges and do not need to be weighted against each other. The tracking loss L track is down-weighted with a factor of Œª = 0.05. We describe each loss term in turn.</p><p>The camera loss L camera supervises the cameras ƒù:</p><formula xml:id="formula_7">L camera = N i=1 ‚à•ƒù i -g i ‚à• œµ</formula><p>, comparing the predicted cameras ƒùi with the ground truth g i using the Huber loss | ‚Ä¢ | œµ .</p><p>The depth loss L depth follows DUSt3R <ref type="bibr" target="#b128">[129]</ref> and implements the aleatoric-uncertainty loss <ref type="bibr" target="#b58">[59,</ref><ref type="bibr">75]</ref> weighing the discrepancy between the predicted depth Di and the ground-truth depth D i with the predicted uncertainty map Œ£D i . Differently from DUSt3R, we also apply a gradientbased term, which is widely used in monocular depth estimation. Hence, the depth loss is</p><formula xml:id="formula_8">L depth = N i=1 ‚à•Œ£ D i ‚äô ( Di -D i )‚à• + ‚à•Œ£ D i ‚äô (‚àá Di -‚àáD i )‚à• -Œ± log Œ£ D i ,</formula><p>where ‚äô is the channel-broadcast element-wise product. The point map loss is defined analogously but with the point-map uncertainty</p><formula xml:id="formula_9">Œ£ P i : L pmap = N i=1 ‚à•Œ£ P i ‚äô ( Pi -P i )‚à• + ‚à•Œ£ P i ‚äô (‚àá Pi -‚àáP i )‚à• -Œ± log Œ£ P i .</formula><p>Finally, the tracking loss is given by L track = M j=1 N i=1 ‚à•y j,i -≈∑j,i ‚à•. Here, the outer sum runs over all ground-truth query points y j in the query image I q , y j,i is y j 's ground-truth correspondence in image I i , and ≈∑j,i is the corresponding prediction obtained by the application T ((y j ) M j=1 , (T i ) N i=1 ) of the tracking module. Additionally, following CoTracker2 <ref type="bibr" target="#b56">[57]</ref>, we apply a visibility loss (binary cross-entropy) to estimate whether a point is visible in a given frame.</p><p>Ground Truth Coordinate Normalization. If we scale a scene or change its global reference frame, the images of the scene are not affected at all, meaning that any such variant is a legitimate result of 3D reconstruction. We remove this ambiguity by normalizing the data, thus making a canonical choice and task the transformer to output this particular variant. We follow <ref type="bibr" target="#b128">[129]</ref> and, first, express all quantities in the coordinate frame of the first camera g 1 . Then, we compute the average Euclidean distance of all 3D points in the point map P to the origin and use this scale to normalize the camera translations t, the point map P , and the depth map D. Importantly, unlike <ref type="bibr" target="#b128">[129]</ref>, we do not apply such normalization to the predictions output by the transformer; instead, we force it to learn the normalization we choose from the training data.</p><p>Implementation Details. By default, we employ L = 24 layers of global and frame-wise attention, respectively. The model consists of approximately 1.2 billion parameters in total. We train the model by optimizing the training loss (2) with the AdamW optimizer for 160K iterations. We use a cosine learning rate scheduler with a peak learning rate of 0.0002 and a warmup of 8K iterations. For every batch, we randomly sample 2-24 frames from a random training scene. The input frames, depth maps, and point maps are resized to a maximum dimension of 518 pixels. The aspect ratio is randomized between 0.33 and 1.0. We also randomly apply color jittering, Gaussian blur, and grayscale augmentation to the frames. The training runs on 64 A100 GPUs over nine days. We employ gradient norm clipping with a threshold of 1.0 to ensure training stability. We leverage bfloat16 precision and gradient checkpointing to improve GPU memory and computational efficiency.</p><p>Training Data. The model was trained using a large and diverse collection of datasets, including: Co3Dv2 <ref type="bibr" target="#b87">[88]</ref>, BlendMVS <ref type="bibr" target="#b145">[146]</ref>, DL3DV <ref type="bibr" target="#b68">[69]</ref>, MegaDepth <ref type="bibr" target="#b63">[64]</ref>, Kubric <ref type="bibr" target="#b40">[41]</ref>, WildRGB <ref type="bibr" target="#b134">[135]</ref>, ScanNet <ref type="bibr" target="#b17">[18]</ref>, Hyper-Sim <ref type="bibr" target="#b88">[89]</ref>, Mapillary <ref type="bibr" target="#b70">[71]</ref>, Habitat <ref type="bibr" target="#b106">[107]</ref>, Replica <ref type="bibr" target="#b103">[104]</ref>, MVS-Synth <ref type="bibr" target="#b49">[50]</ref>, PointOdyssey <ref type="bibr" target="#b158">[159]</ref>, Virtual KITTI <ref type="bibr" target="#b6">[7]</ref>, Aria Synthetic Environments <ref type="bibr" target="#b81">[82]</ref>, Aria Digital Twin <ref type="bibr" target="#b81">[82]</ref>, and a synthetic dataset of artist-created assets similar to Objaverse <ref type="bibr" target="#b19">[20]</ref>. These datasets span various domains, including indoor and outdoor environments, and encompass synthetic and real-world scenarios. The 3D annotations for these datasets are derived from multiple sources, such as direct sensor capture, synthetic engines, or SfM techniques <ref type="bibr" target="#b94">[95]</ref>. The combination of our datasets is broadly comparable to those of MASt3R [30] in size and diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section compares our method to state-of-the-art approaches across multiple tasks to show its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Camera Pose Estimation</head><p>We first evaluate our method on the CO3Dv2 <ref type="bibr" target="#b87">[88]</ref> and RealEstate10K <ref type="bibr" target="#b160">[161]</ref> datasets for camera pose estimation, as shown in Tab. 1. Following <ref type="bibr" target="#b123">[124]</ref>, we randomly select 10 images per scene and evaluate them using the standard metric AUC@30, which combines RRA and RTA. RRA (Relative Rotation Accuracy) and RTA (Relative Translation Accuracy) calculate the relative angular errors in rotation and translation, respectively, for each image pair. These angu-  Table <ref type="table">4</ref>. Two-View matching comparison on ScanNet-1500 <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b91">92]</ref>. Although our tracking head is not specialized for the twoview setting, it outperforms the state-of-the-art two-view matching method Roma. Measured in AUC (higher is better).</p><p>ods across all metrics on both datasets, including those that employ computationally expensive post-optimization steps, such as Global Alignment for DUSt3R/MASt3R and Bundle Adjustment for VGGSfM, typically requiring more than 10 seconds. In contrast, VGGT achieves superior performance while only operating in a feed-forward manner, requiring just 0.2 seconds on the same hardware. Compared to concurrent works <ref type="bibr" target="#b110">[111,</ref><ref type="bibr" target="#b126">127,</ref><ref type="bibr" target="#b140">141,</ref><ref type="bibr" target="#b155">156]</ref> (indicated by ‚Ä° ), our method demonstrates significant performance advantages, with speed similar to the fastest variant Fast3R <ref type="bibr" target="#b140">[141]</ref>. Furthermore, our model's performance advantage is even more pronounced on the RealEstate10K dataset, which none of the methods presented in Tab. 1 were trained on. This validates the superior generalization of VGGT.</p><p>Our results also show that VGGT can be improved even further by combining it with optimization methods from visual geometry optimization like BA. Specifically, refining the predicted camera poses and tracks with BA further improves accuracy. Note that our method directly predicts close-to-accurate point/depth maps, which can serve as a good initialization for BA. This eliminates the need for triangulation and iterative refinement in BA as done by <ref type="bibr" target="#b124">[125]</ref>, making our approach significantly faster (only around 2 seconds even with BA). Hence, while the feed-forward mode of VGGT outperforms all previous alternatives (whether they are feed-forward or not), there is still room for improvement since post-optimization still brings benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multi-view Depth Estimation</head><p>Following MASt3R [62], we further evaluate our multiview depth estimation results on the DTU <ref type="bibr" target="#b50">[51]</ref> dataset. We report the standard DTU metrics, including Accuracy (the smallest Euclidean distance from the prediction to ground truth), Completeness (the smallest Euclidean distance from the ground truth to prediction), and their average Overall (i.e., Chamfer distance). In Tab. 2, DUSt3R and our VGGT are the only two methods operating without the knowledge of ground truth cameras. MASt3R derives depth maps by triangulating matches using the ground truth cameras. Net use ground truth cameras to construct cost volumes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meanwhile, deep multi-view stereo methods like GeoMVS-</head><note type="other">Ours CoTracker +Ours</note><p>Our method substantially outperforms DUSt3R, reducing the Overall score from 1.741 to 0.382. More importantly, it achieves results comparable to methods that know ground-truth cameras at test time. The significant performance gains can likely be attributed to our model's multiimage training scheme that teaches it to reason about multiview triangulation natively, instead of relying on ad hoc alignment procedures, such as in DUSt3R, which only averages multiple pairwise camera triangulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Point Map Estimation</head><p>We also compare the accuracy of our predicted point cloud to DUSt3R and MASt3R on the ETH3D <ref type="bibr" target="#b96">[97]</ref> dataset. For each scene, we randomly sample 10 frames. The predicted point cloud is aligned to the ground truth using the Umeyama <ref type="bibr" target="#b116">[117]</ref> algorithm. The results are reported after filtering out invalid points using the official masks. We report Accuracy, Completeness, and Overall (Chamfer distance) for point map estimation. As shown in Tab. 3, although DUSt3R and MASt3R conduct expensive optimization (global alignment--around 10 seconds per scene), our method still outperforms them significantly in a simple feed-forward regime at only 0.2 seconds per reconstruction.</p><p>Meanwhile, compared to directly using our estimated point maps, we found that the predictions from our depth and camera heads (i.e., unprojecting the predicted depth maps to 3D using the predicted camera parameters) yield higher accuracy. We attribute this to the benefits of decomposing a complex task (point map estimation) into simpler subproblems (depth map and camera prediction), even though camera, depth maps, and point maps are jointly supervised during training.</p><p>We present a qualitative comparison with DUSt3R on inthe-wild scenes in Fig. <ref type="figure" target="#fig_1">3</ref> and further examples in Fig. <ref type="figure" target="#fig_2">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Image Matching</head><p>Two-view image matching is a widely-explored topic <ref type="bibr" target="#b7">[68,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b104">105]</ref> in computer vision. It represents a specific case of rigid point tracking, which is restricted to only two views, and hence a suitable evaluation benchmark to measure our tracking accuracy, even though our model is not specialized for this task. We follow the standard protocol <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b92">93]</ref> on the ScanNet dataset <ref type="bibr" target="#b17">[18]</ref> and report the results in Tab. 4.</p><p>For each image pair, we extract the matches and use them to estimate an essential matrix, which is then decomposed to a relative camera pose. The final metric is the relative pose accuracy, measured by AUC. For evaluation, we use ALIKED <ref type="bibr" target="#b157">[158]</ref> to detect keypoints, treating them as query points y q . These are then passed to our tracking branch T to find correspondences in the second frame. We adopt the evaluation hyperparameters (e.g., the number of matches, RANSAC thresholds) from Roma <ref type="bibr" target="#b32">[33]</ref>. Despite not being explicitly trained for two-view matching, Tab. 4 shows that VGGT achieves the highest accuracy among all baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies</head><p>Feature Backbone. We first validate the effectiveness of our proposed Alternating-Attention design by comparing it against two alternative attention architectures: (a) global self-attention only, and (b) cross-attention. To ensure a fair comparison, all model variants maintain an identical number of parameters, using a total of 2L attention layers. For the cross-attention variant, each frame independently attends to tokens from all other frames, maximizing cross-frame information fusion although significantly increasing the runtime, particularly as the number of input frames grows. The hyperparameters such as the hidden dimension and the number of heads are kept the same. Point map estimation accuracy is chosen as the evaluation metric for our ablation study, as it reflects the model's joint understanding of scene geometry and camera parameters. Results in Tab. 5 demonstrate that our Alternating-Attention architecture outperforms both baseline variants by a clear margin. Additionally, our other preliminary exploratory experiments consistently showed that architectures using crossattention generally underperform compared to those exclusively employing self-attention.</p><p>Multi-task Learning. We also verify the benefit of training a single network to simultaneously learn multiple 3D quantities, even though these outputs may potentially overlap (e.g., depth maps and camera parameters together can produce point maps). As shown in Tab. 6, there is a noticeable decrease in the accuracy of point map estimation when training without camera, depth, or track estimation. Notably, incorporating camera parameter estimation clearly enhances point map accuracy, whereas depth estimation contributes only marginal improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Finetuning for Downstream Tasks</head><p>We now show that the VGGT pre-trained feature extractor can be reused in downstream tasks. We show this for feedforward novel view synthesis and dynamic point tracking.</p><p>Feed-forward Novel View Synthesis is progressing rapidly [8, <ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b107">108,</ref><ref type="bibr" target="#b125">126,</ref><ref type="bibr" target="#b139">140,</ref><ref type="bibr" target="#b154">155]</ref>. Most existing methods take images with known camera parameters as input and predict the target image corresponding to a new camera viewpoint. Instead of relying on an explicit 3D representation, we follow LVSM <ref type="bibr" target="#b52">[53]</ref> and modify VGGT to directly output the target image. However, we do not assume known camera parameters for the input frames. We follow the training and evaluation protocol of LVSM closely, e.g., using 4 input views and adopting Pl√ºcker rays to represent target viewpoints. We make a simple modification to VGGT. As before, the input images are converted into tokens by DINO. Then, for the target views, we use a convolutional layer to encode their Pl√ºcker ray images into tokens. These tokens, representing both the input images and the target views, are concatenated and processed by the AA transformer. Subsequently, a DPT head is used to regress the RGB colors for the target views. It is important to note that we do not input the Pl√ºcker rays for the source images. Hence, the model is not given the camera parameters for these input frames. LVSM was trained on the Objaverse dataset <ref type="bibr" target="#b19">[20]</ref>. We use a similar internal dataset of approximately 20% the size of Objaverse. Further details on training and evaluation can be found in <ref type="bibr" target="#b52">[53]</ref>. As shown in Tab. 7, despite not requiring the input camera parameters and using less training data than LVSM, our model achieves competitive results on the GSO dataset <ref type="bibr" target="#b27">[28]</ref>. We expect that better results would be obtained using a larger training dataset. Qualitative examples are shown in Fig. <ref type="figure">6</ref>.</p><p>Dynamic Point Tracking has emerged as a highly competitive task in recent years <ref type="bibr">[25,</ref><ref type="bibr">44,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b135">136]</ref>, and it serves as another downstream application for our learned features. Following standard practices, we report these point-tracking metrics: Occlusion Accuracy (OA), which comprises the binary accuracy of occlusion predictions; Œ¥ vis avg , comprising the Table <ref type="table">8</ref>. Dynamic Point Tracking Results on the TAP-Vid benchmarks. Although our model was not designed for dynamic scenes, simply fine-tuning CoTracker with our pretrained weights significantly enhances performance, demonstrating the robustness and effectiveness of our learned features. mean proportion of visible points accurately tracked within a certain pixel threshold; and Average Jaccard (AJ), measuring tracking and occlusion prediction accuracy together.</p><p>We adapt the state-of-the-art CoTracker2 model <ref type="bibr" target="#b56">[57]</ref> by substituting its backbone with our pretrained feature backbone. This is necessary because VGGT is trained on unordered image collections instead of sequential videos. Our backbone predicts the tracking features T i , which replace the outputs of the feature extractor and later enter the rest of the CoTracker2 architecture, that finally predicts the tracks. We finetune the entire modified tracker on Kubric <ref type="bibr" target="#b40">[41]</ref>. As illustrated in Tab. 8, the integration of pretrained VGGT significantly enhances CoTracker's performance on the TAP-Vid benchmark <ref type="bibr" target="#b22">[23]</ref>. For instance, VGGT's tracking features improve the Œ¥ vis avg metric from 78.9 to 84.0 on the TAP-Vid RGB-S dataset. Despite the TAP-Vid benchmark's inclusion of videos featuring rapid dynamic motions from various data sources, our model's strong performance demonstrates the generalization capability of its features, even in scenarios for which it was not explicitly designed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussions</head><p>Limitations. While our method exhibits strong generalization to diverse in-the-wild scenes, several limitations remain. First, the current model does not support fisheye or panoramic images. Additionally, reconstruction performance drops under conditions involving extreme input rotations. Moreover, although our model handles scenes with minor non-rigid motions, it fails in scenarios involving substantial non-rigid deformation.</p><p>However, an important advantage of our approach is its flexibility and ease of adaptation. Addressing these limitations can be straightforwardly achieved by fine-tuning the model on targeted datasets with minimal architectural modifications. This adaptability clearly distinguishes our method from existing approaches, which typically require extensive re-engineering during test-time optimization to accommodate such specialized scenarios. Runtime and Memory. As shown in Tab. 9, we evaluate inference runtime and peak GPU memory usage of the feature backbone when processing varying numbers of input frames. Measurements are conducted using a single NVIDIA H100 GPU with flash attention v3 <ref type="bibr" target="#b97">[98]</ref>. Images have a resolution of 336 √ó 518.</p><p>We focus on the cost associated with the feature backbone since users may select different branch combinations depending on their specific requirements and available resources. The camera head is lightweight, typically accounting for approximately 5% of the runtime and about 2% of the GPU memory used by the feature backbone. A DPT head uses an average of 0.03 seconds and 0.2 GB GPU memory per frame.</p><p>When GPU memory is sufficient, multiple frames can be processed efficiently in a single forward pass. At the same time, in our model, inter-frame relationships are handled only within the feature backbone, and the DPT heads make independent predictions per frame. Therefore, users constrained by GPU resources may perform predictions frame by frame. We leave this trade-off to the user's discretion.</p><p>We recognize that a naive implementation of global selfattention can be highly memory-intensive with a large number of tokens. Savings or accelerations can be achieved by employing techniques used in large language model (LLM) deployments. For instance, Fast3R <ref type="bibr" target="#b140">[141]</ref> employs Tensor Parallelism to accelerate inference with multiple GPUs, which can be directly applied to our model.</p><p>Patchifying. As discussed in Sec. 3.2, we have explored the method of patchifying images into tokens by utilizing either a 14 √ó 14 convolutional layer or a pretrained DI-NOv2 model. Empirical results indicate that the DINOv2 model provides better performance; moreover, it ensures much more stable training, particularly in the initial stages. The DINOv2 model is also less sensitive to variations in hyperparameters such as learning rate or momentum. Consequently, we have chosen DINOv2 as the default method for patchifying in our model.</p><p>Differentiable BA. We also explored the idea of using differentiable bundle adjustment as in VGGSfM <ref type="bibr" target="#b124">[125]</ref>. In small-scale preliminary experiments, differentiable BA demonstrated promising performance. However, a bottleneck is its computational cost during training. Enabling differentiable BA in PyTorch using Theseus <ref type="bibr" target="#b84">[85]</ref> typically makes each training step roughly 4 times slower, which is expensive for large-scale training. While customizing a framework to expedite training could be a potential solution, it falls outside the scope of this work. Thus, we opted not to include differentiable BA in this work, but we recognize it as a promising direction for large-scale unsupervised training, as it can serve as an effective supervision signal in scenarios lacking explicit 3D annotations.</p><p>Single-view Reconstruction. Unlike systems like DUSt3R and MASt3R that have to duplicate an image to create a pair, our model architecture inherently supports the input of a single image. In this case, global attention simply transitions to frame-wise attention. Although our model was not explicitly trained for single-view reconstruction, it demonstrates surprisingly good results. Some examples can be found in Fig. <ref type="figure" target="#fig_1">3</ref> and Fig. <ref type="figure" target="#fig_5">7</ref>. We strongly encourage trying our demo for better visualization.</p><p>Normalizing Prediction. As discussed in Sec. 3.4, our approach normalizes the ground truth using the average Euclidean distance of the 3D points. While some methods, such as DUSt3R, also apply such normalization to network predictions, our findings suggest that it is neither necessary for convergence nor advantageous for final model performance. Furthermore, it tends to introduce additional instability during the training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We present Visual Geometry Grounded Transformer (VGGT), a feed-forward neural network that can directly estimate all key 3D scene properties for hundreds of input views. It achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multiview depth estimation, dense point cloud reconstruction, and 3D point tracking. Our simple, neural-first approach departs from traditional visual geometry-based methods, which rely on optimization and post-processing to obtain accurate and task-specific results. The simplicity and efficiency of our approach make it well-suited for real-time applications, which is another benefit over optimization-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In the Appendix, we provide the following: ‚Ä¢ formal definitions of key terms in Appendix A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Formal Definitions</head><p>In this section, we provide additional formal definitions that further ground the method section.</p><p>The camera extrinsics are defined in relation to the world reference frame, which we take to be the coordinate system of the first camera. We thus introduce two functions. The first function Œ≥(g, p) = p ‚Ä≤ applies the rigid transformation encoded by g to a point p in the world reference frame to obtain the corresponding point p ‚Ä≤ in the camera reference frame. The second function œÄ(g, p) = y further applies perspective projection, mapping the 3D point p to a 2D image point y. We also denote the depth of the point as observed from the camera g by œÄ D (g, p) = d ‚àà R + .</p><p>We model the scene as a collection of regular surfaces S i ‚äÇ R 3 . We make this a function of the i-th input image as the scene can change over time <ref type="bibr" target="#b150">[151]</ref>. The depth at pixel location y ‚àà I(I i ) is defined as the minimum depth of any 3D point p in the scene that projects to y, i.e., D i (y) = min{œÄ D (g i , p) : p ‚àà S i ‚àß œÄ(g i , p) = y}. The point at pixel location y is then given by P i (y) = Œ≥(g, p), where p ‚àà S i is the 3D point that minimizes the expression above, i.e., p ‚àà S i ‚àß œÄ(g i , p) = y ‚àß œÄ D (g i , p) = D i (y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Architecture. As mentioned in the main paper, VGGT consists of 24 attention blocks, each block equipped with one frame-wise self-attention layer and one global selfattention layer. Following the ViT-L model used in DI-NOv2 <ref type="bibr" target="#b77">[78]</ref>, each attention layer is configured with a feature dimension of 1024 and employs 16 heads. We use the official implementation of the attention layer from PyTorch, i.e., torch.nn.MultiheadAttention, with flash attention enabled. To stabilize training, we also use QKNorm <ref type="bibr" target="#b47">[48]</ref> and LayerScale <ref type="bibr" target="#b114">[115]</ref> for each attention layer. The value of Lay-erScale is initialized with 0.01. For image tokenization, we use DINOv2 <ref type="bibr" target="#b77">[78]</ref> and add positional embedding. As in <ref type="bibr" target="#b142">[143]</ref>, we feed the tokens from the 4-th, 11-th, 17-th, and 23-rd block into DPT [87] for upsampling.</p><p>Training. To form a training batch, we first choose a random training dataset (each dataset has a different yet approximately similar weight, as in <ref type="bibr" target="#b128">[129]</ref>), and from the dataset, we then sample a random scene (uniformly). During the training phase, we select between 2 and 24 frames per scene while maintaining the constant total of 48 frames within each batch. For training, we use the respective training sets of each dataset. We exclude training sequences containing fewer than 24 frames. RGB frames, depth maps, and point maps are first isotropically resized, so the longer size has 518 pixels. Then, we crop the shorter dimension (around the principal point) to a size between 168 and 518 pixels while remaining a multiple of the 14-pixel patch size. It is worth mentioning that we apply aggressive color augmentation independently across each frame within the same scene, enhancing the model's robustness to varying lighting conditions. We build ground truth tracks following <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b124">125]</ref>, which unprojects depth maps to 3D, reprojects points to target frames, and retains correspondences where reprojected depths match target depth maps. Frames with low similarity to the query frame are excluded during batch sampling. In rare cases with no valid correspondences, the tracking loss is omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Experiments</head><p>Camera Pose Estimation on IMC We also evaluate using the Image Matching Challenge (IMC) <ref type="bibr" target="#b53">[54]</ref>, a camera pose estimation benchmark focusing on phototourism data. Until recently, the benchmark was dominated by classical incremental SfM methods <ref type="bibr" target="#b93">[94]</ref>.</p><p>Baselines. We evaluate two flavors of our model: VGGT and VGGT + BA. VGGT directly outputs camera pose estimates, while VGGT + BA refines the estimates using an additional Bundle Adjustment stage. We compare to the classical incremental SfM methods such as <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b93">94]</ref> and to recently-proposed deep methods. Specifically, recently VGGSfM <ref type="bibr" target="#b124">[125]</ref> provided the first end-to-end trained deep method that outperformed incremental SfM on the challenging phototourism datasets.</p><p>Besides VGGSfM, we additionally compare to recently popularized DUSt3R <ref type="bibr" target="#b128">[129]</ref> and MASt3R <ref type="bibr">[62]</ref>. It is important to note that DUSt3R and MASt3R utilized a substantial portion of the MegaDepth dataset for training, only excluding scenes 0015 and 0022. The MegaDepth scenes employed in their training have some overlap with the IMC benchmark, although the images are not identical; the same scenes are present in both datasets. For instance, the MegaDepth scene 0024 corresponds to the British Museum, while the British Museum is also a scene in the IMC benchmark. For an apples-to-apples comparison, we adopt the same training split as DUSt3R and MASt3R. In the main paper, to ensure a fair comparison on ScanNet-1500, we exclude the corresponding ScanNet scenes from our training. Table <ref type="table" target="#tab_0">10</ref>. Camera Pose Estimation on IMC <ref type="bibr" target="#b53">[54]</ref>. Our method achieves state-of-the-art performance on the challenging phototropism data, outperforming VGGSfMv2 <ref type="bibr" target="#b124">[125]</ref> which ranked first on the latest CVPR'24 IMC Challenge in camera pose (rotation and translation) estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>methods, our VGGT's feed-forward performance is on par with the state-of-the-art VGGSfMv2 with AUC@10 of 71.26 versus 76.82, while being significantly faster (0.2 vs. 10 seconds per scene). Remarkably, VGGT outperforms both MASt3R [62] and DUSt3R <ref type="bibr" target="#b128">[129]</ref> significantly across all accuracy thresholds while being much faster. This is because MASt3R's and DUSt3R's feed-forward predictions can only process pairs of frames and, hence, require a costly global alignment step. Additionally, with bundle adjustment, VGGT + BA further improves drastically, achieving state-of-the-art performance on IMC, raising AUC@10 from 71.26 to 84.91, and raising AUC@3 from 39.23 to 66.37. Note that our model directly predicts 3D points, which can serve as the initialization for BA. This eliminates the need for triangulation and iterative refinement of BA as in <ref type="bibr" target="#b124">[125]</ref>. As a result, VGGT + BA is much faster than <ref type="bibr" target="#b124">[125]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Examples</head><p>We further present qualitative examples of single-view reconstruction in Fig. <ref type="figure" target="#fig_5">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Related Work</head><p>In this section, we discuss additional related works.</p><p>Vision Transformers. The Transformer architecture was initially proposed for language processing tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">22,</ref><ref type="bibr" target="#b119">120]</ref>. It was later introduced to the computer vision community by ViT <ref type="bibr" target="#b26">[27]</ref>, sparking widespread adoption. Vision Transformers and their variants have since become dominant in the design of architectures for various computer vision tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">12,</ref><ref type="bibr">83,</ref><ref type="bibr" target="#b136">137]</ref>, thanks to their simplicity, high capacity, flexibility, and ability to capture long-range dependencies.</p><p>DeiT <ref type="bibr" target="#b113">[114]</ref> demonstrated that Vision Transformers can be effectively trained on datasets like ImageNet using strong data augmentation strategies. DINO [10] revealed intriguing properties of features learned by Vision Transformers in a self-supervised manner. CaiT <ref type="bibr" target="#b114">[115]</ref> introduced layer scaling to address the challenges of training deeper Vision Transformers, effectively mitigating gradient-related issues. Further, techniques such as QKNorm <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b149">150]</ref> have been proposed to stabilize the training process. Additionally, <ref type="bibr" target="#b137">[138]</ref> also explores the dynamics between frame-wise and global attention modules in object tracking, though using cross-attention.</p><p>Camera Pose Estimation. Estimating camera poses from multi-view images is a crucial problem in 3D computer vision. Over the last decades, Structure from Motion (SfM) has emerged as the dominant approach <ref type="bibr" target="#b45">[46]</ref>, whether incremental [2, <ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b133">134]</ref> or global <ref type="bibr">[3, 14-17, 52, 73, 79, 81, 90, 106]</ref>. Recently, a set of methods treat camera pose estimation as a regression problem [65, <ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b108">109,</ref><ref type="bibr" target="#b111">112,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b117">118,</ref><ref type="bibr" target="#b121">122,</ref><ref type="bibr" target="#b122">123,</ref><ref type="bibr" target="#b130">131,</ref><ref type="bibr" target="#b151">152,</ref><ref type="bibr" target="#b152">153,</ref><ref type="bibr" target="#b159">160]</ref>, which show promising results under the sparse-view setting. Ace-Zero <ref type="bibr" target="#b4">[5]</ref> further proposes to regress 3D scene coordinates and FlowMap <ref type="bibr" target="#b100">[101]</ref> focuses on depth maps, as intermediates for camera prediction. Instead, VGGSfM <ref type="bibr" target="#b124">[125]</ref> simplifies the classical SfM pipeline to a differentiable framework, demonstrating exceptional performance, particularly with phototourism datasets. At the same time, DUSt3R <ref type="bibr">[62,</ref><ref type="bibr" target="#b128">129]</ref> introduces an approach to learn pixel-aligned point map, and hence camera poses can be recovered by simple alignment. This paradigm shift has garnered considerable interest as the point map, an over-parameterized representation, offers seamless integration with various downstream applications, such as 3D Gaussian splatting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Architecture Overview. Our model first patchifies the input images into tokens by DINO, and appends camera tokens for camera prediction. It then alternates between frame-wise and global self attention layers. A camera head makes the final prediction for camera extrinsics and intrinsics, and a DPT [87] head for any dense output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Qualitative comparison of our predicted 3D points to DUSt3R on in-the-wild images. As shown in the top row, our method successfully predicts the geometric structure of an oil painting, while DUSt3R predicts a slightly distorted plane. In the second row, our method correctly recovers a 3D scene from two images with no overlap, while DUSt3R fails. The third row provides a challenging example with repeated textures, while our prediction is still high-quality. We do not include examples with more than 32 frames, as DUSt3R runs out of memory beyond this limit.</figDesc><graphic coords="4,358.79,259.55,94.51,62.21" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Additional Visualizations of Point Map Estimation. Camera frustums illustrate the estimated camera poses. Explore our interactive demo for better visualization quality.</figDesc><graphic coords="5,373.41,256.30,178.99,115.87" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Visualization of Rigid and Dynamic Point Tracking. Top: VGGT's tracking module T outputs keypoint tracks for an unordered set of input images depicting a static scene. Bottom: We finetune the backbone of VGGT to enhance a dynamic point tracker CoTracker [56], which processes sequential inputs.</figDesc><graphic coords="8,77.36,141.56,118.77,66.06" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head/><label/><figDesc>‚Ä¢ comprehensive implementation details, including architecture and training hyperparameters in Appendix B. ‚Ä¢ additional experiments and discussions in Appendix C. ‚Ä¢ qualitative examples of single-view reconstruction in Appendix D. ‚Ä¢ an expanded review of related works in Appendix E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Single-view Reconstruction by Point Map Estimation. Unlike DUSt3R, which requires duplicating an image into a pair, our model can predict the point map from a single input image. It demonstrates strong generalization to unseen real-world images.</figDesc><graphic coords="13,142.40,143.26,79.62,58.06" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head/><label/><figDesc/><graphic coords="1,129.90,236.45,423.26,177.67" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Camera Pose Estimation on RealEstate10K<ref type="bibr" target="#b160">[161]</ref> and CO3Dv2<ref type="bibr" target="#b87">[88]</ref> with 10 random frames. All metrics the higher the better. None of the methods were trained on the Re10K dataset. Runtime were measured using one H100 GPU. Methods marked with ‚Ä° represent concurrent work.</figDesc><table><row><cell cols="2">Methods</cell><cell cols="2">Re10K (unseen) AUC@30 ‚Üë</cell><cell>CO3Dv2 AUC@30 ‚Üë</cell><cell>Time</cell></row><row><cell cols="2">Colmap+SPSG [92]</cell><cell>45.2</cell><cell/><cell>25.3</cell><cell>‚àº 15s</cell></row><row><cell cols="2">PixSfM [66]</cell><cell>49.4</cell><cell/><cell>30.1</cell><cell>&gt; 20s</cell></row><row><cell cols="2">PoseDiff [124]</cell><cell>48.0</cell><cell/><cell>66.5</cell><cell>‚àº 7s</cell></row><row><cell cols="2">DUSt3R [129]</cell><cell>67.7</cell><cell/><cell>76.7</cell><cell>‚àº 7s</cell></row><row><cell cols="2">MASt3R [62]</cell><cell>76.4</cell><cell/><cell>81.8</cell><cell>‚àº 9s</cell></row><row><cell cols="2">VGGSfM v2 [125]</cell><cell>78.9</cell><cell/><cell>83.4</cell><cell>‚àº 10s</cell></row><row><cell cols="2">MV-DUSt3R [111]  ‚Ä°</cell><cell>71.3</cell><cell/><cell>69.5</cell><cell>‚àº 0.6s</cell></row><row><cell cols="2">CUT3R [127]  ‚Ä°</cell><cell>75.3</cell><cell/><cell>82.8</cell><cell>‚àº 0.6s</cell></row><row><cell cols="2">FLARE [156]  ‚Ä°</cell><cell>78.8</cell><cell/><cell>83.3</cell><cell>‚àº 0.5s</cell></row><row><cell cols="2">Fast3R [141]  ‚Ä°</cell><cell>72.7</cell><cell/><cell>82.5</cell><cell>‚àº 0.2s</cell></row><row><cell cols="2">Ours (Feed-Forward)</cell><cell>85.3</cell><cell/><cell>88.2</cell><cell>‚àº 0.2s</cell></row><row><cell cols="2">Ours (with BA)</cell><cell>93.5</cell><cell/><cell>91.8</cell><cell>‚àº 1.8s</cell></row><row><cell>Known GT camera</cell><cell cols="2">Method</cell><cell cols="2">Acc.‚Üì Comp.‚Üì</cell><cell>Overall‚Üì</cell></row><row><cell>‚úì</cell><cell cols="2">Gipuma [40]</cell><cell>0.283</cell><cell>0.873</cell><cell>0.578</cell></row><row><cell>‚úì</cell><cell cols="2">MVSNet [144]</cell><cell>0.396</cell><cell>0.527</cell><cell>0.462</cell></row><row><cell>‚úì</cell><cell cols="2">CIDER [139]</cell><cell>0.417</cell><cell>0.437</cell><cell>0.427</cell></row><row><cell>‚úì</cell><cell cols="2">PatchmatchNet [121]</cell><cell>0.427</cell><cell>0.377</cell><cell>0.417</cell></row><row><cell>‚úì</cell><cell cols="2">MASt3R [62]</cell><cell>0.403</cell><cell>0.344</cell><cell>0.374</cell></row><row><cell>‚úì</cell><cell cols="2">GeoMVSNet [157]</cell><cell>0.331</cell><cell>0.259</cell><cell>0.295</cell></row><row><cell>‚úó</cell><cell cols="2">DUSt3R [129]</cell><cell>2.677</cell><cell>0.805</cell><cell>1.741</cell></row><row><cell>‚úó</cell><cell/><cell>Ours</cell><cell>0.389</cell><cell>0.374</cell><cell>0.382</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Dense MVS Estimation on the DTU<ref type="bibr" target="#b50">[51]</ref> Dataset. Methods operating with known ground-truth camera are in the top part of the table, while the bottom part contains the methods that do not know the ground-truth camera.</figDesc><table><row><cell>Methods</cell><cell cols="3">Acc.‚Üì Comp.‚Üì Overall‚Üì</cell><cell>Time</cell></row><row><cell>DUSt3R</cell><cell>1.167</cell><cell>0.842</cell><cell>1.005</cell><cell>‚àº 7s</cell></row><row><cell>MASt3R</cell><cell>0.968</cell><cell>0.684</cell><cell>0.826</cell><cell>‚àº 9s</cell></row><row><cell>Ours (Point)</cell><cell>0.901</cell><cell>0.518</cell><cell>0.709</cell><cell>‚àº 0.2s</cell></row><row><cell>Ours (Depth + Cam)</cell><cell>0.873</cell><cell>0.482</cell><cell>0.677</cell><cell>‚àº 0.2s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Point Map Estimation on ETH3D<ref type="bibr" target="#b96">[97]</ref>. DUSt3R and MASt3R use global alignment while ours is feed-forward and, hence, much faster. The row Ours (Point) indicates the results using the point map head directly, while Ours (Depth + Cam) denotes constructing point clouds from the depth map head combined with the camera head. lar errors are then thresholded to determine the accuracy scores. AUC is the area under the accuracy-threshold curve of the minimum values between RRA and RTA across varying thresholds. The (learnable) methods in Tab. 1 have been trained on Co3Dv2 and not on RealEstate10K. Our feedforward model consistently outperforms competing meth-</figDesc><table><row><cell>Method</cell><cell cols="3">AUC@5 ‚Üë AUC@10 ‚Üë AUC@20 ‚Üë</cell></row><row><cell>SuperGlue [92]</cell><cell>16.2</cell><cell>33.8</cell><cell>51.8</cell></row><row><cell>LoFTR [105]</cell><cell>22.1</cell><cell>40.8</cell><cell>57.6</cell></row><row><cell>DKM [32]</cell><cell>29.4</cell><cell>50.7</cell><cell>68.3</cell></row><row><cell>CasMTR [9]</cell><cell>27.1</cell><cell>47.0</cell><cell>64.4</cell></row><row><cell>Roma [33]</cell><cell>31.8</cell><cell>53.4</cell><cell>70.9</cell></row><row><cell>Ours</cell><cell>33.9</cell><cell>55.2</cell><cell>73.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Ablation Study for Transformer Backbone on ETH3D.</figDesc><table><row><cell>.</cell></row></table><note><p><p>We compare our alternating-attention architecture against two variants: one using only global self-attention and another employing cross-attention.</p>well, excelling on challenging out-of-domain examples, such as oil paintings, non-overlapping frames, and scenes with repeating or homogeneous textures like deserts.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Ablation Study for Multi-task Learning, which shows that simultaneous training with camera, depth and track estimation yields the highest accuracy in point map estimation on ETH3D.</figDesc><table><row><cell>‚úó</cell><cell>‚úì</cell><cell>‚úì</cell><cell>1.042</cell><cell>0.627</cell><cell>0.834</cell></row><row><cell>‚úì</cell><cell>‚úó</cell><cell>‚úì</cell><cell>0.920</cell><cell>0.534</cell><cell>0.727</cell></row><row><cell>‚úì</cell><cell>‚úì</cell><cell>‚úó</cell><cell>0.976</cell><cell>0.603</cell><cell>0.790</cell></row><row><cell>‚úì</cell><cell>‚úì</cell><cell>‚úì</cell><cell>0.901</cell><cell>0.518</cell><cell>0.709</cell></row></table><note><p>w. Lcamera w. L depth w. L track Acc.‚Üì Comp.‚Üì Overall‚Üì</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Qualitative Examples of Novel View Synthesis. The top row shows the input images, the middle row displays the ground truth images from target viewpoints, and the bottom row presents our synthesized images. Quantitative comparisons for view synthesis on GSO<ref type="bibr" target="#b27">[28]</ref> dataset. Finetuning VGGT for feed-forward novel view synthesis, it demonstrates competitive performance even without knowing camera extrinsic and intrinsic parameters for the input images. Note that</figDesc><table><row><cell>Input Images</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>Ground Truth</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>Prediction</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>Figure 6. Method</cell><cell>Known Input Cam</cell><cell>Size</cell><cell cols="3">PSNR ‚Üë SSIM ‚Üë LPIPS ‚Üì</cell></row><row><cell>LGM [110]</cell><cell>‚úì</cell><cell>256</cell><cell>21.44</cell><cell>0.832</cell><cell>0.122</cell></row><row><cell>GS-LRM [154]</cell><cell>‚úì</cell><cell>256</cell><cell>29.59</cell><cell>0.944</cell><cell>0.051</cell></row><row><cell>LVSM [53]</cell><cell>‚úì</cell><cell>256</cell><cell>31.71</cell><cell>0.957</cell><cell>0.027</cell></row><row><cell>Ours-NVS  *</cell><cell>‚úó</cell><cell>224</cell><cell>30.41</cell><cell>0.949</cell><cell>0.033</cell></row></table><note><p>* indicates using a small training set (only 20%).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head/><label/><figDesc>LocoTrack<ref type="bibr" target="#b12">[13]</ref> 52.9 66.8 85.3 69.7 83.2 89.5 62.9 75.3 87.2 BootsTAPIR [26] 54.6 68.4 86.5 70.8 83.0 89.9 61.4 73.6 88.7</figDesc><table><row><cell>Method</cell><cell>Kinetics AJ Œ¥ vis avg OA AJ Œ¥ vis RGB-S avg OA AJ Œ¥ vis DAVIS avg OA</cell></row><row><cell>TAPTR [63]</cell><cell>49.0 64.4 85.2 60.8 76.2 87.0 63.0 76.1 91.1</cell></row><row><cell>CoTracker [56]</cell><cell>49.6 64.3 83.3 67.4 78.9 85.2 61.8 76.1 88.3</cell></row><row><cell cols="2">CoTracker + Ours 57.2 69.0 88.9 72.1 84.0 91.6 64.7 77.5 91.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>Runtime and peak GPU memory usage across different numbers of input frames. Runtime is measured in seconds, and GPU memory usage is reported in gigabytes.</figDesc><table><row><cell>Input Frames</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>10</cell><cell>20</cell><cell>50</cell><cell>100</cell><cell>200</cell></row><row><cell>Time (s)</cell><cell>0.04</cell><cell cols="2">0.05 0.07</cell><cell cols="2">0.11 0.14</cell><cell>0.31</cell><cell>1.04</cell><cell>3.12</cell><cell>8.75</cell></row><row><cell>Mem. (GB)</cell><cell>1.88</cell><cell cols="2">2.07 2.45</cell><cell cols="2">3.23 3.63</cell><cell>5.58</cell><cell>11.41</cell><cell cols="2">21.15 40.63</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head/><label/><figDesc>Table 10 contains the results of our evaluation. Although phototourism data is the traditional focus of SfM Method Test-time Opt. AUC@3 ‚Ä¢ AUC@5 ‚Ä¢ AUC@10 ‚Ä¢ Runtime</figDesc><table><row><cell>COLMAP (SIFT+NN) [94]</cell><cell>‚úì</cell><cell>23.58</cell><cell>32.66</cell><cell>44.79</cell><cell>&gt;10s</cell></row><row><cell>PixSfM (SIFT + NN) [66]</cell><cell>‚úì</cell><cell>25.54</cell><cell>34.80</cell><cell>46.73</cell><cell>&gt;20s</cell></row><row><cell>PixSfM (LoFTR) [66]</cell><cell>‚úì</cell><cell>44.06</cell><cell>56.16</cell><cell>69.61</cell><cell>&gt;20s</cell></row><row><cell>PixSfM (SP + SG) [66]</cell><cell>‚úì</cell><cell>45.19</cell><cell>57.22</cell><cell>70.47</cell><cell>&gt;20s</cell></row><row><cell>DFSfM (LoFTR) [47]</cell><cell>‚úì</cell><cell>46.55</cell><cell>58.74</cell><cell>72.19</cell><cell>&gt;10s</cell></row><row><cell>DUSt3R [129]</cell><cell>‚úì</cell><cell>13.46</cell><cell>21.24</cell><cell>35.62</cell><cell>‚àº 7s</cell></row><row><cell>MASt3R [62]</cell><cell>‚úì</cell><cell>30.25</cell><cell>46.79</cell><cell>57.42</cell><cell>‚àº 9s</cell></row><row><cell>VGGSfM [125]</cell><cell>‚úì</cell><cell>45.23</cell><cell>58.89</cell><cell>73.92</cell><cell>‚àº 6s</cell></row><row><cell>VGGSfMv2 [125]</cell><cell>‚úì</cell><cell>59.32</cell><cell>67.78</cell><cell>76.82</cell><cell>‚àº 10s</cell></row><row><cell>VGGT (ours)</cell><cell>‚úó</cell><cell>39.23</cell><cell>52.74</cell><cell>71.26</cell><cell>0.2s</cell></row><row><cell>VGGT + BA (ours)</cell><cell>‚úì</cell><cell>66.37</cell><cell>75.16</cell><cell>84.91</cell><cell>1.8s</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lama</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyamal</forename><surname>Anadkat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Building rome in a day</title>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Global motion estimation from point matches</title>
		<author>
			<persName><forename type="first">Mica</forename><surname>Arie-Nachimson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ira</forename><surname>Kovalsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second international conference on 3D imaging, modeling, processing, visualization &amp; transmission</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Luƒçiƒá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scene coordinate reconstruction: Posing of image collections via incremental learning of a relocalizer</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Wynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Cavallari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">√Åron</forename><surname>Monszpart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniyar</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><surname>Tom B Brown</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<idno>. 12</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Yohann</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naila</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Humenberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.10773</idno>
		<title level="m">Virtual kitti 2</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lightplane: Highly-scalable components for neural 3Dfields</title>
		<author>
			<persName><forename type="first">Ang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision (3DV)</title>
		<meeting>the International Conference on 3D Vision (3DV)</meeting>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving transformer-based image matching by cascaded capturing spatially informative keypoints</title>
		<author>
			<persName><forename type="first">Chenjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="12129" to="12139"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv√©</forename><surname>J√©gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to match features with seeded graph matching network</title>
		<author>
			<persName><forename type="first">Hongkai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiew-Lan</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6301" to="6310"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Masked-attention mask transformer for universal image segmentation</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Local all-pair correspondence for point tracking</title>
		<author>
			<persName><forename type="first">Seokju</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jisu</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honggyu</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sfm with mrfs: Discrete-continuous optimization for large-scale structure from motion</title>
		<author>
			<persName><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hsfm: Hybrid structure-from-motion</title>
		<author>
			<persName><forename type="first">Hainan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanyi</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1212" to="1221"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Global structure-from-motion by similarity averaging</title>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="864" to="872"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Linear global translation estimation with feature tracks</title>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianjuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01832</idno>
		<idno>. 13</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><surname>Nie√üner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Timoth√©e</forename><surname>Darcet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16588</idno>
		<idno>. 4</idno>
		<title level="m">Vision transformers need registers</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Objaverse: A universe of annotated 3d objects</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Deitke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Weihs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Vanderbilt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiana</forename><surname>Ehsani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Superpoint: Self-supervised interest point detection and description</title>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Daniel Detone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="224" to="236"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Tap-vid: A benchmark for tracking any point in a video</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larisa</forename><surname>Markeeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adri√†</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo√£o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">TAPIR: Tracking any point with per-frame initialization and temporal refinement</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mel</forename><surname>Vecerik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilara</forename><surname>Gokay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv, 2306.08637</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">TAPIR: tracking any point with per-frame initialization and temporal refinement</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mel</forename><surname>Vecerik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilara</forename><surname>Gokay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilara</forename><surname>Gokay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skanda</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Heyward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo√£o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00847</idno>
		<idno>. 10</idno>
		<title level="m">Bootstap: Bootstrapped training for tracking-any-point</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An image is worth 16√ó16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Google scanned objects: A high-quality dataset of 3d scanned household items</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Downs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Kinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Hickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krista</forename><surname>Reymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Mchugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The Llama 3 herd of models</title>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiesha</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aobo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archi</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archie</forename><surname>Sravankumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Hinsvark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aur√©lien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austen</forename><surname>Gregerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ava</forename><surname>Spataru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozi√®re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bethany</forename><surname>Biron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binh</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobbie</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Caucheteux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaya</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Marra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mcconnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Touret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corinne</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian Canton</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyrus</forename><surname>Nikolaidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Allonsius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Pintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Livshits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Garcia-Olano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Perino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieuwke</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Egor</forename><surname>Lakomkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehab</forename><surname>Albadawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elina</forename><surname>Lobanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabrielle</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Lewis Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graeme</forename><surname>Nail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gr√©goire</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guan</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Korevaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iliyan</forename><surname>Zarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arrieta</forename><surname>Imanol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><forename type="middle">M</forename><surname>Ibarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewon</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename><surname>Geffert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Vranes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeet</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelmer</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Van Der Linde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Billock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenya</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiecao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongsoo</forename><surname>Spisak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Rocca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Johnstun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junteng</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikeya</forename><surname>Vasuden Alwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Upasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Plawiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName><surname>Stone</surname></persName>
		</author>
		<idno>arXiv, 2407.21783</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">MASt3R-SfM: a fully-integrated solution for unconstrained structure-from-motion</title>
		<author>
			<persName><forename type="first">Bardienus</forename><surname>Duisterhof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lojze</forename><surname>Zust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yohann</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<idno>arXiv, 2409.19152</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">D2net: A trainable cnn for joint description and detection of local features</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Dusmanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee/cvf conference on computer vision and pattern recognition</title>
		<meeting>the ieee/cvf conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8092" to="8101"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DKM: Dense kernelized feature matching for geometry estimation</title>
		<author>
			<persName><forename type="first">Johan</forename><surname>Edstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Athanasiadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M√•rten</forename><surname>Wadenb√§ck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Roma: Robust dense feature matching</title>
		<author>
			<persName><forename type="first">Johan</forename><surname>Edstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>B√∂kman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M√•rten</forename><surname>Wadenb√§ck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj√∂rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Building rome on a cloudless day</title>
		<author>
			<persName><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Fite-Georgel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Raguram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changchang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Hung</forename><surname>Jen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrique</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Clipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010: 11th European Conference on Computer Vision</title>
		<title level="s">Proceedings</title>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">September 5-11, 2010. 2010</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Geo-neus: Geometry-consistent neural implicit surfaces learning for multi-view reconstruction</title>
		<author>
			<persName><forename type="first">Qiancheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingshan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yew</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Soon</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3403" to="3416"/>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-view stereo: A tutorial</title>
		<author>
			<persName><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Hern√°ndez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends¬Æ in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="148"/>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Massively parallel multiview stereopsis by surface normal diffusion</title>
		<author>
			<persName><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="873" to="881"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Massively parallel multiview stereopsis by surface normal diffusion</title>
		<author>
			<persName><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Kubric: a scalable dataset generator</title>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Belletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Duckworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gnanapragasam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Golemo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Issam</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">(</forename><surname>Hsueh-Ti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename><surname>Derek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishu</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cengiz</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noha</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Rebain</surname></persName>
		</author>
		<author>
			<persName><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matan</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhani</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwang</forename><forename type="middle">Moo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangcheng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><surname>Tagliasacchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cascade cost volume for highresolution multi-view stereo and stereo matching</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feitong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2495" to="2504"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Junlin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippos</forename><surname>Kokkinos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.00890</idno>
		<title level="m">Flex3d: Feed-forward 3d generation with flexible reconstruction model and input view curation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Particle video revisited: Tracking through occlusions using point trajectories</title>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Detector-free structure from motion</title>
		<author>
			<persName><forename type="first">Xingyi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arxiv</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Query-key normalization for transformers</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><surname>Prudhvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Dachapally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Pawar</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04245</idno>
		<idno>. 11</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">LRM: Large reconstruction model for single image to 3D</title>
		<author>
			<persName><forename type="first">Yicong</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Difan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deepmvs: Learning multi-view stereopsis</title>
		<author>
			<persName><forename type="first">Po-Han</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Large scale multi-view stereopsis evaluation</title>
		<author>
			<persName><forename type="first">Rasmus</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Engil</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Aanaes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="406" to="413"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A global linear method for camera pose registration</title>
		<author>
			<persName><forename type="first">Nianjuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">LVSM: a large view synthesis model with minimal 3D inductive bias</title>
		<author>
			<persName><forename type="first">Haian</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fujun</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<idno>arXiv, 2410.17242</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Image matching across wide baselines: From paper to practice</title>
		<author>
			<persName><forename type="first">Yuhe</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasiia</forename><surname>Mishchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwang</forename><forename type="middle">Moo</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Karaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iurii</forename><surname>Makarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.11831</idno>
		<title level="m">Cotracker3: Simpler and better point tracking by pseudolabelling real videos</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Cotracker: It is better to track together</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Karaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Co-Tracker: It is better to track together</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Karaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Modelling uncertainty in deep learning for camera relocalization</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICRA. IEEE</title>
		<meeting>ICRA. IEEE</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in Bayesian deep learning for computer vision?</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. NeurIPS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dense optical tracking: Connecting the dots</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moing</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Ep n p: An accurate o (n) solution to the p n p problem</title>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="155" to="166"/>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yohann</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J√©r√¥me</forename><surname>Revaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.09756</idno>
		<title level="m">Grounding image matching in 3d with mast3r</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Taptr: Tracking any point with transformers as detection</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.13042</idno>
		<idno>. 2</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Megadepth: Learning single-view depth prediction from internet photos</title>
		<author>
			<persName><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2041" to="2050"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Amy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.04926</idno>
		<idno>. 13</idno>
		<title level="m">Relpose++: Recovering 6d poses from sparseview observations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Pixel-perfect structure-from-motion with featuremetric refinement</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Lindenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno>arXiv.cs, abs/2108.08291</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Lightglue: Local feature matching at light speed</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Lindenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.13643</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">LightGlue: local feature matching at light speed</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Lindenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Dl3dv-10k: A large-scale scene dataset for deep learning-based 3d vision</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yawen</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="22160" to="22169"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Robust incremental structure-from-motion with hybrid features</title>
		<author>
			<persName><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R√©mi</forename><surname>Pautrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>Sch√∂nberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Mapillary planet-scale depth dataset</title>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Lopez-Antequera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pau</forename><surname>Gargallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Hofinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">√É¬≤</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yubin</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Multiview stereo with cascaded epipolar raft</title>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Global fusion of relative motions for robust, accurate and scalable structure from motion</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Moulon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Monasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renaud</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3504" to="3515"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Learning 3D object categories by looking around them</title>
		<author>
			<persName><forename type="first">David</forename><surname>Novotn√Ω</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Capturing the geometry of object categories from video supervision</title>
		<author>
			<persName><forename type="first">David</forename><surname>Novotn√Ω</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A critique of structure-from-motion algorithms</title>
		<author>
			<persName><forename type="first">John</forename><surname>Oliensis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="172" to="214"/>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">DINOv2: Learning robust visual features without supervision</title>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth√©e</forename><surname>Darcet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Th√©o</forename><surname>Moutakanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasil</forename><surname>Szafraniec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Khalidov</surname></persName>
		</author>
		<author>
			<persName><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haziza</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mido</forename><surname>Assran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Galuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Howes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Robust camera location estimation by convex programming</title>
		<author>
			<persName><forename type="first">Onur</forename><surname>Ozyesil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A survey of structure from motion*</title>
		<author>
			<persName><forename type="first">Vladislav</forename><surname>Onur √ñzyes ¬∏il</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Voroninski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="305" to="364"/>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Global Structure-from-Motion Revisited</title>
		<author>
			<persName><forename type="first">Linfei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Barath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><forename type="middle">Lutz</forename><surname>Sch√∂nberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Aria digital twin: A new benchmark dataset for egocentric 3d machine perception</title>
		<author>
			<persName><forename type="first">Xiaqing</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Charron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng (</forename><surname>Carl) Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="20133" to="20143"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision</title>
		<meeting>the IEEE/CVF In-ternational Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Rethinking depth estimation for multiview stereo: A unified representation</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yawen</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8645" to="8654"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Theseus: A library for differentiable nonlinear optimization</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taosha</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>Monge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shobha</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paloma</forename><surname>Sodhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky Tq</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName><forename type="first">Ren√©</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Common Objects in 3D: Large-scale learning and evaluation of real-life 3D category reconstruction</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Shapovalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Henzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Sbordone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atulit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><forename type="middle">Angel</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Paczan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV) 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Linear multiview reconstruction of points, lines, planes and cameras using a reference plane</title>
		<author>
			<persName><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Ninth IEEE International Conference on Computer Vision</title>
		<meeting>Ninth IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Particle video: Long-range motion estimation using point trajectories</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Teller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Superglue: Learning feature matching with graph neural networks</title>
		<author>
			<persName><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">SuperGlue: learning feature matching with graph neural networks</title>
		<author>
			<persName><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sch√∂nberger</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sch√∂nberger</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multi-view stereo</title>
		<author>
			<persName><forename type="first">Enliang</forename><surname>Johannes L Sch√∂nberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Michael</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2016: 14th European Conference</title>
		<title level="s">Proceedings, Part III</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">October 11-14, 2016. 2016</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A multi-view stereo benchmark with highresolution images and multi-camera videos</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Schops</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Flashattention-3: Fast and accurate attention with asynchrony and low-precision</title>
		<author>
			<persName><forename type="first">Jay</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Bikshandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Clustergnn: Cluster-based coarseto-fine graph neural network for efficient feature matching</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Xiong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoli</forename><surname>Shavit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tai-Jiang</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wensen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12517" to="12526"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Sparsepose: Sparseview camera pose regression and refinement</title>
		<author>
			<persName><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gilitschenski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Lindell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Charatan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.15259</idno>
		<idno>. 13</idno>
		<title level="m">Flowmap: High-quality camera poses, intrinsics, and depth via gradient descent</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">FlowMap: high-quality camera poses, intrinsics, and depth via gradient descent</title>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Charatan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<idno>. 2</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Photo tourism: exploring photo collections in 3d</title>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM siggraph 2006 papers</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<author>
			<persName><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingni</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><forename type="middle">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shobhit</forename><surname>Verma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05797</idno>
		<idno>. 6</idno>
		<title level="m">The replica dataset: A digital replica of indoor spaces</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Loftr: Detector-free local feature matching with transformers</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Optimizing the viewing graph for structure-from-motion</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Hollerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Habitat 2.0: Training home assistants to rearrange their habitat</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Szot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Clegg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Undersander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yili</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Maestre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Mukadam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Maksymets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vondrus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Dharur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Galuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Splatter image: Ultra-fast single-view 3d reconstruction</title>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Szymanowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chrisitian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="10208" to="10217"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<author>
			<persName><forename type="first">Chengzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04807</idno>
		<title level="m">Ba-net: Dense bundle adjustment network</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Lgm: Large multiview gaussian model for high-resolution 3d content creation</title>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoxi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Mv-dust3r+: Single-stage scene reconstruction from sparse views in 2 seconds</title>
		<author>
			<persName><forename type="first">Zhenggang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.06974</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Deepv2d: Video to depth with differentiable structure from motion</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04605</idno>
		<idno>. 2</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv√©</forename><surname>J√©gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv√©</forename><surname>J√©gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Disk: Learning local features with policy gradient</title>
		<author>
			<persName><forename type="first">Micha≈Ç</forename><surname>Tyszkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="14254" to="14265"/>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Least-squares estimation of transformation parameters between two point patterns</title>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Umeyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Demon: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">≈Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Patchmatchnet: Learned multi-view patchmatch stereo</title>
		<author>
			<persName><forename type="first">Fangjinhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Speciale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Deep two-view structure-from-motion revisited</title>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Smolyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Posediffusion: Solving pose estimation via diffusion-aided bundle adjustment</title>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">PoseDiffusion: solving pose estimation via diffusion-aided bundle adjustment</title>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">VGGSfM: visual geometry grounded deep structure from motion</title>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Karaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2024">2024. 7, 10, 12</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">PF-LRM: pose-free large reconstruction model for joint pose and shape prediction</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fujun</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<idno>. 9</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Continuous 3d perception model with persistent state</title>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2025">2025</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">MoGe: unlocking accurate monocular geometry estimation for opendomain images with optimal training supervision</title>
		<author>
			<persName><forename type="first">Ruicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassie</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<idno>arXiv, 2410.19115</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">DUSt3R: Geometric 3D vision made easy</title>
		<author>
			<persName><forename type="first">Shuzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yohann</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Chidlovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2024">2024. 6, 7, 11, 12</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Adaptive patch deformation for textureless-resilient multi-view stereo</title>
		<author>
			<persName><forename type="first">Yuesong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaojie</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenkai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luoyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1621" to="1630"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Deepsfm: Structure from motion via deep bundle adjustment</title>
		<author>
			<persName><forename type="first">Xingkui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">MeshLRM: large reconstruction model for highquality mesh</title>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fujun</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Deschaintre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<idno>arXiv, 2404.12385</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Nerfingmvs: Guided optimization of neural radiance fields for indoor multi-view stereo</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5610" to="5619"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Towards linear-time incremental structure from motion</title>
		<author>
			<persName><forename type="first">Changchang</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 International Conference on 3D Vision-3DV 2013</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Rgbd objects in the wild: Scaling real-world 3d object learning from rgb-d videos</title>
		<author>
			<persName><forename type="first">Hongchi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Spatialtracker: Tracking any 2d pixels in 3d space</title>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangzhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="20406" to="20417"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Correlation-aware deep tracking</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wankou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Learning inverse depth regression for multi-view stereo with correlation cost volume</title>
		<author>
			<persName><forename type="first">Qingshan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">GRM: Large gaussian reconstruction model for efficient 3D reconstruction and generation</title>
		<author>
			<persName><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zifan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Yifan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
		<idno>. 9</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<author>
			<persName><forename type="first">Jianing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joyce</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.13928</idno>
		<title level="m">Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass</title>
		<imprint>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Depth anything: Unleashing the power of large-scale unlabeled data</title>
		<author>
			<persName><forename type="first">Lihe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main">Depth anything v2</title>
		<author>
			<persName><forename type="first">Lihe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.09414</idno>
		<idno>. 11</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Mvsnet: Depth inference for unstructured multiview stereo</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Mvsnet: Depth inference for unstructured multiview stereo</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="767" to="783"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Blendedmvs: A large-scale dataset for generalized multi-view stereo networks</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1790" to="1799"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Multiview neural surface reconstruction by disentangling geometry and appearance</title>
		<author>
			<persName><forename type="first">Lior</forename><surname>Yariv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoni</forename><surname>Kasten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dror</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meirav</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matan</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2492" to="2502"/>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title level="m" type="main">Generative pre-trained transformer: A comprehensive review on enabling technologies, potential applications, emerging challenges, and future directions</title>
		<author>
			<persName><forename type="first">Gokul</forename><surname>Yenduri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chemmalar</forename><surname>Selvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Supriya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Kumar Reddy Maddikunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepti</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Rutvij</forename><forename type="middle">H</forename><surname>Jhaveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prabadevi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Athanasios</forename><forename type="middle">V</forename><surname>Vasilakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thippa</forename><forename type="middle">Reddy</forename><surname>Gadekallu</surname></persName>
		</author>
		<idno>. 2</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">LIFT: Learned Invariant Feature Transform</title>
		<author>
			<persName><forename type="first">Kwang</forename><surname>Moo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Stabilizing transformer training by preventing attention entropy collapse</title>
		<author>
			<persName><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etai</forename><surname>Littwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Busbridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title level="m" type="main">MonST3R: a simple approach for estimating geometry in the presence of motion</title>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno>. 11</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Relpose: Predicting probabilistic relative rotation for single objects in the wild</title>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Cameras as rays: Pose estimation via ray diffusion</title>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moneish</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tzu-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Gs-lrm: Large reconstruction model for 3d gaussian splatting</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbo</forename><surname>Xiangli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanxuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<title level="m" type="main">GS-LRM: large reconstruction model for 3D Gaussian splatting</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbo</forename><surname>Xiangli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanxuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<idno>. 9</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">Flare: Feed-forward geometry, appearance and camera estimation from uncalibrated sparse views</title>
		<author>
			<persName><forename type="first">Shangzhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2025">2025</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Geomvsnet: Learning multi-view stereo with geometry perception</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Aliked: A lighter keypoint and descriptor extraction network via deformable transformation</title>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingsong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengguo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Instrumentation and Measurement</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Pointodyssey: A large-scale synthetic dataset for long-term point tracking</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bokui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and egomotion from video</title>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<monogr>
		<title level="m" type="main">Stereo magnification: Learning view synthesis using multiplane images</title>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09817</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>