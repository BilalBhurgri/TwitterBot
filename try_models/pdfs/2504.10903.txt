# Efficient Reasoning Models: A Survey

## 1 Introduction

Recent reasoning-oriented models, or Large Reasoning Models (LRMs) , have achieved remarkable performance on complex reasoning tasks by generating long Chain-of-Thoughts (CoTs), enabling effective problem-solving in domains such as mathematics and coding . However, while LRMs significantly improve performance on reasoning tasks, they also cause substantial overhead. Compared to standard LLMs, reasoning models lead to redundancy across multiple dimensions.

A salient characteristic of reasoning models is their tendency to overthink by generating excessively long reasoning chains , which has naturally motivated efforts to improve efficiency by shortening reasoning paths. Meanwhile, recent studies challenge the assumption that longer CoTs always lead to better performance, showing even negative returns. To address this kind of CoT length redundancy, a range of methods have been proposed: reinforcement learning (RL) with length penalties , supervised fine-tuning (SFT) on variable-length CoT data , and promptdriven strategies that either guide reasoning paths or route inputs to more efficient solutions . Furthermore, latent reasoning performs the process in latent space without generating explicit CoTs, making reasoning chains more concise .

Figure : Overview of efficient reasoning. We categorize existing efficient reasoning methods into three key directions based on how they improve reasoning efficiency: (1) make long CoT short (shorter); ( ) build small language model with strong reasoning ability (smaller); and (3) let decoding more efficient (faster).

In addition to excessively long reasoning chains, reasoning models typically rely on large model sizes to achieve strong reasoning performance (e.g., DeepSeek R1 ) has 685B parameters), which leads to substantial computational and memory costs. To address this, model compression has proven effective in reducing model size redundancy in standard LLMs, naturally inspiring interest in how these techniques (e.g., distillation , quantization , and pruning ) can be applied to improve reasoning efficiency. In parallel, another line of works directly builds small language models with strong reasoning abilities using RL .

Beyond length and model size redundancy, inefficiency can also arise during the decoding stage. To tackle this issue, a growing body of work focuses on accelerating inference through more efficient decoding strategies. Test-time scaling (TTS) strategies, while enhancing reasoning performance , also introduce latency redundancy during the decoding stage. Some methods specifically target and optimize the speed of certain TTS strategies . Other approaches, like parallel decoding and problem decomposition , also mitigate inefficiency.

This survey aims to provide an overview of research in efficient reasoning. As illustrated in Figure , we categorize existing works into three key directions based on the type of redundancy they target: (1) making long CoT short (shorter), which focuses on enabling models to produce shorter reasoning paths while maintaining performance; (2) building small language model with strong reasoning abilities (smaller), which aims to develop compact language models with strong reasoning abilities; (3) making decoding more efficient (faster), which explores strategies to reduce latency during the decoding stage.

The following sections of this survey cover the content as outlined below. Section 2 will explore key backgrounds closely related to efficient reasoning. Section 3 will systematically introduce various methods and their relationships across three categories: making long CoT short (see Section 3.1), building small language model with strong reasoning abilities (see Section 3.2), and letting decoding more efficient (see Section 3.3). Section 4 presents the evaluation metrics, as well as datasets and benchmarks. Section 5 will discuss the key challenges in the field and propose some potential future research directions, while Section 6 will conclude the survey. Additionally, Figure illustrates the taxonomy of efficient reasoning methods discussed in this survey.

( §3.3.2)

AoT , DISC , Meta-Reasoner , AR , MRT , TTC-Optimal Scaling , RSD , SpecReason Figure : Taxonomy of efficient reasoning.

## 2 Background

## 2.1 Chain-of-Thought Reasoning

CoT serves as a baseline reasoning approach, enabling LLMs to generate a sequence of intermediate steps before reaching the final answer, thus significantly improving performance on complex reasoning tasks. Various extensions have subsequently been proposed to further enhance reasoning capabilities. For instance, Tree-of-Thought (ToT) generalizes the linear CoT structure into a tree, facilitating the exploration of multiple reasoning paths through backtracking and lookahead strategies. Graph-of-Thoughts (GoT) has expanded this approach into graph structures to better capture dependencies and compositional relationships among reasoning steps, substantially improving reasoning quality. Additionally, some specialized CoT variants are task-specific. PoT disentangles reasoning from computation by having the language model generate programmatic reasoning steps (i.e., expressing thoughts as code), which an external calculator executes to obtain the final answer, making this approach particularly effective for math and financial reasoning tasks. CoS , on the other hand, targets spatial reasoning by leveraging compressed symbolic representations of spatial relations to reduce token usage.

Scaling test-time computation (TTC) is another road for enhancing reasoning performance. For instance, Best-of-N selects the top-scoring answer from multiple samples, whereas self-consistency chooses the most consistent answer across multiple reasoning chains. Initially, the focus of TTS strategies was primarily on maximizing performance, often at the cost of efficiency. As the demand grew, more recent approaches tried to find ways to improve efficiency without significantly sacrificing performance. Fast Best-of-N ., When More is Less Safety Risks High efficiency Alleviate resource constraints Lower costs ... For CoT length, longer is not always better Lengthy CoT Attacker Why We Need Efficient Reasoning Accuracy ops/step 2024a) introduces speculative rejection during sampling to proactively discard low-quality reasoning paths.

SoT employs a two-stage decoding strategy by generating a skeleton and filling nodes in parallel. Additionally, an empirical study investigates the trade-offs between the efficiency and performance of various TTS strategies (e.g., Best-of-N, weighted voting) under different model sizes and computation budgets, providing practical insights for further research and deployment.

## 2.2 Why We Need Efficient Reasoning

Efficiency is a valuable research direction across many fields, and in the context of reasoning, we highlight key motivations for pursuing efficient reasoning (see Figure ). Reasoning models often generate excessively long reasoning chains to solve reasoning tasks, even for simple samples (see Appendix A.1 for a concrete example), and typically rely on larger model sizes to achieve stronger reasoning performance. Additionally, some strategies, such as Best-of-N and self-consistency , further scale the decoding process to enhance reasoning performance. These lead to substantial computational and memory demands. Moreover, overly long reasoning paths can accumulate errors and negatively impact final accuracy .

On the other hand, efficient reasoning is also essential in real-world applications such as embodied AI , agent systems , and real-time platforms (e.g., autonomous driving ). In these scenarios, efficiency enables agents to process sensory inputs in real time, make swift and accurate decisions, and interact seamlessly with dynamic environments. Additionally, unnecessarily lengthy reasoning may increase safety risks , posing unpredictable threats. These challenges collectively highlight the limitations of current reasoning models, underscoring the necessity of improving reasoning efficiency.

## 3 Efficient Reasoning

## 3.1 Make Long CoT Short

CoT prompting has significantly improved the reasoning capabilities of LLMs by explicitly generating intermediate reasoning steps. However, generating overly long CoTs can lead to negative issues (see Section 2.2).

Recent works have explored various approaches to mitigate these drawbacks by shortening CoT length without compromising reasoning performance. Among them, RL with length penalty is widely used for encouraging concise and effective reasoning paths (see Section 3.1.1). Another line of work explores SFT with variable-length CoT data to improve reasoning efficiency, as discussed in Section 3.1.2. In addition, prompt-driven techniques improve reasoning efficiency by utilizing prompts, with further details available Table : Overview of efficient reasoning methods in Section 3.1. The speedup ratio is computed by comparing either the latency (L.) or the token count (T.). Avg 1 represents the average of Llama-3.2-3B, Gemma2-2B, Qwen2.5-3B, Qwen2.5-Math-1.5B, and DeepSeekMath-7B; Avg 2 represents the average of GPT-4o, GPT-4o-mini, Yi-lightning, o3-mini, and LLaMA3.1-8B-I.

Type Methods Training Scheme Acc. / #Tokens Base Model Speedup RL O1-Pruner PP GSM8K: 96.50% / 543 QwQ-32B 1.5 -2.0 (L.) RL DAST SimP MATH-500: 92.60% / 2802 DeepSeek-R1-Distill-Qwen-7B 1.6 -2.2 (T.) RL AGPO GRP MATH-500: 77.20% / 463 Qwen2.5-Math-7B 1.3 -1.5 (T.) RL THINKPRUNE GRP MATH-500: 83.90% / 2209 DeepSeek-R1-Distill-Qwen-1.5B 1.7 -2.0 (T.) RL Think When You Need GRP --1.3 (T.) SFT TokenSkip SF GSM8K: 78.20% / 113 LLaMA3.1-8B-I 1.7 -1.8 (L.) SFT C3oT SF GSM8K: 47.10% / -LLaMA2-Chat-13B 2.0 (T.) SFT Self-Training SF GSM8K: 78.07% / 176 Avg 1 1.3 -1.5 (T.) SFT TALE SFT / DP GSM8K: 78.57% / 140 Avg 2 1.7 (T.) SFT CoT-Valve Progressive SF GSM8K: 95.40% / 289 QwQ-32B 2.6 (T.) Prompting Concise CoT Training-free --1.9 -2.0 (T.) Prompting Break the Chain Training-free GSM8K: 74.22% / -ChatGPT -Prompting TALE-EP Training-free GSM8K: 84.46% / 77 GPT-4o-mini 4.1 (T.) Prompting CoD Training-free GSM8K: 91.10% / 44 GPT-4o 4.7 (T.) Routing RouteLLM LLaMA3-8B Router GSM8K: 74.82% / -GPT-4 1.5 (T.) Routing Sketch-of-Thought DistillBERT Router --3.6 (T.) Routing Self-REF SF GSM8K: 81.60% / -LLaMA3-8B-I 1.2 -2.0 (L.) Latent reasoning Implicit-KD SF GSM8K: 20.00% / -GPT-2 small 8.2 (L.) Latent reasoning SI Progressive SF GSM8K: 30.00% / -GPT-2 small 4.0 -11.0 (L.) Latent reasoning CCoT SF GSM8K: 17.90% / -CCOT &amp; DECODE 10.4 -24.5 (L.) Latent reasoning SoftCoT SF GSM8K: 85.81% / -Qwen2.5-7B-I 4.0 -5.0 (L.) Latent reasoning CODI Self-distillation (LoRA) GSM8K: 43.70% / -GPT-2 small 2.5 -2.7 (L.) Latent reasoning LightThinker SF GSM8K: 90.14% / -Qwen2.5-7B up to 1.4 (L.) Latent reasoning Coconut Progressive SF GSM8K: 34.10% / 8 GPT-2 3.0 (T.) Latent reasoning Token Assorted SF GSM8K: 84.10% / 194 LLaMA3.1-8B 1.2 (T.)

in Section 3.1.3. Finally, we explore latent reasoning, which performs the reasoning process in latent space and drastically reduces CoT length, with details provided in Section 3.1.4. Additionally, Table provides an overview of these methods. Furthermore, we present the performance of various efficient reasoning methods on the challenging AIME dataset in Table .

## 3.1.1 Reinforcement Learning Helps Efficiency Improvement

Incorporating explicit chain length penalties into RL is a natural strategy for shortening reasoning chains . L1 takes this further by introducing designated length-constraint instructions into the training data. O1-Pruner develops a specialized reward design by utilizing length and accuracy from a reference model as baselines, explicitly rewarding shorter reasoning paths and higher accuracy to ensure efficiency without sacrificing performance. DAST aims to achieve a balanced Co. Specifically, it proposes a Token Length Budget (TLB), defined as a weighted sum of the mean token count in accurate answers and a predefined upper bound on generation length to quantify problem difficulty, penalizing excessively verbose reasoning for simple questions while encouraging comprehensive reasoning for complex ones. THINKPRUNE designs a length-aware reward function that only provides a reward if the correct answer is generated within a specified token budget. The model is trained using the Group Relative Policy Optimization (GRPO) algorithm with progressively tightened length constraints. Additionally, Think When You Need utilizes pairwise comparisons to generate rewards based on the relative length and accuracy of reasoning, guiding models to produce concise yet accurate solutions.

## 3.1.2 Supervised Fine-Tuning with Variable-Length CoT Data Helps Efficiency Improvement

Fine-tuning LLMs using variable-length CoT data effectively teaches models to solve reasoning tasks with shorter paths. Following a clear fine-tuning pipeline, we organize the discussion of this line of research into two stages: (1) how variable-length CoT data is constructed and (2) which SFT approach (i.e., standard or progressive) is adopted. For each work, we explicitly address these two questions to facilitate comparison and analysis.

How variable-length CoT data is constructed? To construct variable-length CoT data, long reasoning chains are commonly generated by prompting LLMs with inputs, whereas the key challenge lies in obtaining the corresponding shorter reasoning chains. To address this, existing approaches generally fall into two categories. The first approach involves compressing existing long reasoning paths into shorter ones. For instance, TokenSkip identifies and skips less important tokens based on their semantic contribution to the final answer. Distill2-to-1 discards reasoning steps entirely, retaining only high-quality (input, answer) pairs through consistency-based filtering. C3oT leverages GPT-4 as a compressor to shorten chain length by preserving essential reasoning details. Additionally, SPIRIT uses perplexity to evaluate step importance, thus selectively compressing reasoning paths.

The alternative approach directly generates short reasoning paths. Self-training employs multiple sampling combined with few-shot prompting, selecting the shortest correct reasoning paths. TALE observes that LLMs naturally follow token budget constraints specified in prompts and introduces a binary search-based algorithm to identify the optimal token budget for generating concise reasoning paths. TOPS begins with a small set of o1-like responses (i.e., either generated by existing models or manually constructed) as seed data. Each response corresponds to a different level of reasoning effort. Using this data, it trains a tag model that learns to produce variable-length reasoning paths conditioned on effort-specific prompts, enabling the construction of diverse CoT data with controllable lengths. Inspired by model merging , CoT-Valve achieves chain length control by adjusting a specific direction of the parameter space, merging parameters from a base LLM with those of a reasoning-enhanced model of identical architecture . Additionally, LLM-Skip manually shortens reasoning paths for complex datasets at the initial training stage, explicitly labeling prompts with "Solve it in n steps.". In the subsequent progressive SFT process, shorter reasoning paths generated by the model are continuously integrated into the training set.

Which SFT approach is adopted? Most works adopt a standard SFT approach , typically leveraging either LoRA or full fine-tuning . Notably, C3oT designs a conditioned training strategy, enabling the model to learn both long and short reasoning styles during training and generate concise reasoning paths at inference by simply appending a short condition in the prompt. TALE further explores DPO as an alternative fine-tuning objective, allowing direct control over the model's output preference.

Another line of works adopts progressive fine-tuning strategies . LLM-Skip CoT-Valve+P, on the other hand, progressively trains the model on samples sorted from long to short chains, guiding it to shorten the chain length over successive fine-tuning stages.

## 3.1.3 Prompt-Driven Efficiency Enhancement in Reasoning

Prompt-driven techniques have emerged as a flexible approach to improving reasoning efficiency. We categorize related works into two directions: (1) prompt-guided reasoning, which leverages well-designed prompt to guide reasoning models toward more effective reasoning paths and (2) prompt-based routing, which utilizes prompt-level attributes (e.g., complexity) to adaptively select appropriate computational paths (i.e., route easy questions to lightweight models and hard ones to powerful large models).

Prompt-guided Efficient Reasoning. This line of work focuses on enhancing reasoning efficiency through prompt engineering, encouraging the model to produce concise reasoning paths while preserving performance. Concise CoT shows that simply adding "Be concise" to the prompt can shorten reasoning chains. Break the Chain leverages carefully crafted instructions (e.g., "rapidly evaluate and use the most effective reasoning shortcut") to trigger the model's ability to exploit shortcuts and skip unnecessary steps. TALE-EP employs an LLM-based estimator to predict the minimal token budget required for each question, which is then incorporated into the prompt to guide efficient reasoning. CoD develops the instruction "Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most.", which significantly reduces token usage under few-shot settings without compromising accuracy. However, its performance degrades in zero-shot settings and on small language models. MARP ) boosts per-step information density and reduces step count under a fixed reasoning boundary, achieving high efficiency gains through prompt design, and can be further combined with PoT for better computation-reasoning separation. Token-Complexity presents token complexity to measure the minimal tokens needed for correct reasoning and derives the theoretical compression limit of CoT chains. Through prompt variations (e.g., "use 10 words or less" or "remove all punctuation"), they explore the trade-off between performance and efficiency and show that current methods still fall far from the optimal bound, leaving room for improvement. Additionally, these methods can effectively construct variable-length CoT data, thereby supporting the approaches introduced in Section 3.1.2.

Prompt Attribute-Aware Efficient Reasoning. This line of work improves reasoning efficiency by leveraging prompt attributes (e.g., question complexity) to design routing strategies that dynamically adapt inference models. Claude 3.7 Sonnet offers two response modes (e.g., quick answers or step-by-step thinking), allocating more compute to complex reasoning tasks. Although the implementation details remain undisclosed, it is the first hybrid reasoning model and a foundation for subsequent methods.

Routing strategies primarily fall into two categories: classifier-based and uncertainty-based. Classifier-based approaches train a separate router to categorize incoming questions and route them to the most suitable model. RouteLLM trains a router using preference data to dispatch easy questions to lightweight and harder ones to stronger models. routes each input to the most appropriate reasoning pattern by referencing cognitive science , introducing three heuristic modes: Conceptual Chaining, which links ideas using minimal language; Chunked Symbolism, which organizes reasoning into symbolic blocks; and Expert Lexicons, which leverage domain-specific shorthand.

Uncertainty-based methods rely on confidence to guide routing. Self-REF adds two special tokens (i.e., &lt;CN&gt; for confident and &lt;UN&gt; for unconfident) to indicate confidence, training the model on annotated responses to self-assess its confidence level. If uncertain, the model defers to a more potent model or abstains. Confident or Seek Stronger further analyzes uncertaintybased routing, observing that uncertainty distributions are relatively stable across tasks but vary significantly across models and uncertainty quantification (UQ) methods. It further designs a calibrated data construction strategy that improves the reliability of routing decisions for small language models.

## 3.1.4 Reasoning in Latent Space

Unlike explicit CoT reasoning, latent reasoning performs the reasoning process entirely in the model's latent space, skipping the generation of explicit intermediate steps. Latent reasoning brings two key benefits: it allows for more human-like thinking by modeling complex ideas beyond language and improves efficiency by reducing the need for explicit reasoning chains. This section first examines how models transition from explicit to implicit reasoning. Then, we explore how reasoning is represented in latent space (e.g., discrete tokens or continuous tokens).

From Explicit CoT to Implicit CoT. As the seminal work introducing implicit CoT, Implicit-KD proposed a distillation-based framework where a student model learns to reason implicitly by mimicking the hidden states across different layers of an explicit CoT teacher. To eliminate the reliance on the teacher model during inference, they further trained a simulator that directly maps input to teacher hidden states. SI progressively removes intermediate reasoning steps through SFT, enabling the model to internalize reasoning without explicit chains. Similarly, Distill2-to-1 showed that SFT on (input, answer) pairs alone can yield strong implicit reasoning capabilities. CODI ) introduces a novel self-distillation framework where a shared model acts both as teacher and student-explicit CoT is learned via language modeling, while implicit CoT is learned by aligning the hidden activation of the token intermediately preceding the answer. LightThinker proposes a dynamic compression strategy for CoT. It segments the reasoning chain and compresses each step into special tokens, with a focus on the KV cache compression. These latent representations are used for subsequent reasoning, with attention masks designed to ensure the model can only access compressed content rather than whole previous steps. Another line of work explores using an auxiliary model to generate latent reasoning tokens directly from the input. CCo trains a lightweight CCOT module (a LoR) to produce compressed latent reasoning tokens directly from input, which are then fed into a decoding module to generate concise answers, while HCo adopts a similar pipeline but places greater emphasis on semantic alignment during compression. SoftCo adopts a similar strategy by training a lightweight assistant model to produce implicit representations conditioned on the input. Furthermore, Reasoning with Latent Thoughts (Saunshi et al., 2025) demonstrated that looping a transformer multiple times could emulate a deeper model and naturally induce latent thoughts, effectively capturing iterative reasoning without tokenized steps. RELAY follows this idea by aligning each iteration of a looped transformer with explicit CoT steps. The trained looped model is then leveraged to produce high-quality CoT chains to train stronger autoregressive models on long reasoning tasks.

Latent Space Representations for Reasoning. A common choice for latent space representation is to use continuous tokens , which naturally align with the internal computation of neural networks.

Coconut models reasoning in the hidden space by feeding the final-layer hidden states back into the model without decoding explicit CoT tokens, enabling more continuous and efficient reasoning. This approach unlocks advantages that explicit CoT cannot offer, such as backtracking and parallel decoding. Inspired by Coconut, Heima introduces thinking tokens into multimodal large language models (MLLMs) to replace explicit reasoning steps, enabling reasoning in latent space.

Another alternative approach is to employ discrete tokens as explicit representations of intermediate reasoning stages. Planning-Token ) employs a set of planning tokens inserted before each reasoning step to guide the model to generate a latent plan before producing the detailed explanation. These tokens are obtained by clustering the hidden states of reasoning steps, yielding semantically meaningful and distinct discrete representations. Filler-Token proposes inserting meaningless filler tokens (e.g., repeated dots) into the reasoning path, allowing the model to perform additional hidden computation, thereby enhancing performance on reasoning tasks. Token Assorted improves reasoning efficiency by mixing text tokens with latent tokens obtained through VQ-VAE , reducing sequence length while preserving key information. Disentangling-Memory-and-Reasoning introduces explicit discrete markers such as 〈memory〉 and 〈reason〉, which enable the model to disentangle reasoning into separate phases (i.e., retrieving relevant knowledge and performing logical inference) within the latent space. This separation facilitates more structured and interpretable reasoning behaviors.

## 3.2 Build Small Language Model with Strong Reasoning Ability

An alternative approach to improving reasoning efficiency is to empower small language models (SLMs) with strong reasoning capabilities. Due to their lower memory and computational requirements, SLMs are inherently more efficient and easier to deploy in real-world applications. Model compression , Apart from model compression and RL, some studies explore the reasoning ability of small language models from alternative perspectives. For example, shows that small language models can match or even surpass the reasoning performance of much larger LLMs with carefully designed TTS strategies. However, these TTS strategies are not universally applicable, and the optimal approach depends heavily on factors such as the model architecture, reward model design, and problem complexity. While small language models have shown impressive reasoning capabilities, they often face challenges in instruction following and self-reflection, indicating that further adaptation or fine-tuning may still be required to better align the model's behavior with human intent.

## 3.2.1 Distillation Transfers Reasoning Ability to Small Language Model

CoT-KD first demonstrated that distillation can transfer reasoning ability from LLMs to small language models. However, due to limited capacity, small language models struggle to learn complex reasoning , motivating the development of more advanced strategies. Based on the optimization target, existing methods can be grouped into two directions: (1) data-focused, which improves the quality or composition of training data and (2) model-focused, which concentrates on the distilled model itself or its generation strategy.

Data-focused. MD adopts mix distillation by combining data generated with different prompting strategies (CoT and PoT) as training data, and Mix ) applies a similar strategy using a mix of long and short CoT samples. CD enhances training diversity by mixing original data with counterfactual samples derived from it, while NAT leverages negative data. DLCoT improves training data quality by segmenting and simplifying long reasoning paths. SCORE enables self-correction by allowing the model to generate, identify, and refine its reasoning, using the corrected outputs for further distillation. Distill2-to-1 only retrains (input, answer) pairs as training data. The above methods rely on standard SFT, but some adopt progressive SFT. FDD progressively adjusts data difficulty based on the small language model's performance on LLM-generated data, while SKIntern proposes a progressive process that removes symbolic knowledge and examples step by step, encouraging the model to internalize reasoning ability.

Model-focused. PRR distills two separate models: a probing model for retrieving relevant knowledge and a reasoning model for generating answers based on the question and retrieved content. Thinking slow, fast explores distilling reasoning ability from transformer-based models into Mamba or Mamba-Transformer architectures to reduce inference cost to lower costs. Additionally, ATM designs an adaptive mechanism that enables the student model to dynamically choose between pre-thinking (i.e., thinking before answering) and post-thinking (i.e., answering before thinking) based on question complexity.

## 3.2.2 Pruning or Quantization Retain Reasoning Ability

Recent work systematically explores the impact of compression techniques like pruning and quantization on the reasoning capabilities of small language models, which shows that while quantization methods have minimal impact on reasoning performance, pruning approaches significantly degrade reasoning abilities. Similarly, When Reasoning Meets Compression presents a comprehensive benchmark of compressed LRMs across various reasoning tasks. It also finds that quantized models retain strong reasoning performance and sometimes even surpass the original model, while aggressive pruning causes performance collapse at moderate sparsity. Furthermore, Quantization Hurts Reasoning? systematically evaluates the impact of quantization on reasoning models. It finds that high-bit (e.g., 8-bit) quantization is nearly lossless, while low-bit settings (e.g., 4-bit) significantly degrade performance, especially on complex tasks. Interestingly, the output length of CoT reasoning remains largely unchanged, except under aggressive quantization or when using small models.

## 3.2.3 Reinforcement Learning Helps Building Small Language Model

SLM-Foresee ) conducted a systematic study on the reasoning abilities of diverse small language models, demonstrating that small language models can exhibit strong reasoning potential. Certain models, such as the Qwen2.5 series , even achieve performance comparable to or surpassing some LLMs. Open-RS enhanced the reasoning capability of small language models using RL with the GRPO algorithm and curated a high-quality mathematical reasoning dataset derived from the s1 dataset and DeepScaleR dataset . They further develop a cosine reward to control response length effectively. Their 1.5B model, trained on 7K samples within 24 hours on 4A40 GPUs, achieved performance on benchmarks (e.g., AIME2024, MATH-500) that matches or surpasses models like o1-preview (AI., 2024). SimpleRL-Zoo systematically evaluated the generality of ZeroR. The study proposed several key design strategies for successful ZeroRL training: using simple correctness-based rewards, aligning data difficulty with model capacity, and employing stable RL algorithms like GRPO. Remarkably, verification behavior was observed for the first time in small language models outside the Qwen2.5 series , further validating the reasoning potential of small language models. Additionally, DeepScale leverages iterative scaling of GRPO to extend thinking length (i.e., 8K 16K 24K), significantly improving performance on math reasoning benchmarks. The 1.5B model, DeepScaleR-1.5B-Preview, surpasses O1-Preview and achieves 43.1% Pass@1 on AIME.

## 3.3 Let Decoding More Efficient

In the previous sections, we discussed two main directions for improving reasoning efficiency: shortening the reasoning chain and building smaller reasoning models. However, this section covers strategies to accelerate reasoning during inference, focusing on decoding-stage methods that enhance generation speed without compromising reasoning performance. It begins with techniques to reduce computational overhead during TT, followed by an overview of other methods for making reasoning faster, with details provided in Section 3.3.2. These methods are summarized in Table .

## 3.3.1 Efficiency for Test-Time Scaling Strategy

While TTS strategies have shown great promise in improving reasoning performance without modifying model weights, they often cost significant computational overhead. To make TTS more efficient, we categorize this series of works into two directions: (1) efficient sampling methods that optimize the generation process in sampling-based TTS strategies and (2) efficient self-consistency techniques that reduce the cost of consistency-based reasoning.

Efficient Sampling During the sampling process, the quality of generated reasoning chains often varies, and low-quality outputs lead to substantial redundant computation. A key challenge lies in how to allocate computation more effectively. A natural solution is to terminate low-quality outputs early. Fast Bestof-N proposes speculative rejection, which halts underperforming candidates based on early-stage partial rewards. ST-BoN adopts early consistency checks to identify and retain high-potential candidates while truncating the rest. Early path evaluation can also be applied to reasoning data synthesis. FastMCTS leverages MCTS to build reasoning paths while evaluating quality at each step, allowing for dynamic path adjustment. Another line of work explores predicting the future trajectory to reduce redundancy and improve overall quality. Inspired by Model Predictive Control , proposes Predictive-Decoding, which mitigates the myopic nature of token-level generation in CoT by simulating several future reasoning steps (i.e., foresight trajectories) to reweight the token distribution. Similarly, trains a value model from the language model's step-by-step generation dynamics to estimate the utility of intermediate reasoning states and decide whether to proceed. ϕ-Decoding ) takes a step further by simulating multiple future paths at each step, clustering them to form a representative distribution and sampling the next step from this estimate.

Beyond token-level sampling, recent efforts have focused on structured sampling strategies within multipath reasoning frameworks such as ToT and SoT. DPTS proposes a Dynamic Parallel Tree Search framework that parallelizes reasoning path generation and dynamically manages cache states, enabling flexible path switching without deep exploration. It also incorporates early path evaluation to prioritize promising branches. Similarly, FETCH improves efficiency by merging semantically similar reasoning states to avoid redundant exploration and applying Temporal Difference (TD) learning with lambda-return to stabilize verifier scores, reducing unnecessary switching. SGD builds a graph over sub-questions to capture their dependencies in a SoT-based framework and introduces difficulty-aware strategies to enable more efficient and higher-quality parallel decoding.

Efficient Self-Consistency Self-consistency also relies on repeated sampling, which leads to substantial computational overhead. Its core challenge aligns with efficient sampling-how to allocate computation adaptively. ASC estimates answer confidence during sampling and stops early once sufficient confidence is observed, while ESC divides the sampling process into sequential windows and stops sampling as soon as one window yields unanimous answers. DSC further incorporates difficulty awareness to better adjust the sample budget per instance. RASC develops a similar early-stopping mechanism, terminating once sufficient high-quality samples are collected, followed by a score-weighted vote to determine the final answer. RPC combines self-consistency with perplexity-based estimation to accelerate convergence (i.e., the rate at which confidence estimation error for the final answer decreases with more samples). It also applies reasoning pruning to eliminate low-probability reasoning paths, reducing redundant computation. CISC augments each sampled response with a model-predicted confidence score and performs confidence-weighted voting to improve final accuracy under the same sampling budget. Following the same idea, Self-Calibration distills consistency signals from self-consistency into the model itself, enabling it to predict confidence scores during inference. This confidence is then used to guide earlystopping policies. Lastly, Path-Consistency extracts high-confidence reasoning prefixes from early samples and reuses them to guide future sampling, improving generation speed and answer quality.

## 3.3.2 Other Methods for Making Reasoning Faster

Several other strategies aimed at improving reasoning efficiency also exist, such as introducing decoding stage tricks. One common approach is to decompose the original problem into sub-problems, reducing redundant token generation and skipping uninformative reasoning paths. AoT constructs a DAG to model the dependencies among initially decomposed sub-problems. It then solves the overall task by iteratively decomposing and merging sub-problems. At each step, the model only processes a simplified version of the problem, reducing unnecessary token usage, minimizing attention overhead, and avoiding memory issues caused by long contexts. DISC dynamically partitions the problem into sub-steps and applies reward-based dynamic sampling and early stopping for each step to control compute costs, achieving efficient inference. AR decomposes the reasoning process into atomic reasoning actions organized into an atomic tree and performs structured reasoning via cognitive routing (e.g., reflection, backtracking, and termination). This atomic reasoning paradigm has also proven effective in multimodal large language models (MLLMs) .

In real-world applications, we expect LLMs to adjust their output length based on input complexity automatically. For complex problems, the model should allocate more compute to generate a thorough reasoning chain, while for simpler ones, a concise and correct response is preferred. Several methods have been proposed to achieve this. TTC-Optimal Scaling proposes a test-time compute-optimal scaling strategy that first estimates the difficulty of a prompt (i.e., either via oracle or model-predicted difficulty) and then adaptively selects different TTS strategies. For instance, on easy questions where the initial response is likely close to correct, self-verification is more efficient than multiple sampling; for complex problems, tree search with a verifier helps explore diverse reasoning paths. MRT further improves efficiency by introducing dense rewards based on reasoning progress (i.e., rewarding steps that increase the likelihood of reaching a correct answer) and training LLMs to progress toward solutions and avoid unnecessary computation. RSD enhances efficiency by combining a smaller draft model with a larger target model guided by a reward function. The draft model generates candidate steps, and if the reward is high, the output is accepted; otherwise, the target model refines it. Inspired by meta-cognition , Meta-Reasoner acts as a strategic advisor to guide the reasoning process, evaluate reasoning progress, and provide high-level guidance (e.g., backtracking, restarting) based on task complexity. Additionally, SpecReason leverages the semantic tolerance in reasoning processes by using a lightweight model to speculate intermediate steps while reserving the large model for verification and correction.

## 4 Evaluation and Benchmark

## 4.1 Metrics

Assessing reasoning efficiency requires diverse metrics reflecting computational costs and model performance (e.g., accuracy). These metrics provide insights into the trade-offs between computational efficiency and model capability, moving beyond traditional evaluation methods that solely focus on performance by incorporating additional criteria such as token count, model size, and inference latency. In the following paragraphs, we present metrics for evaluating reasoning efficiency from both general and reasoning-specific perspectives. For the general perspective, we focus on metrics related to memory, computation, and power.

For the reasoning-specific perspective, we first review classic metrics used to assess reasoning capability and then discuss metrics tailored specifically for reasoning efficiency.

## 4.1.1 General Perspective

Memory.

• Model Size is a critical factor influencing its storage requirements and computational demands.

It is commonly measured in megabytes (MB) or gigabytes (GB) and is particularly important for deployment in resource-constrained environments. Several key factors contribute to a model's size, including parameter count, data type, and specific architectural design choices.

• Memory Footprint refers to the amount of Random Access Memory (RAM) required to run a model during training or inference. This metric is essential for understanding the model's resource demands, particularly in environments with limited memory capacity, such as edge devices or lightweight servers. Memory is measured in units like MB or GB and is primarily determined by the model size and additional temporary data such as intermediate variables and data structures.

## Computation.

• Floating Point Operations (FLOPs) measures the number of floating-point arithmetic operations a model performs during inference or training. This metric reflects a model's computational complexity and is commonly used to assess its efficiency.

• Latency (i.e., inference time) measures the time required for an LLM to generate a response after receiving an input. This metric reflects the model's responsiveness and is particularly important in real-world applications (e.g., chatbots) where timely outputs are essential. Latency is typically measured in seconds (s) and depends on hardware capabilities, model size, and system optimizations. Additionally, latency can be evaluated in two key ways: end-to-end latency, which measures the total time from receiving an input to producing the final output, and next-token latency, which assesses the time required to generate each token in autoregressive models.

• Throughput measures an LLM's efficiency by the number of tokens generated per second, typically expressed as tokens per second (TPS). It indicates overall processing capability and is crucial for batch processing or large-scale deployments. For concurrent request scenarios, throughput can be expressed as queries per second (QPS).

Power.

• Power Cost refers to the total energy consumed by an LLM throughout its lifecycle, typically measured in Watt-hours (Wh) or Joules (J). It reflects the energy usage of key hardware components such as GPUs, CPUs, and DRAM.

• Carbon Emission measures the environmental impact of LLMs by quantifying the greenhouse gases produced during their life cycle. It is typically expressed in kilograms (kg) or tons of CO 2 equivalent (CO 2 eq) and is influenced by factors such as hardware efficiency and model runtime.

Carbon emissions can be estimated as follows (see Appendix A.2.1 for the formula). Several tools are providing real-time emission tracking (e.g., CodeCarbon and Carbon-Tracker ) and predicting environmental costs (e.g., MLCO2 Impact ).

## 4.1.2 Reasoning-specific Persective

For reasoning evaluation, several accuracy variants are used. For example, greedy accuracy measures the accuracy when decoding deterministically (i.e., selecting the most likely token at each step). Minimummaximum spread quantifies stability by computing the accuracy gap across multiple runs. To better evaluate potential performance, the widely used Pass@k, which was initially proposed for generated code , has been adopted for reasoning tasks .

It measures the probability of obtaining at least one correct answer among k independent model outputs (see Appendix A.2.2 for the formula).

To capture stability, Passk is proposed, which measures the probability that all k generations are correct (see Appendix A.2.3 for the formula). Passk forms the basis for G-Pass@k tau , which further incorporates a tolerance threshold tau , requiring only a minimum proportion of correct responses among the k outputs. Furthermore, to jointly assess potential and stability, mG-Pass@k tau interpolates G-Pass@k tau over the interval [0.5, 1.0], producing a comprehensive metric (see Appendix A.2.4 for formulas).

These metrics provide a complete view of LLM reasoning performance, balancing one-shot potential with consistency across trials. Additionally, Total Agreement Rate@ evaluates the consistency of a model by running it N times and measuring how often it produces identical outputs. It has two variants: TARa@N, which checks for agreement in the final answers, and TARr@N, a stricter version that requires an exact string-level match of the full outputs across runs.

To assess reasoning efficiency, token count (i.e., the number of output tokens generated by the model) is commonly used as an evaluation metric. Some studies have proposed composite metrics that integrate multiple dimensions of reasoning efficiency. CoT-Valve proposes Accuracy per Computation Unit (ACU), calculated as accuracy divided by the product of parameter count and token count, explicitly considering the trade-offs among reasoning path length, model size, and model performance. proposes two metrics: the outcome efficiency metric and the process efficiency metric (see Appendix A.2.5 for formulas). The outcome efficiency metric evaluates the proportion of efficient tokens (i.e., the tokens used until the first correct answer is produced) in the model-generated outputs. In contrast, the process efficiency metric assesses the diversity of reasoning paths within generated solutions.

Additionally, introduced the overthinking score, a reliable metric explicitly designed for quantifying the degree of overthinking in LLMs. The score is obtained using an LLM-based evaluator combined with structured prompt templates. proposed the reasoning boundary (RB) to quantify the upper limit of LLM capability in handling complex reasoning tasks (see Appendix A.2.6 for the formula). proposed the underthinking metric to evaluate whether a model prematurely abandons effective reasoning paths in incorrect responses, resulting in a large number of unproductive tokens (see Appendix A.2.7 for the formula).

Preference for Metrics: Trade-off between Performance and Efficiency. In most efficient reasoning studies, performance and efficiency are typically evaluated separately-performance is measured by accuracy or Pass@k, while efficiency is assessed via token count, latency, or model size. This decoupled evaluation is simple and effective. However, some recent works have proposed unified metrics that jointly capture both aspects. For example, CoT-Valve introduces ACU, which combines parameter count, token count, and accuracy into a single metric. TALE proposes the optimal token budget, defined as the minimum number of tokens required to maintain correctness, and uses search algorithms to guide the model toward more efficient reasoning. Moving forward, there is a growing need for better evaluation metrics that can balance performance and efficiency more holistically and practically. O1-Pruner proposes a novel metric called the Accuracy Efficiency Score (AES), which considers both the solution length and model accuracy and penalizes accuracy degradation more than it rewards improvement (see more details in Appendix A.2.8).

## 4.2 Datasets and Benchmarks

Datasets and benchmarks are crucial in evaluating language models' reasoning capabilities and efficiency. They provide standardized protocols for assessing how well models can perform reasoning tasks under various resource constraints, such as limited computing or inference budgets. These resources cover a broad spectrum of reasoning types-including mathematical, logical, and multi-hop reasoning-enabling comprehensive evaluation across diverse domains and difficulty levels (see more details in Table ).

## Datasets.

To evaluate LLM reasoning ability, researchers commonly utilize developing reasoning benchmarks and datasets. Datasets are commonly categorized based on underlying reasoning types , such as math reasoning (e.g., GSM8K , MATH &amp; MATH-500 , AIME, and AQuA ), logical Reasoning (e.g., ProntoQA ), common sense reasoning (e.g., StrategyQA , Hot-PotQA ), algorithmic reasoning (e.g., Game of 24 , Bin Packing ), and planning (e.g., BlocksWorld ), Rubik's Cube ).

Benchmarks. Sys2Bench ) is a benchmark suite designed for evaluating LLMs, comprising 11 datasets that cover five categories of reasoning abilities (arithmetic, logical, commonsense, algorithmic, and planning). In addition to general reasoning benchmarks, several specialized benchmarks have emerged to evaluate some special situations. Overthinking Bench proposed a framework to assess the extent of overthinking in LLMs. Analyzing 4,018 trajectories revealed that LLMs prefer extended internal reasoning rather than environmental interactions, and it identified several undesirable behavioral patterns, such as Analysis Paralysis, Rogue Actions, and Premature Disengagement. Bag of Tricks evaluates explicitly the impact of TTC techniques on the reasoning abilities of LLMs and presents a benchmark covering six test-time optimization strategies evaluated on eight reasoning tasks. DNA Bench is a benchmark to assess the over-reasoning problem prevalent in current reasoning models. It comprises 150 adversarial prompts covering four key challenges (e.g., instruction adherence, hallucination avoidance, redundancy filtering, and unanswerable question recognition). DNA Bench highlights that reasoning models often produce redundant or invalid responses to simple yet misleading tasks, causing unnecessary computation and reduced accuracy.

## 5 Discussions and Future Directions

Efficiency Up Brings Safety Down? While long CoT has been shown to enhance reasoning capabilities, H-CoT reveals that LRMs can be exploited via extended CoT paths to bypass safety guardrails, leading to harmful outputs . This suggests a tension between safety and efficiency: enhancing safety requires longer, more deliberate reasoning for self-correction, which undermines efficiency, while shorter, efficient reasoning paths may skip critical safety checks. Balancing safety and efficiency thus emerges as a key challenge for future research in LLM reasoning.

Efficient Reasoning for Multimodal Large Language Model. Some efficient reasoning methods can be naturally extended to the multimodal large language model (MLLM) setting. The decomposition strategy discussed in Section 3.3.2, which breaks complex tasks into atomic reasoning units, can also benefit multimodal reasoning. Similarly, latent reasoning has shown promise in MLLMs (see Heima in Section 3.1.4). LatentLM further explores this direction by unifying discrete and continuous modalities through latent language modeling. It uses a variational autoencoder (VAE) to encode continuous data into latent vectors and then applies next-token diffusion for autoregressive generation, enabling scalable and efficient multimodal generation.

Break Memory Limitation. While long reasoning paths bring remarkable performance, they also cause severe memory issues due to long context. PENCIL addresses this by progressively erasing outdated and unimportant reasoning steps during generation. INFTYTHINK adopts a segmentation strategy, breaking the reasoning path into shorter fragments and inserting concise intermediate summaries, enabling chunk-wise thinking. OMNIKV observes that adjacent layers share highly similar token importance distributions and thus dynamically select key tokens and reuse them across subsequent layers. MCoT models multi-step reasoning as a Markov chain, where each step depends only on the previous one, avoiding the accumulation of long historical states in the KV cache. Training Efficiency. Training long reasoning models remains a computationally intensive task. Recent work has aimed to improve training efficiency through both curriculum learning and RL optimization. Curriculum-based approaches, such as Light-R1 (Wen et al., 2025) and FASTCUR, progressively increase task complexity to facilitate stable learning. Light-R1 employs curriculum SFT and multi-stage post-training, achieving strong performance with public datasets. FASTCURL extends this idea by combining curriculum RL with progressive context window extension, enabling efficient training of R1like models even on limited hardware. On the RL front, DAP proposes a scalable and open-source RL system, leveraging decoupled clipping and dynamic sampling for improved training stability. AGP addresses critical instability in the popular GRP by introducing a revised advantage estimation that mitigates zero-variance issues. Some coreset methods focus on reducing the quantity of training data. LIM argues that complex reasoning abilities are not learned from scratch but elicited through high-quality samples. By constructing a carefully curated dataset of only 817 reasoning samples, the model trained on this data significantly outperforms those trained on nearly 100K examples. The dataset construction involves filtering out easy problems, retaining challenging ones where advanced models struggle, and performing diversitybased sampling. Similarly, s1 (Muennighoff et al., 2025) constructs a compact dataset of 1,000 examples by jointly optimizing for difficulty, diversity, and quality.

## Opportunities in Traditional Model Compression. Traditional model compression techniques offer

valuable opportunities for improving reasoning efficiency. Among them, distillation has demonstrated significant potential in enhancing reasoning efficiency. Distillation effectively transfers reasoning abilities from larger models to smaller ones, enabling them to achieve strong reasoning while significantly reducing costs (see Section 3.2.1). systematically investigates three key factors that influence the effectiveness of CoT distillation: the granularity of reasoning paths, the format in which reasoning is presented, and the choice of teacher model. These insights offer practical guidance for advancing the distillation of reasoning abilities in small language models. Furthermore, distillation can play a role in other efficient reasoning directions, such as latent reasoning, where it helps compress explicit CoTs into more compact implicit reasoning paths (see Section 3.1.4) and SFT with variable-length CoT data (see Section 3.1.2). Distillation is a promising strategy for efficient reasoning, though there remains room for improvement. Additionally, enhancing the efficiency of the distillation process itself is also a valuable direction for future research. Beyond distillation, other model compression techniques, such as quantization and pruning, also show potential.

Although preliminary pruning experiments were not promising, successful quantization suggests that model compression can maintain reasoning performance while improving efficiency in areas like memory usage.

## 6 Conclusion

In conclusion, this survey provides a comprehensive overview of efficient reasoning techniques. We categorize current efforts into three main directions-shorter, smaller, and faster-each addressing reasoning efficiency from a unique perspective: compressing reasoning chains, building small language models with strong reasoning abilities, and accelerating the decoding stage. We further emphasize the role of model compression in achieving efficient reasoning. As reasoning efficiency continues to gain traction, we believe it holds significant promise for enabling scalable and practical deployment of reasoning models across diverse applications, from real-time systems to resource-constrained environments. We hope this survey serves as a valuable foundation for future research and development in this critical and rapidly evolving field.

## A.2.3 Passk

where n is the number of sampled outputs and c is the number of correct ones.

## A.2.5 Outcome and Process Efficiency Metric

Outcome Efficiency Metric:

where N is the number of instances, T i denotes the total number of tokens generated for instance i, Ti is the number of tokens until the first correct answer, and sigma i indicates correctness:

1, if at least one solution is correct 0, otherwise Process Efficiency Metric:

where D i represents tokens contributing to solution diversity, defined as:

where N is the number of incorrect response instances in the test set, T i is the total number of tokens in the i-th incorrect response, Ti is the number of tokens from the beginning of the i-th response up to and including the first correct thought. Then, the AES is computed as:

## A.2.8 Accuracy Efficiency

where alpha &gt; 0, beta &gt; 0, and gamma &gt; 0 are weighting factors. Default values alpha = 1, beta = 3, and gamma = 5 are used to emphasize penalizing accuracy drop more heavily than rewarding accuracy improvement.

## A.3 Complete List of Datasets and Benchmarks

A complete list of the datasets and benchmarks used in this area is summarized in Table , offering researchers an organized reference for efficient reasoning evaluation.