# When Deep Learning Meets Information Retrieval-based Bug Localization: A Survey

## 1 INTRODUCTION

The scale and complexity of software systems have expanded progressively, resulting in a tremendous increase in software bugs. Software bugs can trigger software failures, resulting in significant losses and widespread inconvenience. For example, on July 19, 2024, a faulty update distributed by CrowdStrike to its Falcon Sensor security software caused approximately 8.5 million Microsoft Windows systems to crash and fail to restart. This incident, described as "historic in scale, " resulted in widespread disruption across industries such as airlines, banks, hospitals, and governmental services, with an estimated financial loss of at least US$10 billion. 1 Given the significant losses caused by software bugs, software maintenance, particularly bug fixing, plays a critical role throughout the software lifecycle, consuming approximately 70% of the time and cost associated with software development . As an vital process of bug fixing, bug localization aims to identify code entities (e.g., classes, methods or changesets) related to software bugs . Over the years, researchers have proposed multiple approaches for bug localization, leveraging development history , bug report similarity , and others. Zou et al. categorized bug localization techniques into seven main families based on their inputs: spectrum-based bug localization (SBBL) , information retrieval-based bug localization (IRBL) , mutation-based bug localization , dynamic 1 Authors' addresses: Feifei Niu, feifeiniu96@gmail.com; Chuanyi Li, lcy@nju.edu.cn, State Key Laboratory for Novel Software Technology, Software Institute, Nanjing University, Nanjing, China; Kui Liu, brucekuiliu@gmail.com, Software Engineering Application Technology Lab, Huawei, Hangzhou, China; Xin Xia, xin.xia@acm.org, College of Computer Science and Technology, Zhejiang University, China; David Lo, davidlo@smu.edu.sg, School of Computing and Information Systems, Singapore Management University, Singapore. program slicing , history-based bug localization , stack trace analysis , predicate switching . Among these, SBBL and IRBL are among the most studied and widely recognized techniques . SBBL relies on run-time behavior, such as test coverage information, to compare program spectra on passed and failed test cases, enabling the ranking of program elements based on their likelihood of being faulty. In contrast, IRBL does not require run-time information. Instead, it takes a bug report-typically a natural language document describing the symptoms of a software bug-as input. The bug report serves as a query, while the source code entities of the project are treated as a document corpus. IRBL techniques identify textual overlap between the query (bug report) and the documents (source code entities) to locate the root cause of a bug. The source code entities with the highest overlap are returned in a ranked list, starting with the most relevant ones . This study focuses primarily on IRBL techniques, aiming to explore their effectiveness and challenges in bug localization tasks.

It has been two decades since the introduction of the first IRBL technique , and during the two decades, many new technologies have emerged in response to the development of machine learning and deep learning (DL) . Akbar et al. carried out a large-scale comparative evaluation of IRBL approaches, and they divided the approaches into three generations: The first generation approaches, developed between the years 2004 and 2011, leveraged Bag-of-Words (BoW) and laid foundations for such approaches. Marcus et al. demonstrated that Latent Semantic Indexing (LSI) could be used for concept location. The second generation approaches, scattered from 2010 to 2016 , augment BoWbased approaches with additional software information, including version history , code structure , and stack trace . These studies revealed that similar bug reports, code structure, version history, and stack trace, play an important role in localizing buggy files. Recently, the emergency of DL technology has ignited the development of the third generation of IRBL techniques. Lam et al. introduced the HyLoc approach in 2015, pioneering the utilization of DL for bug localization. Different from the first two generations, the third generation emphasises more on leveraging semantic information within both bug reports and source codes. Additionally, code syntactics structures, such as Abstract Syntax Trees (AST) and Control Flow Graphs (CFG), have also been incorporated . The integration of abundant code information has significantly enhanced the accuracy of third-generation IRBL.

As of now, we are aware of four surveys related to IRBL. Agarwal et al. reviewed 30 key papers on fault localization approaches in 2014. Wong et al. provided a comprehensive classification of fault localization approaches into eight categories, including spectrum-based, machine learning-based, and model-based approaches. Xia and Lo summarized techniques utilized in IRBL. In addition, six empirical studies have extensively evaluated the effectiveness of IRBL techniques, primarily focusing on non-DL-based approaches. Zou et al. and Kochhar et al. explored practitioners' perspectives on IRBL approaches. Despite these efforts, there is no in-depth survey dedicated to the application of DL in the IRBL field. Our research indicates that most DL-based approaches have emerged since 2020 (as shown in Fig. ). Given the rapid growth and increasing popularity of DL-based approaches, a comprehensive and systematic investigation into this third generation of IRBL techniques is imperative., which would provide researchers and practitioners with an overview of state of the art and future trends.

To this end, we systematically select a pool of 61 IRBL studies that leverage DL techniques, and then synthesize quantitative and qualitative analysis using the data extracted from these studies. The objectives of this study are to help researchers to gain a comprehensive understanding about typical concerns and current techniques at different stages of DL-based IRBL approaches, enabling researchers entering this field swiftly familiarize themselves; to help researchers pinpoint gaps and opportunities for future studies on this topic; and (3) to benefit practitioners in selecting gaps and tailoring IRBL models to their quality assurance requirements.

This work differs from the related studies from the following aspects: focusing particularly on the use of DL for IRBL, with substantial level of depth on several aspects of DL-based IRBL, (3) achieving comprehensive coverage of the literature including 61 primary studies published until November 2024, and (4) following a systematic literature survey research method. To the best of our knowledge, we are the first to perform a comprehensive survey on DL-based IRBL techniques. This survey encompasses the latest deep representations of text and code, including both semantic and syntactic information, along with various deep models and language models. Additionally, we highlight the research challenges and propose future directions for the field.

The remaining of the paper is structured as follows: Section 2 demonstrates the research methodology. Section 3 presents an overview of the results. Section 4 presents the overview on the various DL-based approaches for IRBL. Section 5 shows evaluation aspects of this problem. Section 6 outlines the challenges faced by IRBL approaches. Section 7 reports the threats to validity. Section 8 concludes the study and offers our vision on the future developments on the field.

## 2 METHODOLOGY

In this paper, we carry out a systematic literature survey under the guidelines of Kitchenham and Zhang et al. , to ensure an unbiased and repeatable procedure. It was carried out in three steps: planning, execution, and analysis. At the planning stage, we firstly identified the research questions and the survey protocols serving for our research aim and objectives. Then, at the execution stage, we carried out the study according to the protocols, study selection and snowballing to get a diverse set of literature on the subject of DL-based IRBL. Finally, we had four of our co-authors to analyze the selected literature and address the research questions at the analysis stage. Our research process can be visualized in Fig. .

## 2.1 Research Questions

The overall objective of this survey is to gain insights into the application of DL in IRBL. In order to have a detailed view of this topic, the survey addresses three research questions, which would allow us to systematically categorize and understand the current research and identify limitations and future research directions in deep bug localization.

## RQ1. What IRBL approaches leveraging DL have been proposed so far?

This RQ identifies and analyzes the techniques that have been proposed in the literature, including the model structure, the adopted text and code representation, as well as other features utilized for bug localization. We also provided an overview of the performance of all models. RQ2. How are the DL-based approaches evaluated?

This RQ spots how the literature evaluates their approaches, including the dataset employed for evaluation, evaluation metrics and validation approaches, as well as the granularity of different approaches, to facilitate future usage. RQ3. What are the challenges faced when applying DL in IRBL?

This RQ figures out the challenges and problems faced in deep bug localization, as well as points out the road head.

## 2.2 Search Strategy

We began with a search strategy to adequately and effectively search for relevant studies from academic digital libraries. We designed our search string based on the PIC framework , which has been widely used by surveys and systematic mapping studies . The relevant terms for population, intervention and outcomes are as follows:

Based on the PICO structure, the following search string was utilized for searching relevant articles:

("software" OR "program") AN AN AN AN

We applied this search string to the eight electronic databases listed in Table to search for relevant articles. We conducted our search on November 30, 2024, identifying studies published up to that date.

As shown in Fig. , we initially retrieved 440 distinct studies: 9 studies from IEEE Xplore, 21 studies from ACM Digital Library, 10 studies from Science Direct, 20 studies from Springer Link, 5 studies from Wiley InterScience, 11 studies from Elsevier, 420 studies from Google Scholar, and 25 studies from DBLP.

## 2.3 Study Selection

## 2.3.1 Inclusion/Exclusion Criteria.

To identify the most relevant articles to address the research questions in our survey, we specified our inclusion criterias (ICs) and exclusion criterias (ECs), inspired by similar studies . The ICs and ECs are outlined in Table . By applying the ICs and ECs on title, abstract and keywords, we first ensured that the selected studies focus on information retrieval-based approaches to bug localization (IC1) and employ DL algorithms in their methodologies (IC2). We adopt the definition of DL from the seminal work Deep Learning by Yann et al. , which defines it as the use of multilayer neural networks, e.g., Convolutional Neural Networks (CNNs) , Long Short-Term Memory Networks (LSTMs) , Recurrent Neural Networks (RNNs) , and transformers .

Additionally, we excluded literature that was not written in English (EC1), technical reports and theses (EC2), and duplicate or redundant studies reporting the same findings (EC3). Further filtering involved verifying that the research was published before November 2024 (IC3) in peer-reviewed venues or arXiv (IC4). To confirm the publication sources, we extracted information such as the "journal,""URL," "DOI," and "series." For papers from arXiv, particularly those from recent years, we opted to include them due to the emerging nature of this field, where many works are still in the submission process. Although these papers had not undergone peer review, we implemented a quality assessment process to exclude low-quality papers. We excluded papers that did not employ information retrieval techniques (EC4) or DL techniques (EC5). Additionally, we omitted research focused on fault localization methods, such as spectrum-based fault localization, that did not use bug reports as a basis for bug localization (EC6). Furthermore, we did not include studies that targeted bug localization within DL models themselves, as our emphasis was on the broader application of DL techniques for bug localization in software systems rather than within the architecture of DL models.

For empirical studies, we included those that introduced a new IRBL technique. However, if the paper was a secondary study, and the primary study was already included in our pool, we excluded it. Overall, 388 studies were removed based on our criteria, and as a result of this first screening step, we retained 52 studies in our pool.

## 2.3.2 Snowballing.

In order to obtain a thorough literature, we performed snowballing on our initial set of literature, under the guide line of . We carried out both forward and backward snowballing, and iteratively look through the references and citations of the literature. The snowballing process finishes when there are no more new studies adding in. For each round of snowballing, we screened the articles with the IC and EC. Finally, we included additional 12 papers in our final set. So far, we obtained 64 studies in our article pool.

## 2.3.3 Quality Assessment.

Quality assessment is a vital process of survey to ensure that we form a proper and fair representation of the research works . We used a quality checklist to assess the quality of studies and excluded studies that could not pass the checklist. We derived our quality checklist from Hall et al. and Hosseini et al. . We mainly assessed the primary studies from three aspects: the data, the model, and the evaluation criteria, as listed in Table . We assessed all three criteria to ensure high-quality paper.

The quality assessment checklist was independently applied to all 64 primary studies by two authors. Discussions were conducted in the event of disagreement to achieve consensus. Finally, three studies were excluded from our primary study pool for not meeting the quality assessment criteria. Finally, 61 primary studies were included for the data extraction phase.

## 2.4 Data Analysis

2.4.1 Data Extraction. After primary study selection, we formed a data extraction form (Table ) to extract data from primary studies, to answer the research questions. As indicated in the table, there are 17 fields in total. The initial five rows constitute the metadata of the studies, with six fields specifically pertained RQ1, another four fields collectively related to RQ2, and the remaining two fields associated with RQ3. The first author firstly formed an initial extraction form according to previous surveys . Then two authors conducted a pilot study on five randomly selected primary studies to assess the completeness and usability of the form. The two authors continuously discussed and refined the structure of the form until they reached a consensus. All the primary study was divided between the two authors to extract data from each set of studies independently. The data extraction form was collaboratively collected by two authors using an online sheet. Finally, the third author inspected the extraction form to ensure the correctness of the results.

## 2.4.2 Data Synthesis.

The ultimate goal of a survey study is the aggregation of information to provide an overview of the state of the art. We extracted quantitative data of our data extraction form to identify and report the results for RQ1 and RQ2. For RQ3, we carried out qualitative analysis to synthesize the outputs of our data extraction form. Specifically, this is to identify the reported challenges and solutions, as well as gaps of current research. Any discussion in a paper that explicitly mentioning a challenge or future work was extracted to the data extraction form during data extraction. We extract the main themes and revised manually to categorize the challenged towards application of DL in IRBL.

## 3 RESULTS

There are 61 articles in the final study pool. Fig. presents the distribution of selected primary studies over the years. The first approach that employs DL for IRBL appears in 2015. Then there is a significant increase in the number of papers published each year since 2020, which aligns with the rapid development and popularization of DL techniques. This indicates that DL has attracted the attention of an increasing number of researchers and practitioners, and is still undergoing huge growth at the time of this study. Figure shows the distribution of venues for the 61 studies (classified according to the category provided on the SJR website). Among them, 55 papers were published in peer-reviewed venues, including 39 in software engineering venues (such as ICSE, TSE, ASE), 7 in artificial intelligence venues (such as IJCAI), and 9 in other venues (such as Electronics). Fig. 3. Distribution of venues.

The goal of bug localization is to identify and pinpoint the possible location or locations in the source code where a software bug is present. The formulation of bug localization problem is: Let B = {𝑏 1 , 𝑏 2 , ..., 𝑏 𝑁 𝑏 } denote the set of bug reports, and C = {𝑐 1 , 𝑐 2 , ..., 𝑐 𝑁 𝑐 } denote the set of source code entities (e.g., files, methods, statements, or changesets) a software project, where 𝑁 𝑏 , and 𝑁 𝑐 denote the number of bug reports and source code entities, respectively. The target of bug localization is to learn a function 𝑓 : B C -&gt; Y, where Y {+1, -1} indicates whether a source code entity 𝑐 𝑖 C is related to a bug report 𝑏 𝑗 B.

The overall framework of DL-based IRBL approaches is depicted in Fig. . Initially, the link between bug reports and source code entities is constructed using heuristic rules proposed by Bachmann and Bernstein . IRBL approaches take bug reports and source code entities as text and utilize embedding to transform them into vectors. Subsequently, text and code features are extracted employing DL techniques. Finally, matching approaches are applied to correlate bug reports with source code entities.

Bug reports can be linked to different kinds of code entities, e.g., files, classes, methods, statements, or changesets. Therefore, IRBL can be categorized into different granularity, such as file-level, method-level, statement-level, and changeset-level.

Bug Reports Source Code Dataset Neural Network Evaluation Metrics Bug Report Embedding Source Code Embedding Text Feature Code Feature Other Features Validation Approach Testing Data For evaluation purposes, the dataset is divided into training and test set based on the validation approach. Validation approaches in DL include holdout validation, k-fold cross-validation, leaveone-out validation, and time series validation. Model performance is assessed by the evaluation metrics, including MAP, MRR, and Top k.

Since the second generation of IRBL approaches relies on code historical information, such as version history and similar bug reports, cross-project bug localization (CPBL) is challenging to implement. However, the third generation of IRBL approaches primarily focuses on comparing the similarity between bug reports and source code, with a few approaches also incorporating code historical information. To address the cold-start problem in newly established projects, DL-based CPBL approaches have been introduced. Additionally, leveraging the robust learning capabilities of language models, cross-language bug localization (CLBL) approaches have also been proposed. Among the 61 studies, three studies (4.9%) propose solutions for CPBL, while four studies (6.6%) are evaluated on both within-projects and cross-projects. Besides, two studies (3.3%) propose both CPBL and CLBL approaches. The remaining 52 studies (85.2%) exclusively put forth within-project bug localization (WPBL) approaches.

## 4 RQ1: WHAT IRBL APPROACHES LEVERAGING DL HAVE BEEN PROPOSED SO FAR?

Figure illustrates a timeline of IRBL approaches that utilize DL, encompassing 61 papers published from 2015 to October 2024. This timeline highlights the evolution of the third generation of IRBL approaches to date. In the following sections, we categorize these approaches based on their model structures, representation for bug reports and code, as well as other features employed for IRBL.

## 4.1 Model Taxonomy

The essence of IRBL lies in the matching between bug reports and source code elements. The text matching includes two types: semantic matching and relevance matching . Semantic matching evaluates the similarity between two pieces of text based on semantic information. Relevance matching necessitates the identification of documents related to the given query, which is typically keyword-based. The first two bug localization generations are primarily relevance matching-based. With the rapid development of Natural Language Processing (NLP) technology, the third generation approaches extract semantic features from bug reports and source code with deep neural networks and then compare the semantic features. There can be three kinds of model structures: (1) Some approaches employ the same neural network structure to coherently learn semantic features from bug reports and source code, which is homogeneous network (as shown in Figure ). (2) On the other hand, considering that bug reports and source code may have different syntactic structures, some approaches use different neural networks to extract semantic features individually, which is heterogeneous network (as shown in Figure ). (3) The third

DBL[DESSERT] Jiang et al.[59] Cooba[IJCAI] Zhu et al.[188] KGBugLocator[ICPC] Zhang et al.[181] CoLoc[Access] Luo et al. [87] DependLoc[APSEC] Yuan et al. [174] Yang et al. [Applied Computing][165] DRAST[arXiv] Sangle et al. [123] CG-CNN[AAAI] Huo et al. [54] MD-CNN[TSC] Wang et al. [137] Yang et al. [Symmetry][164] ImbalancedBugL oc [APSEC][10] MRAM[ICPC] Yang et al. [166] BLESER[arxiv] Zou et al. [195] DreamLoc[Reliability] Qi et al.[113] DEMOB [DMKD] Zhu et al. [189] BugTranslator[IST] Xiao et al.[156] CNN_Forest[EASE] Xiao et al.[159] Loyola et al. [CIKM][85] LS-CNN[IJCAI] Huo et al. [53] DeepLoc[IST] Xiao et al. [157] TRANP-CNN[TSE] Huo et al.[56] SLS-CNN[Access] Lin et al.[84] CAST[Access] Liang et al.[83] Polisetty et al. [PMDASE][112] Bugradar[IST] Xiao et al. [154] sgAttention[IJCAI] Ma et al. [88] S-BugLocator[IJCNN] Yong et al. [169] PBL[Access] Mohsen et al.[97] HMCBL[FSE] Du and Yu [36] LocFront[SEKE] Xu et al. [161] HGW-SFO-CDNN [ESWA] Li et al. [8] AttentiveBugLocator [Research Square] Ahmad et al. [4] AI-Aidaroos and Bamzahem [IJSEA][6] Ciborowska et al. [arXiv] [29] bjXnet[ASE J] Han et al. [44] SemirFL[QRS-C] Shi et al. [127] BLoco[KBS] Zhu et al. [191] BugPecker[ASE] Cao et al. [21] FBL-BERT[ICSE] Ciborowska et al. [28] FLIM[ESE] Liang et al.[82] CGMBL[QRS] Chen et al.[27] SBugLocater[Mathematical Problems] Huang et a. [51] HyLoc[ASE] Lam et al.[72] 2020 2021 2022 2023 NP-CNN[IJCAI] Huo et al. [55] DNNLOC[ICPC] Lam et al.[73] DeepLocator[APSEC] Xiao et al.[158] Xiao et al. [APSEC] [155] BL-GAN[TKDE] Zhu et al. [190] MLA[ICDM] Ma et al. [90] cFlow[Machine Learning J] Ma et al. [89] F: File M: Method C: Changeset H: Hunk C: CPBL W:WPBL CL:CLBL C M C C&amp;W M C C C&amp;W C C&amp;W C&amp;W C&amp;F&amp;H Yan et al. [JSEE][163] C&amp;F&amp;H TROBO[KSEM] Zhu et al. [192] M 2015 2016 2017 2018 2019 2020 2021 2022 2023 2015 2016 2017 2018 2019 2024 MSP-COA-IDN-LSTM [JSEP] Ali et. Al [7] bjEnet [SQJ] Han et al. [43] IBL [ICCEA] Zhao et al. [185] Alseadi et al. [Electrics][9] MACL-IRFL [arXiv] Zhou et al. [186] BRS_BL[ICDM] Zhang et al. [184] Chandramohan et al. [arXiv][25] C&amp;CL F&amp;M C By default: F W BLAZE [arXiv] Chakraborty et al.[23] C&amp;CL RLocator[TSE] Chakraborty et al. [24] category of models, referred to as relevance matching models, convert bug reports and source code into vector representations-using techniques such as TF-IDF, word embeddings, or pre-trained language models (e.g., BERT)-and compute their semantic or lexical similarity through vector similarity measures (e.g., cosine similarity or dot product), as illustrated in Figure ). These models focus purely on content relevance between bug reports and code entities and do not incorporate any structural or relational information. Besides, some approaches also integrate other features (r.f. Section 4.4), such as code fixing history, code comments, and more. In table 1 (column "Model", "HE" stands for heterogeneous, "HO" for homogeneous, "R" for relevance matching, and "O" for others ), we list the category of each approach using different model structures. A total of 33 studies utilized heterogeneous models, 13 studies employed homogeneous models, six papers adopted relevance matching models, and the remaining nine papers utilized other models, including encoder-decoder, and adversarial model. Overall, the majority of papers still favor the use of heterogeneous models, extracting distinct features from bug reports and source code, respectively.

Bug Report Source Code Score Bug Report Source Code Score Vector Deep Neural Network Semantic Representation Matching (b) Heterogeneous Model (c) Relevance Matching Model Matching Bug Report Source Code Score (a) Homogeneous Model HyLoc , DNNLOC , DRAST , and Bugradar utilized rSVM to extract feature vectors from bug reports and source code, subsequently employing DNN to match them against each other. Anh and Luyen [10] adopted a pre-trained GloVe model to build word embedding vectors for bug reports and source files, and then used DNN to matching the bug reports and source code. SBugLocater leveraged ALBERT to extract semantic feature from bug reports and source code, and then used DNN for semantic matching. Coloc and FLIM approach used pre-trained CodeBERT to convert bug reports and source code into vectors, and them calculated the cosine similarity between vectors. FBL-BERT used ColBERT to encode bug reports and changesets, and then matched each other with DNN models. SgAttention used Transformer encoder to encode bug reports, and used CodeBERT model to encode source code, then predicted the relevance of bug reports and source code with DNN . Ciborowska et al. used BERT to convert bug reports and source code into vectors, and then matched them with DNN models. Instead of using DNN models, Chandramohan et al. , Alsaedi et al. , the PBL and the Blaze approach utilized different embedding techniques -UnixCoder , GloVe, and SentenceTransformer , BERT, as well as CodeSage -to vectorize bug reports and source code, subsequently calculating their cosine similarity. Apart from the above three model structures, some other model structures have also been proposed for IRBL. Xiao et al. used LSTM-based encoder-decoder architecture, which took the feature vector of bug reports as input of the encoder, and the feature vector of source code as input of the decoder. Similarly, BugTranslator used an attention-based RNN encoderdecoder model to translate natural languages (i.e., bug reports) into code tokens. TROBO , CGMBL and BL-GAN used adversarial learning to bridge the semantic gap between code and bug reports, which includes a generator and a discriminator. The generator is designed to generate new synthetic instances from the task domain that can fool the discriminator, whereas the discriminator is desired to classify instances as either real (i.e., from the real task domain) or fake (i.e., generated by the generator) . LocFront embedded bug reports and source code with Word2Vec, and then computed the scaled dot-product attention score for each project information with respect to bug report. The BRS_BL approach generates summaries of bug reports, while the bjEnet approach creates summaries of code methods, followed by embedding processes.

Different models adopt varying strategies for linking bug reports with source code. The heterogeneous model offers flexibility and specialization by employing separate networks for bug reports and source code, allowing for better representation of each input type but increasing both complexity and resource demands . In contrast, the homogeneous model simplifies the architecture by using a single network for both inputs, resulting in greater efficiency and ease of training; however, it lacks the ability to generate specialized representations for bug reports and source code . The relevance matching model, on the other hand, focuses on computing a direct matching score between inputs, making it computationally efficient but potentially less capable of capturing complex semantic relationships . While the heterogeneous model excels at handling diverse input types, the homogeneous model is more streamlined but may underperform when the inputs vary significantly in structure and semantics. Moving forward, challenges for IRBL models include improving generalization across different programming languages and domains, managing ambiguous or incomplete bug reports, and ensuring scalability for large systems. Additionally, models must enhance explainability to foster trust and adoption, enable real-time detection, and incorporate multimodal data like runtime logs and version histories. Human-AI collaboration and model interpretability will also be critical for future advancements.

## 4.2 Text and Code Representation

Given that bug reports and source code are textual data, the process of vectorization and feature extraction are essential for DL algorithms to evaluate their relevance. Figure shows representation and feature extraction approaches used in the primary studies in our study pool.

84 55,53, 112,27 53, 156 159,157,56,8 4,83,54,59,8 9,127,191,16 9,8,6,7,184 85, 188 157 55,53, 112 156, 27 174,1 65,13 7,7 10 51,28, 36,29, 185,43 4,24 44 158, 155 Character Embedding 85, 155 181, 21,90, 184 190 24 164 164 166 131,51 ,161,1 63,7 85 155 155 114,112,4 1,62,61,44 ,131,93,8, 7,184 164 164 10 166 192,51,2 8,88,36, 29,43 113, 51,7 189 90, 161 174,16 5,137, 8,7 6,44 24 24 163, 184 159 One-hot TF-IDF Word2Vec GloVe Sent2Vec Doc2Vec BOW d-dimensional vector BERT ELMo TF-IDF One-hot Word2Vec GloVe 88 BOW BERT Character Embedding 189 7,8 ELMo n-gram Bug Report Source Code 2 12 2 2 1 5 1 25 3 5 15 16 5 1 19 5 11 1 1 1 2 1 15 2 29 17 9 2 3 1 5 1 17 28 18 192 1 d-dimensional vector Code T5 158, 83,18 8,195 158, 156,4,1 63,185 59 188,1 92,19 1,190 21 54, 192 192, 191 154 169 169 90 54,8 9,90 174 AST CDG CFG PDG 1 14 6 1 7,8 n-gram 2 82, 97 82, 97 195 7 7 Sentence Transformer Sentence Transformer 9 9 9 9 9 9 Other 85, 87 44 43 UniX coder 25 Code Sage 23 UniX coder 25 Code Sage 185 186 186 1 1 1 1 1 1 3 5 1 23 DNN CNN RNN Transformer Other TransH GCN ASTNN Attention GNN 72,73,12 3,10,127, 154,161, 8,163,7 72,73,12 3,127,10, 154,88,1 61,163,7

## 4.2.1 Bug Report Representation.

Various approaches have been adopted to convert the bug reports into easy-to-process representations, such as lists of features or embedding-based vector representations . The basic representation model is Vector Space Model (VSM), which represents text as a term-by-document matrix . Such techniques include one-hot, Bag-of-Words (BOW), and TF-IDF. However, since these representation approaches mainly rely on word frequencies and such, they overlook semantic context. To address this issue, some advanced embedding techniques have been proposed. Word embedding excels in capturing the contextual essence of a word within a document, enabling words with similar meanings to possess analogous vector representations. Famous pre-trained word embeddings include Word2Vec , GloVe , BERT , and ELMo .

Word2Vec. According to Figure , the most popular bug report representation technique is Word2Vec, which has been adopted in 25 studies. There are two main architectures used in Word2Vec: Skip-Gram and Continuous Bag of Words (CBOW). Skip-Gram predicts the context words given a target word, while CBOW predicts the target word given its context words. Both architectures have been used to learn semantic feature from bug reports.

After represent the text with Word2Vec, DL techniques including DNN, CNN, RNN and Transformer are used for extracting semantic features from Word2Vec. Among these four DL techniques, CNN is the most popular, which has been adopted in fourteen studies. Six studies use DNN to extract features. Zhang et al. used LSTM to extract semantic information from Word2Vec embedding of bug reports. Cao et al. and Ma et al. leverage GRU to extract textual feature from Word2Vec embedding. Besides, the self-attention layer is also employed to enhance the word vector in MLA . BL-GAN approach use Word2Vec as the embedding and attention-based Transformer network as the encoder, to achieve a better trade-off between the ability to model long-range dependency and computational efficiency.

TF-IDF. TF-IDF is the second most commonly used technique for bug reports representation. This weighting scheme helps identify key terms in a document and is widely used in various NLP tasks. Moreover, Zhou et al. proposed rVSM, which takes the document length into consideration, and could optimize the classic VSM model for bug localization. Experimental results show that rVSM performs better than classical VSM. As a result, ten out of fourteen studies use rVSM and four studies use the classical TF-IDF.

CNN and DNN are the most popular DL network for extracting features from TF-IDF vectors of bug reports. Ten studies used DNN to compute the similarity of TF-IDF vector with source code vectors. Four studies used CNN for extracting features from TF-IDF vector of bug reports. Alsaedi et al. used cosine similarity.

BERT. The third popular bug report representation is BERT, which is pre-trained on a large corpus of text data using unsupervised learning . It learns to predict missing words in a sentence, considering the context of the surrounding words. Du et al. proposed HMCBL approach which extracts bug report feature vectors from bug reports and then use multi-layer perception neural network as a projector to compress the vector. Ahmad et al. initialized each word using pretraining BERT to produce a dynamic context-dependent representation for each sentence based on the overall context. Then BiLSTM produces hidden for bug report embedding vectors. Ciborowska et al. used BERTOverflow to embed bug reports, which is pre-trained on the StackOverflow corpus. Huang et al. used ALBER as encoder and the k-max pooling layer to extract the feature information and obtain the final semantic matching score through the dense layer. FBL-BERT approach used a massive corpora of relevant text to pre-train the BERT model and then fine-tune on bug reports and bug-inducing changesets. CodeBERT is a bimodal BERT pretrained on both natural language and programming language . Chakraborty et al. utilized CodeBERT to encode bug reports and a DL model composed of CNN and LSTM to identify the most potential buggy files. Zhu et al. mapped bug reports into corresponding embedding sequences with CodeBERT and employ soft attention to automatically highlight the key information. IBL and bjEnet approach used BERT to extract semantic information from bug reports and employ DNNs to assess the relevance with the corresponding code information. MACL-IRFL approach leveraged CodeBERT to convert bug reports into vector representations.

GloVe. GloVe aims to generate dense vector representations (embeddings) for words by leveraging statistical information on the co-occurrence of words in a large corpus . Unlike sparse onehot encoding, GloVe embeddings capture semantic relationships between words in a continuous vector space. GloVe have proven effective in various NLP tasks, facilitating tasks such as text classification , sentiment analysis , and machine translation . Loyola et al. used GloVe pre-trained word embeddings and character-level embeddings, splitting tokens into characters, to learn vector representations of bug reports, which were processed through an LSTM module. Zhu et al. exploited the pre-trained GloVe to map each word into a kdimensional embedding and then exploit a BiLSTM to encode the input sequence. Anh et al. adopted a pre-trained GloVe model for bug reports and source code to address the lexical mismatch between them. Han et al. chose the GloVe as the pre-trained word vectors and then TextCNN network takes the text vector as input and outputs the report feature. Alsaedi et al. utilized GloVe to extract vector representations of bug reports and source code for similarity calculations.

Others. In addition to the commonly used word embedding approaches mentioned above, there are also others. Zhu et al. used pre-trained ELMo to generate dynamic context-dependent word representations in bug reports, followed by a BiLSTM to learn sequential features efficiently. Yang et al. clustered topics using the frequency of words in the bug report, and each created topic consisted of topic words. Xiao et al. noted that summaries in bug reports are concise, while descriptions are more detailed. To differentiate between them, they used Word2Vec with the Skip-gram model for summaries and Sent2Vec for descriptions. These representations were then input into an enhanced CNN model to localize buggy files. Similarly, Liu et al. used Word2Vec to convert summaries into word vectors and Doc2Vec to create sentence vectors from descriptions, extracting features from bug reports using CNN. Chakraborty et al. adopted CodeSage , a GPT-based multi-modal embedding model, to align bug reports and source code files. Chandramohan et al. fine-tuned a UniXcoder model for bug reports and source code.

Apart from the above word embeddings, character-level embeddings are also adopted, where each character in a word is transformed into a k-dimensional character embeddings and then processed by CNN or LSTM , or both . Ali et al employed three techniques such as Word2Vec, bags of n-grams model, and TF-IDF for acquiring the essential features from the "source files and bug reports." Then it used PCA for reducing the feature dimension, before using CNN for feature extraction. Besides, there are also two studies that only mention embedding the bug reports into a d-dimensional vector without pointing out the specific techniques.

## 4.2.2 Code

Representation. On the one hand, source code can be considered as plain text. In this way, source code can be embedded in the same way as bug reports, such as TF-IDF, Word2Vec, BERT, etc. On the other hand, as a kind of machine language, source code follows specific code syntax rules, and structure dictated by the programming language. Therefore, embedding source code may involve capturing the hierarchical structure, relationships between code elements, and even semantic meanings. Du et al. concluded that there are majorly three categories of code representation ways within the literature: token-based ways, syntactic-based ways and semanticbased ways. Token-based ways represent text or code by breaking it down into individual tokens. Tokens can be words, subwords, characters, or any other unit that the approach uses to divide the input. Although the simplicity facilities learning of token-based representation, it ignores the structural nature of code and thus captures limited semantics. Syntactic-based ways focus on capturing the structural relationships and grammar of the input text or code. They often involve parsing the input based on the language's syntax rules to extract syntactic information. Since the tree structure typically has an unusually deep hierarchy, significant refinement efforts of the raw tree representation are often required to enable successful learning in practice. As a result, the learning performance is constrained. Semantic-based ways aim to capture the meaning or semantics of the input text or code. These approaches often leverage pre-trained language models or embeddings to understand the context and meaning of the words or code elements.

As depicted in Figure , 44 studies employ token-based representation for source code, 22 studies utilize syntactic-based code representation, 12 studies adopt semantic-based approaches, with two exceptions.

Token-based Code Representation. According to Figure , most studies primarily adopt the token-based representation for source code. Specifically, these representation include TF-IDF, Onehot encoding, Character embedding, BOW, Word2Vec, GloVe, and n-gram. Among them, the most widely adopted is Word2Vec, used in 19 studies, followed by TF-IDF, which is employed in 16 studies. Next are One-hot (five studies) and GloVe (five studies), n-gram (two studies), character embedding (one study), and BO. Similar to bug reports, token-based code representation mainly use DNN, CNN and RNN to extract textual features from the code, except that Ma et al. embeded code tokens with Word2Vec and enhanced the token vectors with self-attention layer, followed by the max-pooling layer. Xu et al. also obtained the dot-product attention score between program information and bug reports. Xiao et al. converted the words in source code into Word2Vec word vectors and then use the ensemble of random forests with multi-grained scanning to extract code feature.

Syntactic-based Code Representation. Different from human language, code follows the syntax structure of machine language. When it comes to mining syntax feature from source code, the most straightforward approach is the AST structure, which is a hierarchical tree-like data structure that represents the syntactic structure of source code in a programming language. According to Figure , AST is the most dominant syntax-based code representation, which has been used in 14 primary studies. Besides, CFG, class dependency graph (CDG) and Program Dependency Graph (PDG) have also been employed.

Recent studies have revealed that neural models based on ASTs more accurately represent source codes, and programming languages can benefit from syntax and structured representations. In addition to extracting structural information from ASTs using DNN , CNN , and RNN , TransH , GCN , ASTNN , and GRU have also been employed.

The second widely adopted syntactic-based code representation is CFG, which has been adopted in five primary studies. CFG is a graphical representation of the flow of control or the execution flow within a program. It is a directed graph that models the possible paths that a program can take during its execution. In a CFG, nodes represent basic blocks of code, and edges represent the flow of control between these basic blocks. CG-CNN leverages DeepWalk to learn the semantic representation by considering the neighboring statements, after processed by CNN. TROBO combines a CNN layer and multi-layer GCN to process code file based on the CFG. Ma et al. designed a flowbased GRU for feature learning from the CFG, which transmits the semantics of statements along the execution path. Zhu et al. proposed Code-NoN, a hierarchical network that integrates CFG and AST properties to represent source code files. They then employed DG, a variant of Graph Convolutional Networks (GCN) , to effectively capture directional information between nodes by treating information propagation from parent and child classes differently. Ma et al. generated multi-level abstraction of CFG and then design a GNN model for feature learning from the multi-level abstraction of the CFG, where the block feature is alternately propagated within and between abstraction levels. Yuan et al. built a CDG for all source files in a project to illustrate class reference relationships. They then applied a customized Ant Colony algorithm on the CDG to simulate possible reference paths and quantify the intrinsic dependency relationships. Yong et al. constructed PDG of each source code file by analyzing the control flow and data flow and then use a CNN to extract semantic representation of each statement, followed by a BiLSTM for further representation.

Semantic-based Code Representation. Semantic-based code representation aim to represent the inherent meanings and relationships between different code elements rather than just focusing on syntax. It is crucial for tasks that require a deeper understanding of the functionality and intent of the code. In the 61 primary studies, BERT and ELMo are two typical semantic-based code representations. There are different variant of BERT, including basic BERT , CodeBERT , ALBERT and ColBERT . Besides, Du et al. introduced the Semantic Flow Graph (SFG) for compactly representing deep code semantics. Building on SFG, they proposed SemanticCodeBERT, a BERT-like model for learning code representations that consider deep code structure. Experimental results show that SemanticCodeBERT outperforms FBL-BERT , GraphCodeBERT , and UniXcoder . The BERT-based code representations are then input to DNN, CNN, or RNN networks to extract semantic code information. Liang et al. constructed a training dataset to fine-tune the CodeBERT model and then calculate the distance between bug reports and source code with Cosine Similarity or Manhattan Distance. Mohsen et al. fine-tuned BERT model with bug reports and source code and then calculate cosine similarity between the semantic representation of bug reports and source code. Zhu et al. embeded source code with ELMo-based embedding layer and then leverage MDCL encoder to extract the multi-grained features of the source file, which contains multiple DCNNs and a BiLSTM layer. Chandramohan et al. fine-tuned a UniXcoder model for their source code. Blaze relied on fine-tuned CodeSage for code representation. Alsaedi et al. used SentenceTransformers for method names embedding. bjEnet approach firstly leveraged CodeT5+ for code summarization, and then fine-tune the BERT model based on the code summarization.

Others. Except for the above most common code representations, Loyola et al. structured code changes into a code change genealogy , which considers additions or modifications of method calls. They then compute random walks over the directed graph starting from each node. Luo et al. proposed CoLoc, initially pre-trained on a large bug report corpus in an unsupervised manner and then further refined using a contrastive learning objective to capture semantic differences between bug reports and buggy files. Yang et al. converted the source code into d-dimensional vector without pointing out the specific embedding approach. Apart from extracting code semantic information with GloVe, Han et al. used code property graph (CPG) and encoded both the node and edge information with Gated Graph Convolution Networks (Gat-edGCN) . CPG consists of AST, CFG, and DDG, and can well reflect the information on the source code structure, the statement execution process, the control dependence, and the data dependence .

## 4.3 Feature Extraction Models

From Figure , we observe that currently CNN, DNN, and RNN are the most commonly used deep models for extracting features from textual representations of bug reports and source code. Additionally, graph-based structure, such as AST, is predominantly processed using GCN to extract features from the graph structure.

## 4.4 Other Features

Despite the advancements in DL technology that have enhanced the models' understanding of bug reports and source code, treating bug reports solely as natural language texts for NLP processing fails to capture their unique characteristics. Unlike formal texts, such as press releases, which adhere to structured grammar and style guidelines, bug reports are shaped by their practical purpose and context. They tend to be more informal, often incorporating technical jargon, abbreviations, and incomplete sentences. Moreover, bug reports frequently include domain-specific terminology and contextual clues (such as class names) that demand a deep understanding of the software and its environment. They often combine structured data, such as error codes and stack traces, with unstructured narrative descriptions, which can aid in bug localization. To fully leverage these features, existing research has also explored extracting additional features from bug reports, such as stack trace, bug fixing history, collaborative filtering score, class name, code comments. We listed such features and the corresponding papers in Table .

Stack Trace. Stack trace in bug reports can be a valuable source of localization hints, which is information regarding the suspicious file and the specific suspicious buggy line in the file, provided by the compiler when a program error occurs. Schroter et al. discovered that approximately 60% of the bugs within bug reports containing stack trace information can be resolved by modifying the functions identified in the stack trace. Additionally, they found that around 40% of the bugrelated source files can be located in the first frame of the stack trace, and an impressive 90% of the bug-related source files can be identified within the first 10 frames. These findings underscore the significance of stack traces in bug localization. In this bug report example, it also includes the error stack trace, which would facilitate the fixing of the reported bug. Yang et al. applied BRTracer to analyze the stack trace and then combine the extracted information from bug reports, source code and stack traces into an autoencoder . Xiao et al. extracted stack traces with regular expression, and then construct a knowledge graph called TriGraph. Xu et al. used regular expressions to extract effective stack frame information and then rank each file in the stack trace to score the source file. Ahmad et al. leveraged BRTracer to calculate stack trace score and then combine with other features to improve bug localization performance. Yan et al. and Alsaedi et al. extracted stack trace with regular expressions and then calculate the score as Schroter et al. . Ciborowska and Damevski used infozilla to distinguish stack traces and code snippets from natural language in bug reports.

## Niu et al.

Bug Fixing History. Several researchers have shown that recently fixed files in the recent past are most likes to be fixed in the near future . To this end, bug fixing history has been used to improve performance of IRBL approaches , which includes bug fixing recency and frequency. Ye et al. calculated the bug fixing recency as the inverse of the time interval between the creation of the bug report and the last time the source file was fixed, as shown in Equotion (1). In the formula, b.month represents the month in which bug report b was reported and f.month denotes the month in which source file f was last fixed before bug report b was created. The bug-fixing frequency is expressed by the number of times source file f was fixed before bug report b was submitted . Google's developers proposed BugCache algorithm, which maintains the commit history of each file in the system by the day of a new bug fix . They calculated the cache score based on Equotion , where 𝑓 is one of the buggy files in commit 𝑐 𝐶, 𝑡 𝑖 is the elapsed time in days since the file's creation. This formula tends to select the most recently and frequently changed files.

Collaborative Filtering Score. Researchers assume that similar bug reports are more likely to be corresponded to the same files . To this end, they proposed collaborative filtering score to measure the similarity of a bug report and previously fixed bug reports by the same file . Many DL-based bug localization approaches also take this score as an additional feature to enhance the model performance . It is calculated by Formula 2, where br(b, f) indicates the set of fixed bug reports related with the source file f before b was reported, and the collaborative filtering score cfScore(b, f) is the VSM similarity between b and the summaries of all the bug reports in br(b, f).

𝑐 𝑓 𝑆𝑐𝑜𝑟𝑒 (𝑏, 𝑓 ) = 𝑠𝑖𝑚𝑖 (𝑛, 𝑏𝑟 (𝑏, 𝑓 ))

(2) Class Name. At times, bug reports may include error messages that reference specific elements such as class names, functions, APIs, etc. . These terms serve as vital indicators for bug localization, demanding careful consideration. Lam et al. extracted name of identifiers, name of API classes and interfaces comments and string literals in the source code files and calculate the similarity with bug reports. Xiao et al. calculated class name similarity as Ye et al. did. DRAST approach extracted the function names, identifiers, macros, unions, typedef, struct, cpp from the C file and function names, class names, identifiers from Java files. The SemirFL model combined both CNN and rVSM, which is fed with four metadata features (bug-fixing recency, bug-fixing frequency, collaborative filtering score, and class name similarity.

In summary, due to the informality of bug reports, the variability in how users describe bugs can lead to ambiguity and inconsistency, presenting a significant challenge for models designed to process them. For instance, users might describe the same issue in different ways, using subjective terms that may not always align with the technical language used in the source code. Consequently, models aimed at understanding and linking bug reports to source code need to handle this informality, technical specificity, and occasional incompleteness or vagueness. Overcoming these challenges is crucial for accurate bug localization, as these tasks require deep contextual understanding and flexible language processing capabilities.

## 5 RQ2: HOW ARE THE DL-BASED APPROACHES EVALUATED?

As shown in Figure , in the evaluation process, there mainly involve three parts: dataset, evaluation metrics, and validation approaches. Moreover, approaches can also be categorized based on varying levels of granularity. Distribution. In order to understand which dataset has been used for evaluation, we extracted the datasets used in the primary studies, which usually consist of a few projects. Table presents information of projects that have been used in no less than ten primary studies. All these projects are developed using Java, and they are open source. In addition to the frequency of project data usage, we also recorded the size of each project, which includes the number of bug reports, the granularity of the dataset for bug localization, and the number of code entities corresponding to different granularities. Since different studies may choose different time duration, in Table we only present the mode of dataset sizes across all the studies. JDT, AspectJ, and SWT are the top three projects that have been used in over 40 primary studies. Meanwhile, they are also widely used in the first two generations of IRBL studies . According to the table, JDT has 8,184 files with 6,274 bug reports. The substantial number of bug reports and code contributes to its popularity.

## 5.1 Datasets

In addition to the typical projects mentioned above, there are also studies that contribute unique datasets. Table presents the open sourced dataset published in the 61 primary studies. Most datasets are in Java, but the authors of DRAST provided a dataset in C, and BLAZE contributed a dataset including C++, Go, Java, JavaScript and Python.

Xiao et al. used AspectsJ, Eclipse UI, JDT, SWT and Tomcat to evaluate the performance of CNN-Forest. The before-fixed version of the source code in each project are available in the dataset, which includes 10,754 bug reports in total. The BugC dataset consists of 2,462 bug reports from 21 open-source C projects. Sangle et al. evaluated DRAST on seven projects from BugC dataset, as well as Tomcat and AspectsJ projects from the benchmark dataset. Zhu et al. proposed a cross-project bug localization approach COOBA. It is evaluated on four projects of the dataset by Ye et al. : AspectJ, SWT, JDT Eclipse Platform UI. DreamLoc is evaluated on Birt, Eclipse, JDT, SWT, Tomcat from dataset by Ye et al. . BLESER was evaluated on the Defects4J dataset , which is a benchmark for bug localization and program repair . FLIM used a fine-tuned language model to extract code semantics at the function level, and was evaluated on Eclipse UI, JDT, BIRT, SWT, Tomcat, AspectJ from benchmark dataset by Ye et al. . HGW-SFO was evaluated on AspectJ, JDK, SWT, Tomcat and ZXing, which are collected by the authors . Zhang et al. created a new dataset based on Ye et al. , which includes Tomcat, SWT, JDT, Eclipse. Chakraborty et al. cured the BeetleBox dataset, which comprises 26,321 bugs sourced from 29 projects across C++, Go, Java, JavaScript, and Python.

We note that mainstream research continues to concentrate on the Java language, while only a limited number explore other languages . In the future, it is crucial to shift more attention towards alternative languages like Python and C++. Furthermore, more explanation on crosslanguage research avenues holds promising potential.

Construction. The predominant evaluation dataset, as contributed by Ye et al. , is widely utilized and encompasses six open-source projects: AspectJ, Birt, Eclipse Platform UI, JDT, SWT, and Tomcat. In this dataset, bug reports are linked to their fixed files based on heuristics proposed by Dallmeier and Zimmermann . To prevent the incorporation of future bug-fixing information in the dataset, a before-fix version of the project corresponding to each bug report was checked out. It is noteworthy that the dataset for changeset-level bug localization differs from that of method-level or file-level. Four studies at the changeset-level utilized the dataset contributed by Wen et al. . In this dataset, each bug report is linked to the respective bug-inducing changesets using the SZZ algorithm , and has been validated manually by the authors. Zhao et al. constructed their own dataset using the SZZ algorithm to evaluate IBL.

Data Quality and Bias. In DL-based approaches, data quality plays a critical role. Kochhar et al. highlights that the quality of the bug localization dataset can impact the validity of the results reported in the studies. Potential biases in bug localization include: 1) Misclassified reports; 2) Already localized reports; 3) Incorrect ground truth. Widyasari et al. revealed that bias 1 and bias 3 have no significant impact on bug localization results, while bias 2 has a statistically significant effect. For bias 1 (misclassified reports), bug reports in the datasets are typically collected from issue tracking systems (IST) such as JIRA and Bugzilla. Research shows that issue classification within these systems is sometimes erroneous . In our study, most of the studied research employed datasets from Ye et al. , where the verification of bug reports classification was not mentioned, which may introduce bias to the reported results. In addition, the changeset-level dataset , and the Defects4J dataset has been manually validated, reducing the impact of bias 1 to some extent. For bias 2 (already localized reports), studies found that developers emphasize stack traces, program entity names, and test cases as useful information, which may contain clues regarding bug locations. Their investigation reveals that around 50% (10% includes stack traces, 30% contains test cases, and 45% mentions program entities) of bug reports contain such information. Yang et al. further demonstrated that the presence of such information in bug reports significantly boosts model performance. Notably, in our study on IRBL leveraging DL technologies, six studies explicitly mentioned excluding fully localized reports (bug reports already describe which files and methods are faulty) to mitigate bias 2. Furthermore, FBL-BERT excluded log messages from bug reports to avoid boost the performance of models. While some studies , clearly specified that their data comes from bug reports within a specific time frame, they do not provide details about additional filtering processes. Other studies among the 61 studies did not discuss any data filtering or validation. For bias 3 (incorrect ground truth), previous research acknowledges that code commits often combine different objectives, such as bug-fixing, refactoring, and featureimplementation changes, leading to code tangling . However, current automatically constructed datasets have not addressed the noisy data caused by code tangling. Additionally, Niu et al. found that file renaming caused file paths from previous versions to no longer exist in the new version, which can impact the accuracy of similarity-based retrieval approaches. Duplicate bug reports are also quite common . Although Lee et al. argued that duplicated reports are often not attached to fixed files, if such duplicates appear separately in the training and testing data, it can lead to data leakage. Despite this risk, there has been limited discussion on handling this issue in DL-based bug localization approaches.

## 5.2 Granularity

From the perspective of localization granularity, IRBL approaches can be categorized into file-level, class-level, method-level, statement-level, and change-set-level approaches. As their names suggest, file-level, class-level, method-level, and statement-level localization aim to identify the buggy file, class, method, or statement, respectively. Change-set-level localization, on the other hand, focuses on the set of changes made by developers in a specific commit. While statement-level localization provides a micro-level perspective by honing in on individual buggy statements, change-set-level localization offers a macro-level perspective by examining code modifications as a whole. These varying levels of granularity enable diverse strategies for bug localization, catering to both detailed analysis and broader contextual understanding.

Out of all the studies, 52 specifically concentrate on file-level bug localization, four delve into method-level bug localization, while the remaining five explore changeset-level bug localization. Ciborowska et al. investigated both file-level, changeset-level and hunk-level bug localization. Within the scope of this study, there is no linelevel or statement-level approaches identified yet, while file-level bug localization has been widely researched.

As illustrated in Figure , IRBL models that leverage DL technologies essentially follow the same workflow across different granularities-specifically, file-level, method-level, and changeset-level (with no existing research at the statement-level). These models utilize bug reports and source code as input, employing various embedding techniques to extract semantic features from both text and code for matching purposes. The primary differences among the granularities can be categorized into three aspects:

(1) Construction of Ground Truth: The ground truth for file-level and method-level bug localization is established by linking bug reports to the corresponding bug-fixing commits, which are then associated with the files or methods involved in those commits. In contrast, changeset-level datasets are more closely aligned with just-in-time defect prediction , where bug reports are linked to bug-inducing commits (typically preceding the bug-fixing commits). Using git diff, the code changes from these commits are then associated with the bug reports to establish the ground truth.

(2) Search Space: Based on the construction of the ground truth, the search space for file-level and method-level bug localization consists of the files or methods in a specific version of the code repository. Conversely, the search space for changeset-level bug localization encompasses the entire commit history, treating the code changes in each commit as a distinct document.

(3) Code Processing: For embedding source code, file-level and method-level approaches typically embed each file or method separately. In the case of changeset-level code embedding, Ciborowska et al. proposed three encoding strategies for changeset-level code embedding: treating the changeset as a single document, splitting it into grouped lines by modification type, and splitting it into ordered lines with special tokens for each modification type. Their results showed that the third strategy, which preserves line order, outperforms the others.

File-level bug localization benefits from the richness of code text for precise localization, leading to higher accuracy in current research. However, it still requires developers' significant effort to locate the relevant code within large files . In contrast, finer-grained approaches, such as method-level and changeset-level localization, have emerged to alleviate some of this burden. Method-level localization, while reducing the effort required to locate buggy code , introduces complexities due to the intricate interactions between methods and the presence of multiple candidates within a single file, which can hinder accuracy. Changeset-level localization focuses on analyzing code changes within commits but demands a comprehensive understanding of the entire commit history, making it computationally intensive and complicating the isolation of relevant changes, particularly when multiple modifications are present.

In future research, the challenges of file-level localization lie in enhancing contextual analysis to better capture inter-file dependencies, as these relationships often influence bug occurrences; developing DL models that effectively represent these interactions to significantly improve localization accuracy. Furthermore, noise reduction techniques are needed to filter out irrelevant changes within large files, potentially utilizing advanced neural networks to prioritize significant modifications. Scalability poses another challenge, necessitating the development of efficient DL algorithms capable of managing vast codebases without compromising performance. In the realm of method-level localization, improving dependency tracking is crucial for analyzing the intricate interactions among methods, as this understanding is vital for accurately pinpointing bugs. Researchers should also explore granularity optimization, aiming for hybrid models that balance detailed method-level analysis with higher-level insights, alongside the creation of automated context extraction tools that summarize relevant information around methods to enhance alignment with bug reports. For changeset-level localization, advancing techniques for commit history analysis is critical, enabling the efficient isolation of relevant changes from extensive commit histories through DL-driven summarization methods. Additionally, enhancing the quality of commit messages remains a significant challenge; research could focus on employing natural language processing to generate clearer documentation, which is vital for tracing bug origins. Finally, improving contextual understanding of code changes will require developing models that analyze not only the code modifications but also the rationale behind them, facilitating better correlation with reported bugs. Across all granularities, the integration of DL techniques is paramount for enhancing pattern recognition capabilities, while the development of user-centric tools can simplify the localization process for developers. Emphasizing interdisciplinary approaches that merge software engineering insights with advanced DL methodologies could drive innovation in IRBL techniques, ultimately leading to more effective debugging tools and enhanced software quality.

## 5.3 Evaluation Metrics

Evaluation metrics are necessary for evaluating the performance of the proposed approach. Different evaluation metrics have been adopted for assessing the effectiveness of IRBL techniques. Figure shows the metrics that have been adopted. As most of the IRBL techniques generate ranking lists, which is basically ranking list of code entities (e.g., files, classes, methods, changesets, or statements), studies often use MAP, MRR and Top k as the evaluation metrics. In all 61 primary studies, almost all studies used MA for evaluation, closely followed by MR and Top k (54 studies). Then, a few studies also adopt Area Under Curve (AUC) , precision , recall , F-measure , and accuracy for evaluation, though these metrics are primarily associated with classification tasks. The definition of MAP, MRR and Top k are as follows:

Mean Average Precision (MAP) is a standard metric widely used in information retrieval to evaluate ranking approaches. It considers all the ranks of all buggy files into consideration. It is calculated as the mean of the Average Precision over all queries. Average Precision of a given bug report aggregates precision of positively recommended files as:

where i is a rank of the ranked files, N is the number of ranked files and pos(i) {0,1} indicates whether the ith file is a buggy file or not. is the precision at a given top i files.

Mean Reciprocal Rank (MRR) computes the average of the reciprocal of the positions of the first correctly located buggy files in the ranked files, following this equation:

Top k measures the percentage of bug reports in which at least one of the buggy files is a top k ranked file. The most common values for k are 1, 3, 5, 10, 20, etc. The higher the Top k value, the better the performance of the model/approach. MAP, MRR, and Top-k accuracy are essential metrics for evaluating IRBL tasks, each with a distinct focus. MAP assesses the precision across all ranks of buggy files by averaging the precision at each position, providing a comprehensive measure of retrieval quality. In contrast, MRR zeroes in on the rank of the first relevant buggy file, making it particularly valuable for scenarios where quickly finding any relevant item is crucial. Top-k accuracy is more practical, measuring how often the correct bug appears within the top k retrieved results, thus highlighting the system's effectiveness in presenting relevant results at the highest ranks. While MAP offers a broad perspective on precision across multiple buggy files, allowing us to assess the overall quality of the retrieval process, MRR emphasizes the importance of the first relevant hit, and Top-k is highly practical for real-world applications, where developers typically engage only with the top few candidates for bug fixes without having to sift through extensive lists of results .

## 5.4 Validation Approaches

Validation approaches are frequently employed in software bug localization to guarantee the precision and reliability of the techniques. Researchers use different data partition for training and testing. The validation process of bug localization contains data partitioning, sampling and validation. Since DL is essentially based on training and testing, so it is important to construct the training data and testing data, that is data partitioning. Moreover, in bug localization, since one bug report is typically relevant to a small number of code entities while irrelevant to much larger number of entities. The data is significantly imbalanced. Some studies propose to sample the data . The validation strategy of each approach has been listed in Table . Apart from several studies that did not elaborate on specific validation details, how the studies validate their approaches are as follows:

Partitioning and Validation. For WPBL, the validation approach can be classified according to that if all the bug reports are sorted chronologically: (1) Anachronistically Validation (Cross-Validation), the most common validation approach is k-fold cross-validation, typically with 10 folds. Specifically, for each project in the dataset, the bug reports are randomly divided into 10 equal size folds (as shown in Figure ). Among the 10 folds, the bug reports of a single fold are retained as the unfixed bug reports for testing models, and the remaining nine folds are used as fixed bug reports for training models. This process is repeated 10 times to ensure each fold has been evaluated. Sometimes, researchers may also leave one fold as validation set . Yang et al. divided bug reports in each project into 10 folds and chose seven folds as training set and the rest three folds as test set. Pendlebury et al. criticized that it introduces bias if a model is trained from future data and tested on past data. To resolve this problem, some studies started to sort the bug reports chronologically. (2) Chronologically Validation (Consecutive Validation), firstly all the bug reports in one project are sorted by time of creation. Then the studies either divide the dataset into two parts (the ratio can be 8:2, 7:3, or 6:4, etc), the newer bug reports are evaluated with model trained on older bug reports. Or the studies sort the bug reports chronologically by their report timestamps, and divide the bug reports into k (typically 10) folds with equal sizes, in which 𝑓 𝑜𝑙𝑑 1 is the oldest and 𝑓 𝑜𝑙𝑑 𝑘 is the newest. The model is trained on 𝑓 𝑜𝑙𝑑 𝑖 and tested on 𝑓 𝑜𝑙𝑑 𝑖+1 . The final results are obtained by taking the averages of all k-1 folds. This is called k-fold consecutive validation (as shown in Figure ).

As shown in Figure , k-fold cross-validation shuffles the data randomly between folds to ensure that each fold serves equally as both a training and testing set through k iterations. This approach helps mitigate bias in model evaluation by exposing the model to all data points as part of both the training and testing sets. Conversely, k-fold consecutive validation utilizes historical data for training and predicts using subsequent fold, iterating k-1 times. This strategy is particularly suitable when the order of data is significant, ensuring that future data points are not used to predict past outcomes. Given that the submission and resolution of bug reports occur in chronological order, we believe that consecutive validation provides a more accurate evaluation method.

For CPBL, four studies trained their model on one source project with 20% dataset from the target project, subsequently identifying buggy files for the remaining 80% of bug reports from the target project. To mitigate the impact of randomness, this process was repeated ten times to reduce the influence of randomness . Two studies used one project as training data and the data from another project as the testing data . Liang et al. compared all three strategies: 1) using the early 20% data of the target project to fine-tune the model; 2) using the data of the other five projects; and 3) mixing the above two sets of data. They found out that the third strategy, i.e., mixing the data from the other five projects and the early 20% data from the target project as the training set, performs the best.

Recently, two studies on CLBL came out. Blaze fine-tuned the CodeSage model on the BeetleBox training set and tested it on the BeetleBox test set, SWE-Bench , and the dataset from Ye et al. . The BeetleBox dataset includes C++, Go, Java, JavaScript, and Python languages. Chandramohan et al. fine-tuned UniXcoder on Java-only dataset and evaluated the performance on C/C++ and Golang projects. Although CLBL approaches are not as effective as non-cross-language approaches (as shown in Table ), these two studies still demonstrate that it is a promising direction, worthy of further research in the future.

Before-fix Version. As bug reports are submitted at various times and stages throughout the project, each report is associated with a specific project version. As projects undergo evolution over time, distinct versions may encompass varying source code spaces. Previous bug localization studies used just one code revision to match all the bug reports. However, software bugs exists in different versions of source code. Consequently, using only on revision of source code does not match the actual scenarios and may cause bias to the evaluation results. For example, the adopted revision of code may already have been fixed towards the specific bug report, and thus contains future bug-fixing information. Further, the buggy file might already have been deleted in the evaluated revision. To this end, Ye et al. pointed out that using before-fix version of code file can avoid leaking future bug-fixing information for older bug reports. The "Before-fix version" column in Table presents some studies adopted before-fix version of source code (✓), while some studies did not mention this evaluation setting in the paper (-). For studies involving changeset-level bug localization (O), the target for localization is already at the level of individual commits. Additionally, many papers explicitly state that their datasets follow "the oracle of bug-tofile mappings" , but no further details were provided. LS-CNN only mentioned using multiple versions, without offering additional information. MRAM built code revision graphs to address single revision problem, illustrating the relationships between code entities across different revisions. Previous empirical study pointed out that using a single version (typically the latest) may seem simpler but can undermine accuracy due to file additions and deletions. Niu et al. further demonstrated that selecting an improper revision can introduce significant bias in evaluation results. Associating bug reports with their corresponding code versions may require some effort, but it is a more accurate assessment approach that aligns better with the actual situation. Therefore, in the future, it is recommended that the researchers utilize proper versions for evaluation.

Class Imbalance Handling. In the source code repository, there can be thousands of files/methods/statements. However, one bug report is only relevant to several positive samples, while the number of negative samples is significantly huge. This leads to an extremely imbalanced dataset, where the model may struggle to learn patterns associated with positive classes. Nine studies studies randomly chose a certain number of negative samples, the number includes 200, 300, and 800. Nine studies studies chose negative samples by calculating the textual similarity between samples and bug reports. Among these nine studies, all chose the top k similar files as negative samples, while only Huang et al. chose the 300 least similar files as negative samples. Besides, Sangle et al. experimented with various sampling strategies to address class imbalance, including SMOTE , ADASYN , random over-sampling and under-sampling, Kmeans SMOTE , and TOMEK links . They found that SMOTE provided the best results for their dataset and models, outperforming other methods and no over-sampling. Zou et al. tested the BLESER approach using two re-sampling strategies (random over-sampling and under-sampling) and two cost-sensitive strategies (weightbased binary-cross entropy and Focal Loss). They found that random over-sampling significantly outperformed the other methods on their dataset and model. Chen et al. selected files that were ever buggy files as the negative samples. Wang et al. set the weight W in the binary cross-entropy lost function to solve this problem. Liu et al. and Anh et al. used focal loss function to rectify samples of the minority class within iterative training batches to the proposed models. Du et al. used a memory bank to store rich changesets obtained from different batches for later contrast. There are still some studies did not employ resampling, or the use of resampling was not mentioned in the papers. Over all, re-sampling is currently the most commonly used and most effective strategy for handling class imbalance issue.

## 6 RQ3: WHAT ARE THE CHALLENGES FACED WHEN APPLYING DL IN IRBL?

This section begins with a comprehensive performance overview, with an in-depth analysis of several key studies. It then summarizes the issues mitigated by third-generation technology compared to the previous two generations, as well as the challenges that remain unresolved.

## 6.1 Performance Overview and Key Studies

As shown in Figure , the most commonly used evaluation metrics are MAP, MRR, and Top k (where k is typically 1, 5, or 10). We have compiled the reported experimental results from each DL-based approach. Specifically, we calculated the average performance on the datasets studied, based on the reported results in the respective papers. The performance of all approach is presented in Table (with "-" indicating that the metric was not reported in the corresponding study). We have highlighted high performance approaches (MAP, MRR, Top 1 50%, Top 5 70%, Top 10 80%) in bold. It is important to note that the datasets and projects analyzed vary across different studies, and the experimental setups (e.g., version, data splits, sampling methods) also differ. As a result, this table provides only a rough comparison. A more precise comparison would require systematic evaluation.

Table shows that 18 approaches achieve an average MAP of 50% or higher, 29 approaches have an average MRR of 50% or higher, 11 approaches attain a Top 1 of 50% or higher, 19 approaches reach a Top 5 of 70% or higher, and 15 approaches achieve a Top 10 of 80% or higher. Among them, five approaches have all five evaluation metrics highlighted in bold: Coloc , bjXnet , Yan et al. , bjEnet , MACL-IRFL . At the same time, the table also indicates that although file-level bug localization has achieved commendable results, there is still room for improvement in bug localization at other levels, such as method-level and commit-level. Additionally, there is significant potential for enhancement in cross-project and cross-language bug localization.

In order to further look into the approaches, we selected seven key studies (published in top venues in software engineering) for a more in-depth analysis of their methodology, contributions, and limitations, as shown in Table . Based on these in-depth analyses, we summarize the challenges of existing research in the following sections.

## 6.2 Challenges Mitigated by DL Techniques

Lexical Gap. Lexical gap refers to that the terms used in bug reports to describe a bug are different from the terms and code tokens used in source files . Yang et al. verified through experiments by measuring the textual similarity with TF-IDF. The result shows that the average textual similarity between bug reports and their fixed methods is 0.0153, while that between bug reports and their irrelevant methods is 0.0149. Obviously, there is a big semantic gap between bug reports and methods, and a typical IR-based approach is not able to identify those faulty methods by matching textual similarity. The proceeding two generations primarily used VSM to calculate exact term for bug localization, the effectiveness will be compromised in the common case where there exists a significant lexical gap between the descriptions in the bug reports and naming practices adopted by developers in the software artifacts . To this end, the third generation of IRBL approaches use semantic word embedding techniques, especially language models (BERT, ELMo, GloVe, Word2Vec, etc) to capture the semantic information within bug reports and source code, so that terms with similar meaning will have high similarity.

Ignoring Structural Information within Code. Many previous studies took the source code as natural language, and correlated the bug report and source code by measuring similarity in the same lexical feature space. However, these approaches fail to consider the structure information of source code which carries additional semantics beyond the lexical terms . The program structure specifies how different statements interact with each other to accomplishing certain functionality, which provides additional semantics to the program functionality besides the lexical terms. To extract such structural information from code, syntactic-based code representations have been employed to simulate code structures, including AST , CFG , and PDG .

Cold-start. The proceeding two generations of IRBL approaches leverage previous bug fixing history. However, they may face the cold-start problem, when a bug localization approach needs to be applied to new projects having a limited bug fixing history. These approaches cannot perform well, because there is not sufficient bug fixing data (i.e., bug reports labeled with corresponding buggy code files) for training such supervised models . To address this issue, Huo et. al. presented the deep transfer bug localization task, and proposed the TRANP-CNN as the first solution for the cold-start problem which combines cross-project transfer learning and CNN for file-level bug localization. Subsequently, other approaches have also been proposed in response to the cold-start issue . Recently, there has also been work on cross-programming language approaches .

Although the introduction of DL has indeed mitigated above challenges, these challenges still persist in the third generation of IRBL approaches and have not been fully resolved. For instance, there still remain gaps between bug reports and source code, as the terminology used by users often differs from the terms used in code, resulting in a persistent gap in bug localization. Therefore, this continues to be a significant challenge. Moreover, while several cross-project approaches have been proposed to address the cold-start problem, research has shown that using a portion of the target project's data (typically 20% labeled data) for training yields the best results, outperforming predictions made with entirely new projects . Thus, the cold-start problem still remains an open issue.

## 6.3 Open Questions

Although significant progress has been made in software bug localization, several issues still remain to be addressed. We highlight some open challenges for future research, with the first three challenges being specific to DL-based approaches, while the following challenges are general issues relevant to IRBL.

Sampling. Given the imbalanced nature of the bug report dataset, characterized by a substantial surplus of negative files (i.e., non-buggy files) compared to positive files (i.e., buggy files), the sampling strategy plays a pivotal role in influencing the model's performance. Some researchers have chosen to sample the top k most similar files, while others have focused on the top k least similar files. There are also researchers randomly select a certain amount of files as negative sample. Determining an effective sampling approach to establish a balanced dataset requires further investigation. Additionally, it is crucial for researchers to be mindful of the impact of class rename refactoring, which alters file names. This precaution is necessary to avoid potential misinterpretation during the sampling process.

The Code Size Problem. The size of code files is not fixed; that is, some files are short, while others are long. It is difficult to handle the sparse representation when localizing short files. These short files cannot be simply filtered out for their relatively large proportion of faulty files . Furthermore, language models such as BERT and CodeBERT impose token count limitations, leading to the truncation or discarding of tokens that surpass the specified threshold. As a result, some valuable or buggy lines within large files may be omitted or overlooked.

Interpretability of DL Models. The decisions made by DL models are often opaque and lack clarity, making it challenging for developers to comprehend how predictions are generated. This complexity can diminish trust in the model's outputs, which are essential for effective bug identification and resolution.

Diversity in Programming Language. According to the statistical data in programming language, the majority of current research is primarily focused on bug localization in the Java programming language, possibly due to Java having been one of the most popular programming languages. However, in recent years, with the widespread adoption of DL technologies, Python has gradually replaced Java in its predominant position . This has led to a strong demand for bug localization studies in the Python language. Understanding the bugs and faults in large software repositories built in Python is therefore important . Future research is needed to delve into the language diversity in bug localization, like Python, C, C++, etc, examine variations in bug localization among different languages, propose corresponding bug localization approaches for specific languages, and even suggest language-agnostic bug localization approaches across diverse programming languages. Language diversity stands as an important topic for future research.

Generalization of Approaches. Table. 8 shows that most of the studies were evaluated on the few commonly used projects, such as JDT, AspectJ, SWT, Eclipse, and Tomcat. However, in practical use, it is also important to generalize the approach to other projects. Therefore, it is important to evaluate the approach on more projects, instead of always on the few projects. Also, generalization of replication study can also be practical.

Bug Report Category. The existing state-of-the-art practices often consider all bug reports as natural language descriptions and apply a uniform procedure to identify related buggy files. However, diverse bug reports may emphasize various aspects such as capability, security, function, performance, and more. Distinct approaches may yield varying results when applied to different types of bug reports. Zou et al. also revealed that there is a growing need for effective approaches to locate performance bugs, memory leaks, and environment-related bugs, which are particularly challenging to identify. To the best of our knowledge, there has been no research thus far dedicated to investigating bug localization specifically for different types of bug reports.

The Scale and Quality of Dataset. The AI pioneer Andrew Ng said it is time for smart-sized, "data-centric" solutions to big issues . Previously, researchers and developers primarily focus on identifying more effective models to improve bug localization performance while keeping the data largely unchanged. However, the potential quality issues and undesirable flaws of data needs more attention, such as missing values, incorrect labels, and anomalies. Zimmermann et al. have highlighted that sufficient defect data is often unavailable for many projects and companies. A largescale and high-quality dataset is a crucial step in ensuring the accuracy of bug localization . To this end, future research on datasets can focus on developing high-quality training datasets, guided by practitioners' recommendations , where key characteristics include reliability, relevance, accuracy, compliance, scale, and so on. Additionally, high-quality benchmarks are essential for unbiasedly evaluating the performance of LLMs.

Dealing with Additional Real-World Characteristics. While current state-of-the-art DL approaches have demonstrated promising performance in bug localization, existing studies have primarily concentrated on assessing the first two generations of IRBL approaches in industry projects . Significantly, there is a gap in research specifically evaluating DL approaches within the industry context. Although numerous approaches utilizing diverse DL models have been proposed, empirical research to determine the most effective DL model, particularly in real-world scenarios, is currently scarce.

Empirical evaluations of first-and second-generation IRBL approaches on industrial projects show that while these techniques perform well on small-scale systems, their effectiveness diminishes in large-scale industrial settings . Key challenges include software product lines, bilingual issues, and low-quality bug reports. However, leveraging product-specific information, capturing multiple perspectives from bug reports, and handling multilingual content can improve localization accuracy by enhancing lexical similarity . Moreover, using historical bug reports and applying collaborative filtering-prioritizing frequently modified files-further boosts performance in industrial contexts. found that their approach for Facebook projects relied heavily on word-level overlap between bug reports and commits, requiring both to share relevant terms. The method lacked the ability to assign different weights to features and faced a trade-off between complexity and interpretability. Jarman et al. demonstrated comparable performance on Adobe repositories to that of previous open-source evaluations, and highlighted the benefits of incorporating additional sources, such as commit messages and bug report comments. They also noted that optimal configurations and relevant data can vary significantly across different industrial projects.

When it comes to the application of existing approaches to real-world industry projects, the existing research is performed under idealized assumption, whereas real-world situations can be significantly more complex . For example, current studies often assume a one-to-one or one-to-many relationship between commits and bug reports, meaning that one or several commits fix a single bug. In such cases, all code modifications within these commits are considered as buggy code to that bug report. However, in practice, developers may address not only one but multiple bugs in a single commit. Furthermore, the code changes submitted in a commit may involve code refactoring or the addition of new features. In this way, not all code changes in the commit are necessarily buggy.

Another scenario is that current research assumes independence between different bugs, with no mutual influence. In reality, multiple bugs might emerge simultaneously and impact each other. These real-world situations are more intricate than the assumptions made in existing research. When applying existing solutions to practical scenarios, these issues are inevitable.

From the perspective of practitioners, there is a growing need for effective approaches to locate performance bugs, memory leaks, and environment-related bugs, which are particularly challenging to identify . Practitioners prefer solutions that operate at finer granularity levels, such as method, statement, or block . Additionally, Kochhar et al. found that practitioners have high thresholds for tool adoption: 73.58% consider inspecting more than five program elements unacceptable, and nearly 98% reject tools requiring inspection of over ten elements. Future research should focus on developing efficient and practical approaches that address these challenges while meeting practitioners' expectations for precision and usability.

Application of LLMs. Since the introduction of LLMs, the field of software engineering has undergone revolutionary transformations. LLMs present substantial challenges alongside vast opportunities for research in software engineering, including code generation, software testing, and program repair. While some studies have started exploring the use of LLMs for IRBL, such as BERT , CodeBERT , CodeT5 , and CodeSage , significant potential for further investigation remains. Future research should focus on effectively leveraging popular LLMs like GPT and LLaMA, as well as exploring prompt engineering and agent-based technologies, to enhance IRBL in software management. Furthermore, several benchmarks could be explored to evaluate the performance of LLMs in this domain . Given that using LLMs for IRBL can be resource-intensive, particularly when processing entire code repositories , and considering that some LLMs have token limitations that hinder their ability to handle large files, future work should prioritize optimizing this process to improve both efficiency and scalability.

Multilingual Bug Localization. Existing IRBL approaches primarily focus on bug localization within the same language context, where bug reports, identifiers, and comments in source code files are typically written in English. However, in non-English-speaking countries, bug reports are often written in native languages. Consequently, existing approaches initially translate non-English texts into English . This translation process, however, can introduce biases that negatively affect bug localization performance. Recently, LLMs (such as ChatGPT) have shown remarkable performance across various languages and tasks, indicating promising potential for their application in Multilingual bug localization.

## 7 THREATS TO VALIDITY

In this section, we conclude the threats to validity from four parts: construction, conclusion, internal and external threats. Construction Validity. The first threat to the construction validity comes from the search and selection of the primary studies. We follow Kitchenham et al. guidelines to perform the process. We carefully select our search terms by examining related work and queries widely used software engineering databases used in surveys. Moreover, to comprehensively retrieve articles as much as possible, we also adopt backward and forward snowballing using Google Scholar. Nevertheless, we believe that an adequate set of primary studies was collected for this study.

Another threat to the construction validity is the application of the inclusion and exclusion criteria, which is subject to researcher' bias. The authors create a list of inclusion and exclusion criteria, and independently applied inclusion and exclusion criteria to each candidate paper. A joint voting mechanism is used to mitigate the risk of ambiguous interpretations. The two authors will have a discussion if the two authors hold different opinions towards one paper. Conclusion Validity. Our reported results are directly derived from the data; however, variations in perspectives during the study selection process may lead other researchers to present dissimilar results. To address this potential threat, the authors conducted individual assessments and analyses of the papers. Internal Validity. The study selection process, which comprises inclusion/exclusion criteria, is exposed to the researchers' bias. The inclusion/exclusion criteria was ultimately determined through thorough discussions to reach a consensus. In the final selection procedure of primary studies, we made sure that all the authors were fully involved. In case of disagreement, conflicts were resolved through manual discussion.

External Validity This study exclusively investigates published works dealing with IRBL issues employing DL techniques. Potential data quality issues that have not been explored may exist. Furthermore, our findings are primarily applicable to IRBL and should not be generalized beyond this domain. It is important to highlight that the outcomes of this study may vary from those in spectrum-based fault localization and other fault localization research.

## 8 CONCLUSION AND FUTURE WORK

Bug localization is to identify and pinpoint the specific location or locations within a software program's source code where a bug or defect is present. Bug localization aims to assist developers in efficiently locating and addressing the root cause of software issues, reducing the time and effort required for debugging. This process contributes to enhancing the overall software development and maintenance workflow by facilitating a quicker resolution of bugs. IRBL using DL has particularly gained attention in the recent years. In this study, we conduct a systematic literature survey of existing IRBL techniques using DL to draw a picture on the state of the art. We studied 61 primary studies that leverage DL for IRBL, summarized the state of the art, mitigated issues and open challenges. Our study suggests that the integration of DL in IRBL enhances the model's capacity to extract semantic and syntactic information from both bug reports and source code, addressing issues such as lexical gaps, neglect of code structure information, cold-start problems, and more. Future research avenues for IRBL encompass exploring diversity in programming languages, adopting finer-grained granularity, and focusing on real-world applications. Most importantly, although some studies have started using LLMs for IRBL, there is still a need for more in-depth exploration and thorough investigation in this area.

In future research, our primary focus will be on addressing the aforementioned challenges. Besides, we intend to conduct an empirical study to compare and evaluate the performance of various IRBL approaches on a common comprehensive and representative benchmark containing recent data.

Table 3. List of Digital Libraries. Table 4. Inclusion and Exclusion Criteria.

## Inclusion Criteria

## IC1

The study focuses on information retrieval-based approaches to bug localization.

## IC2

The research employs deep learning algorithms (e.g., CNNs, LSTMs, RNNs, transformers) in its methodology.

## IC3

The paper was published prior to November 2024.

## IC4

The work is published in a peer-reviewed journal, conference proceedings, or workshop, with the full text accessible, including recent publications available on arXiv. Exclusion Criteria EC1

Literature not written in English.

## EC2

The paper is a technical report or thesis.

## EC3

Duplicate studies or different versions of similar work by the same authors that report identical findings.

## EC4

Papers that do not specifically target bug localization or that focus solely on general debugging techniques without an information retrieval component.

## EC5

Studies employing traditional machine learning techniques or non-machine learning approaches (e.g., rule-based systems) that do not incorporate deep learning.

## EC6

Research primarily centered on other fault localization approaches, such as spectrum-based fault localization, rather than utilizing bug reports for bug localization.

## EC7

Studies that address bug localization specifically for deep learning models, rather than focusing on deep learning-based techniques for bug localization. Table 9. Average performance metrics as reported in the original studies for each approach. Values highlighted in bold indicate MAP, MRR, Top 1 50%, Top 5 70%, and Top 10 80%. "-" indicates that the original study did not report.