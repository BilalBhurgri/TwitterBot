# Why Larger Language Models Do In-context Learning Differently?

## 1. Introduction

As large language models (LLM), e.g.,  and GPT4 , are transforming AI development with potentially profound impact on our societies, it is critical to understand their mechanism for safe and efficient deployment. An important emergent ability , which makes LLM successful, is in-context learning (ICL), where models are given a few exemplars of input-label pairs as part of the prompt before evaluating some new input. More specifically, ICL is a few-shot  evaluation method without updating parameters in LLM. Surprisingly, people find that, through ICL, LLM can perform well on tasks that have never been seen before, even without any finetuning. It means LLM can adapt to wide-ranging downstream tasks under efficient sample and computation complexity. The mechanism of ICL is different from traditional machine learning, such as supervised learning and unsupervised learning. For example, in neural networks, learning usually occurs in gradient updates, whereas there is only a forward inference in ICL and no gradient updates. Several recent works, trying to answer why LLM can learn in-context, argue that LLM secretly performs or simulates gradient descent as meta-optimizers with just a forward pass during ICL empirically  and theoretically . Although some insights have been obtained, the mechanism of ICL deserves further research to gain a better understanding.

Recently, there have been some important and surprising observations  that cannot be fully explained by existing studies. In particular,  finds that LLM is not robust during ICL and can be easily distracted by an irrelevant context. Furthermore,  shows that when we inject noise into the prompts, the larger language models may have a worse ICL ability than the small language models, and conjectures that the larger language models may overfit into the prompts and forget the prior knowledge from pretraining, while small models tend to follow the prior knowledge. On the other hand, ;  demonstrate that injecting noise does not affect the in-context learning that much for smaller models, which have a more strong pretraining knowledge bias. To improve the understanding of the ICL mechanism, to shed light on the properties and inner workings of LLMs, and to inspire efficient and safe use of ICL, we are interested in the following question:

Why do larger language models do in-context learning differently?

To answer this question, we study two settings: (1) onelayer single-head linear self-attention network  pretrained on linear regression in-context tasks , with rank constraint on the attention weight matrices for studying the effect of the model scale;

(2) two-layer multiple-head transformers  pretrained on sparse parity classification in-context tasks, comparing small or large head numbers for studying the effect of the model scale. In both settings, we give the closed-form optimal solutions. We show that smaller models emphasize important hidden features while larger models cover more features, e.g., less important features or noisy features. Then, we show that smaller models are more robust to label noise and input noise during evaluation, while larger models may easily be distracted by such noises, so larger models may have a worse ICL ability than smaller ones.

We also conduct in-context learning experiments on five prevalent NLP tasks utilizing various sizes of the Llama model families , whose results are consistent with previous work ) and our analysis.

Our contributions and novelty over existing work:

• We formalize new stylized theoretical settings for studying ICL and the scaling effect of LLM. See Section 4 for linear regression and Section 5 for parity.

• We characterize the optimal solutions for both settings (Theorem 4.1 and Theorem 5.1).

• The characterizations of the optimal elucidate different attention paid to different hidden features, which then leads to the different ICL behavior (Theorem 4.2, Theorem 4.3, Theorem 5.2).

• We further provide empirical evidence on large base and chat models corroborating our theoretical analysis (Figure , Figure ).

Note that previous ICL analysis paper may only focus on (1) the approximation power of transformers , e.g., constructing a transformer by hands which can do ICL, or (2) considering one-layer single-head linear self-attention network learning ICL on linear regression , and may not focus on the robustness analysis or explain the different behaviors. In this work, (1) we extend the linear model linear data analysis to the non-linear model and non-linear data setting, i.e., two-layer multiple-head transformers leaning ICL on sparse parity classification and (2) we have a rigorous behavior difference analysis under two settings, which explains the empirical observations and provides more insights into the effect of attention mechanism in ICL.

## 3. Preliminary

Notations. We denote [n] := {1, 2, . . . , n}. For a positive semidefinite matrix A, we denote ∥x∥ 2 A := x ⊤ Ax as the norm induced by a positive definite matrix A. We denote ∥ • ∥ F as the Frobenius norm. diag() function will map a vector to a diagonal matrix or map a matrix to a vector with its diagonal terms.

In-context learning. We follow the setup and notation of the problem in ; ; ; ; . In the pretraining stage of ICL, the model is pretrained on prompts. A prompt from a task τ is formed by N examples (x τ,1 , y τ,1 ), . . . , (x τ,N , y τ,N ) and a query token x τ,q for prediction, where for any i ∈ [N ] we have y τ,i ∈ R and x τ,i , x τ,q ∈ R d . The embedding matrix E τ , the label vector y τ , and the input matrix X τ are defined as:

x τ,1 x τ,2 . . . x τ,N x τ,q y τ,1 y τ,2 . . . y τ,N 0 ∈ R (d+1)×(N +1) ,

Given prompts represented as E τ 's and the corresponding true labels y τ,q 's, the pretraining aims to find a model whose output on E τ matches y τ,q . After pretraining, the evaluation stage applies the model to a new test prompt (potentially from a different task) and compares the model output to the true label on the query token.

Note that our pretraining stage is also called learning to learn in-context  or in-context training warmup  in existing work. Learning to learn in-context is the first step to understanding the mechanism of ICL in LLM following previous works .

Linear self-attention networks. The linear self-attention network has been widely studied , and will be used as the learning model or a component of the model in our two theoretical settings. It is defined as:

where θ = (W P V , W KQ ), E ∈ R (d+1)×(N +1) is the embedding matrix of the input prompt, and ρ is a normalization factor set to be the length of examples, i.e., ρ = N during pretraining. Similar to existing work, for simplicity, we have merged the projection and value matrices into W P V , and merged the key and query matrices into W KQ , and have a residual connection in our LSA network. The prediction of the network for the query token x τ,q will be the bottom right entry of the matrix output, i.e., the entry at location (d + 1), (N + 1), while other entries are not relevant to our study and thus are ignored. So only part of the model parameters are relevant. To see this, let us denote d+1) ,

where

Then the prediction is:

(2)

## 4. Linear Regression

In this section, we consider the linear regression task for incontext learning which is widely studied empirically  and theoretically .

Data and task. For each task τ , we assume for any i ∈

∼ N (0, Λ), where Λ is the covariance matrix. We also assume a d-dimension task weight w τ i.i.d.

∼ N (0, I d×d ) and the labels are given by y τ,i = ⟨w τ , x τ,i ⟩ and y τ,q = ⟨w τ , x τ,q ⟩. Model and loss. We study a one-layer single-head linear self-attention transformer (LSA) defined in Equation ( ) and we use y τ,q := f LSA,θ (E) (d+1),(N +1) as the prediction. We consider the mean square error (MSE) loss so that the empirical risk over B independent prompts is defined as

Measure model scale by rank. We first introduce a lemma from previous work that simplifies the MSE and justifies our measurement of the model scale. For notation simplicity, we denote

, where C is a constant independent with θ.

Lemma 4.1 tells us that the loss only depends on uU. If we consider non-zero u, w.l.o.g, letting u = 1, then we can see that the loss only depends on U ∈ R d×d ,

Note that U = W KQ 11 , then it is natural to measure the size of the model by rank of U. Recall that we merge the key matrix and the query matrix in attention together, i.e.,

The low-rank key and query matrix are practical and have been widely studied . Therefore, we use r = rank(U) to measure the scale of the model, i.e., larger r representing larger models. To study the behavior difference under different model scale, we will analyze U under different rank constraints.

## 4.1. Low Rank Optimal Solution

Since the token covariance matrix Λ is positive semidefinite symmetric, we have eigendecomposition Λ = QDQ ⊤ , where Q is an orthonormal matrix containing eigenvectors of Λ and D is a sorted diagonal matrix with nonnegative entries containing eigenvalues of Λ, denoting as

Then, we have the following theorem. Then

, where c is any nonzero constant, and

Proof sketch of Theorem 4.1. We defer the full proof to Appendix B.1. The proof idea is that we can decompose the loss function into different ranks, so we can keep the direction by their sorted "variance", i.e., argmin

where

## N

. We get the conclusion by g(x) is an increasing function on [0, ∞).

Theorem 4.1 gives the closed-form optimal rank-r solution of one-layer single-head linear self-attention transformer learning linear regression ICL tasks. Let f LSA,θ denote the optimal rank-r solution corresponding to the U * , u * above.

In detail, the optimal rank-r solution f LSA,θ satisfies

What hidden features does the model pay attention to? Theorem 4.1 shows that the optimal rank-r solution indeed is the truncated version of the optimal full-rank solution, keeping only the most important feature directions (i.e., the first r eigenvectors of the token covariance matrix). In detail, (1) for the optimal full-rank solution, we have for any

(2) for the optimal rank-r solution, we have for any i ≤ r, v * i = N (N +1)λi+tr(D) and for any i &gt; r, v * i = 0. That is, the small rank-r model keeps only the first r eigenvectors (viewed as hidden feature directions) and does not cover the others, while larger ranks cover more hidden features, and the large full rank model covers all features.

Recall that the prediction depends on U * x τ,q = cQV * Q ⊤ x τ,q ; see Equation ( ) and (3). So the optimal rank-r model only uses the components on the first r eigenvector directions to do the prediction in evaluations. When there is noise distributed in all directions, a smaller model can ignore noise and signals along less important directions but still keep the most important directions. Then it can be less sensitive to the noise, as empirically observed. This insight is formalized in the next subsection.

## 4.2. Behavior Difference

We now formalize our insight into the behavior difference based on our analysis on the optimal solutions. We consider the evaluation prompt to have M examples (may not be equal to the number of examples N during pretraining for a general evaluation setting), and assume noise in labels to facilitate the study of the behavior difference (our results can be applied to the noiseless case by considering noise level σ = 0). Formally, the evaluation prompt is:

where w is the weight for the evaluation task, and for any i ∈ [M ], the label noise ϵ i i.i.d.

∼ N (0, σ 2 ).

Recall Q are eigenvectors of Λ, i.e., Λ = QDQ ⊤ and

In practice, we can view the large variance part of x (top r directions in Q) as a useful signal (like words "positive", "negative"), and the small variance part (bottom d -r directions in Q) as the less important or useless information (like words "even", "just").

Based on such intuition, we can decompose the evaluation task weight w accordingly: w = Q(s+ξ), where the r-dim truncated vector s ∈ R d has s i = 0 for any r &lt; i ≤ d, and the residual vector ξ ∈ R d has ξ i = 0 for any 1 ≤ i ≤ r.

The following theorem (proved in Appendix B.2) quantifies the evaluation loss at different model scales r which can explain the scale's effect.

Implications. If N is large enough with N λ r ≫ tr(D) (which is practical as we usually pretrain networks on long text), then

The first term ∥ξ∥ 2 D is due to the residual features not covered by the network, so it decreases for larger r and becomes 0 for full-rank r = d. The second term 1 M (•) is significant since we typically have limited examples in evaluation, e.g., M = 16 ≪ N . Within it, (r + 1)∥s∥ 2 D corresponds to the first r directions, and rσ 2 corresponds to the label noise. These increase for larger r. So there is a trade-off between the two error terms when scaling up the model: for larger r the first term decreases while the second term increases. This depends on whether more signals are covered or more noise is kept when increasing the rank r.

To further illustrate the insights, we consider the special case when the model already covers all useful signals in the evaluation task: w = Qs, i.e., the label only depends on the top r features (like "positive", "negative" tokens). Our above analysis implies that a larger model will cover more useless features and keep more noise, and thus will have worse performance. This is formalized in the following theorem (proved in Appendix B.2).

Theorem 4.3 (Behavior difference for regression, special case). Let 0 ≤ r ≤ r ′ ≤ d and w = Qs where s is r-dim truncated vector. Denote the optimal rank-r solution as f 1 and the optimal rank-r ′ solution as f 2 . Then,

Implications. By Theorem 4.3, in this case,

.

We can decompose the above equation to input noise and label noise, and we know that ∥s∥ 2 D + σ 2 only depends on the intrinsic property of evaluation data and is independent of the model size. When we have a larger model (larger r ′ ), we will have a larger evaluation loss gap between the large and small models. It means larger language models may be easily affected by the label noise and input noise and may have worse in-context learning ability, while smaller models may be more robust to these noises as they only emphasize important signals. Moreover, if we increase the label noise scale σ 2 on purpose, the larger models will be more sensitive to the injected label noise. This is consistent with the observation in ;  and our experimental results in Section 6.

## 5. Sparse Parity Classification

We further consider a more sophisticated setting with nonlinear data which necessitates nonlinear models. Viewing sentences as generated from various kinds of thoughts and knowledge that can be represented as vectors in some hidden feature space, we consider the classic data model of dictionary learning or sparse coding, which has been widely used for text and images . Furthermore, beyond linear separability, we assume the labels are given by the (d, 2)-sparse parity on the hidden feature vector, which is the high-dimensional generalization of the classic XOR problem. Parities are a canonical family of highly non-linear learning problems and recently have been used in many recent studies on neural network learning . Data and task. Let X = R d be the input space, and Y = {±1} be the label space. Suppose G ∈ R d×d is an unknown dictionary with d columns that can be regarded as features; for simplicity, assume G is orthonormal. Let ϕ ∈ {±1} d be a hidden vector that indicates the presence of each feature. The data are generated as follows: for each task τ , generate two task indices t τ = (i τ , j τ ) which determines a distribution T τ ; then for this task, draw examples by ϕ ∼ T τ , and setting x = Gϕ (i.e., dictionary learning data), y = ϕ iτ ϕ jτ (i.e., XOR labels).

We now specify how to generate t τ and ϕ. As some of the hidden features are more important than others, we let A = [k] denote a subset of size k corresponding to the important features. We denote the important task set as

). Then t τ is drawn uniformly from S 1 with probability 1 -p T , and uniformly from S 2 with probability p T , where p T ∈ [0, 1 2 ) is the less-important task rate. For the distribution of ϕ, we assume ϕ [d]\{iτ ,jτ } is drawn uniformly from {±1} d-2 , and assume ϕ {iτ ,jτ } has good correlation (measured by a parameter γ ∈ (0, 1 4 )) with the label to facilitate learning. Independently, we have

Note that without correlation (γ = 0), it is well-known sparse parities will be hard to learn, so we consider γ &gt; 0.

Model. Following , we consider the reduced linear self-attention f LSA,θ (X, y, x q ) = y ⊤ X N W KQ x q (which is a reduced version of Equation ( )), and also denote W KQ as W for simplicity. It is used as the neuron in our two-layer multiple-head transformers:

where σ is ReLU activation, a = [a 1 , . . . , a m ] ⊤ ∈ [-1, 1] m , W (i) ∈ R d×d and m is the number of attention heads. Denote its parameters as θ = (a, W (1) , . . . , W (m) ).

This model is more complicated as it uses non-linear activation, and also has two layers with multiple heads.

Measure model scale by head number. We use the attention head number m to measure the model scale, as a larger m means the transformer can learn more attention patterns. We consider hinge loss ℓ(z) = max(0, 1 -z), and the population loss with weight-decay regularization:

Suppose N → ∞ and let the optimal solution of L λ (g) be

L λ (g).

## 5.1. Optimal Solution

We first introduce some notations to describe the optimal.

Let bin(•) be the integer to binary function, e.g., bin(6) = 110. Let digit(z, i) denote the digit at the i-th position (from right to left) of z, e.g., digit(01000, 4) = 1. We are now ready to characterize the optimal solution (proved in Appendix C.1).

Theorem 5.1 (Optimal solution for parity). Consider k = 2 ν1 , d = 2 ν2 , and let g * 1 and g * 2 denote the optimal solutions for m = 2(ν 1 + 1) and m = 2(ν 2 + 1), respectively. When

, g * 1 neurons are a subset of g * 2 neurons. Specifically, for any i ∈ [2(ν 2 + 1)], let V * ,(i) be diagonal matrix and • For any i ∈ [ν 2 ] and i τ ∈ [d], let a * i = -1 and V * ,(i) iτ ,iτ = (2 digit(bin(i τ -1), i) -1)/(4γ).

• For i = ν 2 + 1 and any i τ ∈ [d], let a * i = +1 and V * ,(i) iτ ,iτ = -ν j /(4γ) for g * j .

•

Proof sketch of Theorem 5.1. The proof is challenging as the non-linear model and non-linear data. We defer the full proof to Appendix C.1. The high-level intuition is transferring the optimal solution to patterns covering problems. For small p T , the model will "prefer" to cover all patterns in S 1 first. When the model becomes larger, by checking the sufficient and necessary conditions, it will continually learn to cover non-important features. Thus, the smaller model will mainly focus on important features, while the larger model will focus on all features.

Example for Theorem 5.1. When ν 2 = 3, the optimal has a 1 = a 2 = a 3 = -1, a 4 = +1 and,

and

On the other hand, the optimal g * 1 for ν 1 = 1 has the {1, 4, 5, 8}-th neurons of g * 2 . By carefully checking, we can see that the neurons in g * 1 (i.e., the {1, 4, 5, 8}-th neurons of g * 2 ) are used for parity classification task from S 1 , i.e, label determined by the first k = 2 ν1 = 2 dimensions. With the other neurons (i.e., the {2, 3, 6, 7}-th neurons of g * 2 ), g * 2 can further do parity classification on the task from S 2 , label determined by any two dimensions other than the first two dimensions.

What hidden features does the model pay attention to? Theorem 5.1 gives the closed-form optimal solution of twolayer multiple-head transformers learning sparse-parity ICL tasks. It shows the optimal solution of the smaller model indeed is a sub-model of the larger optimal model. In detail, the smaller model will mainly learn all important features, while the larger model will learn more features. This again shows a trade-off when increasing the model scale: larger models can learn more hidden features which can be beneficial if these features are relevant to the label, but also potentially keep more noise which is harmful.

## 5.2. Behavior Difference

Similar to Theorem 4.3, to illustrate our insights, we will consider a setting where the smaller model learns useful features for the evaluation task while the larger model covers extra features. That is, for evaluation, we uniformly draw a task t τ = (i τ , j τ ) from S 1 , and then draw M samples to form the evaluation prompt in the same way as during pretraining. To present our theorem (proved in Appendix C.2 using Theorem 5.1), we introduce some notations. Let

where for any i ∈ [2(ν 2 + 1)], V * ,(i) is defined in Theorem 5.1. Let φτ,q ∈ R d satisfy φτ,q,iτ = ϕ τ,q,iτ , φτ,q,jτ = ϕ τ,q,jτ and all other entries being zero. For a matrix Z and a vector v, let P Z denote the projection of v to the space of Z, i.e., P Z

Theorem 5.2 (Behavior difference for parity). Assume the same condition as Theorem 5.1. For j ∈ {1, 2}, Let θ j denote the parameters of g * j . For l ∈ [M ], let ξ l be uniformly drawn from {±1} d , and Ξ = l∈[M ] ξ l M . Then, for any δ ∈ (0, 1), with probability at least 1 -δ over the randomness of test data, we have g * j (X τ , y τ , x τ,q ) = h(θ j , 2γ φτ,q + P Dj (Ξ)) + ϵ j

a * i σ diag V * ,(i) ⊤ 2γ φτ,q + P Dj (Ξ) +ϵ j where ϵ j = O νj M log 1 δ and we have

• 2γ φτ,q is the signal useful for prediction: 0 = ℓ(y q • h(θ 1 , 2γ φτ,q )) = ℓ(y q • h(θ 2 , 2γ φτ,q )).

• P D1 (Ξ)) and P D2 (Ξ)) is noise not related to labels, and

Implications. Theorem 5.2 shows that during evaluation, we can decompose the input into two parts: signal and noise.

Both the larger model and smaller model can capture the signal part well. However, the smaller model has a much smaller influence from noise than the larger model, i.e., the ratio is ν1+1 ν2+1 . The reason is that smaller models emphasize important hidden features while larger ones cover more hidden features, and thus, smaller models are more robust to noise while larger ones are easily distracted, leading to different ICL behaviors. This again sheds light on where transformers pay attention to and how that affects ICL.

Remark 5.1. Here, we provide a detailed intuition about Theorem 5.2. Ξ is the input noise. When we only care about the noise part, we can rewrite the smaller model as g 1 = h(θ 1 , P D1 (Ξ)), and the larger model as g 2 = h(θ 2 , P D2 (Ξ)), where they share the same h function.

## Our conclusion says that E[∥P

, which means the smaller model's "effect" input noise is smaller than the larger model's "effect" input noise. Although their original input noise is the same, as the smaller model only focuses on limited features, the smaller model will ignore part of the noise, and the "effect" input noise is small. However, the larger model is the opposite.

## 6. Experiments

Brilliant recent work  runs intensive and thorough experiments to show that larger language models do in-context learning differently. Following their idea, we conduct similar experiments on binary classification datasets, which is consistent with our problem setting in the parity case, to support our theory statements.  . Larger models are easier to be affected by noise (flipped labels) and override pretrained biases than smaller models for different datasets and model families (original/without instruct turning). Accuracy is calculated over 1000 evaluation prompts per dataset and over 5 runs with different random seeds for each evaluation, using M = 16 in-context exemplars.

Experimental setup. Following the experimental protocols in ; , we conduct experiments on five prevalent NLP tasks, leveraging datasets from GLUE  tasks and Subj . Our experiments utilize various sizes of the Llama model families : 3B, 7B, 13B, 70B. We follow the prior work on in-context learning  and use M = 16 in-context exemplars. We aim to assess the models' ability to use inherent semantic biases from pretraining when facing in-context examples. As part of this experiment, we introduce noise by inverting an escalating percentage of in-context example labels. To illustrate, a 100% label inversion for the SST-2 dataset implies that every "positive" exemplar is now labeled "negative". Note that while we manipulate the in-context example labels, the evaluation sample labels remain consistent. We use the same templates as , a sample evaluation for SST-2 when M = 2: sentence: show us a good time The answer is positive.

## sentence: as dumb and cheesy

The answer is negative. sentence: it 's a charming and often affecting journey The answer is

## 6.1. Behavior Difference

Figure  shows the result of model performance (chat/with instruct turning) across all datasets with respect to the proportion of labels that are flipped. When 0% label flips, we observe that larger language models have better in-context The results show that larger models focus on both sentences, while smaller models only focus on relevant sentences. abilities. On the other hand, the performance decrease facing noise is more significant for larger models. As the percentage of label alterations increases, which can be viewed as increasing label noise σ 2 , the performance of small models remains flat and seldom is worse than random guessing while large models are easily affected by the noise, as predicted by our analysis. These results indicate that large models can override their pretraining biases in-context inputlabel correlations, while small models may not and are more robust to noise. This observation aligns with the findings in  and our analysis.

We can see a similar or even stronger phenomenon in Figure : larger models are more easily affected by noise (flipped labels) and override pretrained biases than smaller models for the original/without instruct turning version (see the "Average" sub-figure). On the one hand, we conclude that both large base models and large chat models suffer from ICL robustness issues. On the other hand, this is also consistent with recent work suggesting that instruction tuning will impair LLM's in-context learning capability.

## 6.2. Ablation Study

To further verify our analysis, we provide an ablation study. We concatenate an irrelevant sentence from GSM-IC  to an input-label pair sentence from SST-2 in GLUE dataset. We use "correct" to denote the original label and "wrong" to denote the flipped label. Then, we measure the magnitude of correlation between labelinput, by computing the norm of the last row of attention maps across all heads in the final layer. We do this between "correct"/"wrong" labels and the original/irrelevant inserted sentences. Figure  shows the results on 100 evaluation prompts; for example, the subfigure Correct+Relevant shows the correlation magnitude between the "correct" label and the original input sentence in each prompt. The results show that the small model Llama 2-13b mainly focuses on the relevant part (original input) and may ignore the irrelevant sentence, while the large model Llama 2-70b focuses on both sentences. This well aligns with our analysis.

## 7. More Discussions about Noise

There are three kinds of noise covered in our analysis:

Pretraining noise. We can see it as toxic or harmful pretraining data on the website (noisy training data). The model will learn these features and patterns. It is covered by ξ in the linear regression case and S 2 in the parity case.

Input noise during inference. We can see it as natural noise as the user's wrong spelling or biased sampling. It is a finite sampling error as x drawn from the Gaussian distribution for the linear regression case and a finite sampling error as x drawn from a uniform distribution for the parity case.

Label noise during inference. We can see it as adversarial examples, or misleading instructions, e.g., deliberately letting a model generate a wrong fact conclusion or harmful solution, e.g., poison making. It is σ in the linear regression case and S 2 in the parity case.

For pretraining noise, it will induce the model to learn noisy or harmful features. During inference, for input noise and label noise, the larger model will pay additional attention to these noisy or harmful features in the input and label pair, i.e., y • x, so that the input and label noise may cause a large perturbation in the final results. If there is no pretraining noise, then the larger model will have as good robustness as the smaller model. Also, if there is no input and label noise, the larger model will have as good robustness as the smaller model. The robustness gap only happens when both pretraining noise and inference noise exist simultaneously.

## 8. Conclusion

In this work, we answered our research question: why do larger language models do in-context learning differently? Our theoretical study showed that smaller models emphasize important hidden features while larger ones cover more hidden features, and thus the former are more robust to noise while the latter are more easily distracted, leading to different behaviors during in-context learning. Our empirical results provided positive support for the theoretical analysis. Our findings can help improve understanding of LLMs and ICL, and better training and application of these models.

). Then, we have

As V * is the minimum rank r solution, we have that v * i ≥ 0 for any i ∈ [d] and if v * i &gt; 0, we have

## N

. It is easy to see that g(x) is an increasing function on [0, ∞). Now, we use contradiction to show that V * only has non-zero entries in the first r diagonal entries. Suppose i &gt; r, such that v * i &gt; 0, then we must have j ≤ r such that v * j = 0 as V * is a rank r solution. We find that if we set

N and all other values remain the same, Equation (6) will strictly decrease as g(x) is an increasing function on [0, ∞). Thus, here is a contradiction. We finish the proof by V * = uQ ⊤ U * Q.

## B.2. Behavior Difference

Theorem 4.2 (Behavior difference for regression). Let w = Q(s + ξ) ∈ R d where s, ξ ∈ R d are truncated and residual vectors defined above. The optimal rank-r solution f LSA,θ in Theorem 4.1 satisfies:

Proof of Theorem 4.2. By Theorem 4.1, w.l.o.g, letting c = 1, the optimal rank-r solution f LSA,θ satisfies θ = (W P V , W KQ ), and

where

We can see that U * and Λ commute. Denote Λ := 1 M M i=1 x i x ⊤ i . Note that we have

Then, we have

,

where the last equality is due to i.i.d. of ϵ i . We see that the label noise can only have an effect in the second term. For the term (I) we have,

,

where the last equality is due to E[ Λ] = Λ and Λ is independent with x q . Note the fact that U * and Λ commute. For the (III) term, we have

By the property of trace, we have,

where the third last equality is by Lemma B.2. Furthermore, injecting w = Q(s + ξ), as ξ ⊤ V * is a zero vector, we have

Similarly, for the term (IV), we have

where the third equality is due to s ⊤ Aξ = 0 for any diagonal matrix A ∈ R d×d . Now, we analyze the label noise term. By U * and Λ being commutable, for the term (II), we have

where all cross terms vanish in the second equality. We conclude by combining four terms.

Theorem 4.3 (Behavior difference for regression, special case). Let 0 ≤ r ≤ r ′ ≤ d and w = Qs where s is r-dim truncated vector. Denote the optimal rank-r solution as f 1 and the optimal rank-r ′ solution as f 2 . Then,

)λi+tr(D) and for any i &gt; r ′ , v ′ * i = 0. Note that V * is a truncated diagonal matrix of V ′ * . By Theorem 4.1 and Theorem 4.2, we have

Proof of Theorem 5.1. Recall t τ = (i τ , j τ ). Let z τ ∈ R d satisfy z τ,iτ = z τ,jτ = 2γ and all other entries are zero. Denote V (i) = G ⊤ W (i) G. Notice that ∥W (i) ∥ 2 F = ∥V (i) ∥ 2 F . Thus, we denote V * ,(i) = G ⊤ W * ,(i) G. Then, we have E τ [ℓ (y τ,q • g(X τ , y τ , x τ,q ))]

a i σ 2γ(V We can get a similar equation for (II).

We make some definitions to be used. We define a pattern as (z 1 , {(i τ , z 2 ), (j τ , z 3 )}), where z 1 , z 2 , z 3 ∈ {±1}. We define a pattern is covered by a neuron means there exists i ∈ [m], such that a * i = z 1 and sign(V * ,(i) iτ ,iτ ) = z 2 and sign(V * ,(i) jτ ,jτ ) = z 3 . We define a neuron as being positive when its a * i = +1 and being negative when its a * i = -1. We define a pattern as being positive if z 1 = +1 and being negative if z 1 = -1.

Then all terms in (I) and (II) can be written as: probability at least 1 -δ over the randomness of test data, we have g * j (X τ , y τ , x τ,q ) = h(θ j , 2γ φτ,q + P Dj (Ξ)) + ϵ j

a * i σ diag V * ,(i) ⊤ 2γ φτ,q + P Dj (Ξ) +ϵ j where ϵ j = O νj M log 1 δ and we have

• 2γ φτ,q is the signal useful for prediction: 0 = ℓ(y q • h(θ 1 , 2γ φτ,q )) = ℓ(y q • h(θ 2 , 2γ φτ,q )).

• P D1 (Ξ)) and P D2 (Ξ)) is noise not related to labels, and

Proof of Theorem 5.2. Let Φ τ = [ϕ τ,1 , . . . , ϕ τ,M ] ⊤ ∈ R M ×d . Recall t τ = (i τ , j τ ). Let z τ ∈ R d satisfy z τ,iτ = z τ,jτ = 2γ and all other entries are zero. We see t τ as an index set and let r τ = [d] \ t τ . Then, we have

tτ ,: ϕ τ,q,tτ + y ⊤ τ Φ τ :,rτ M V * ,(i) rτ ,: ϕ τ,q,rτ .

Note that we can absorb the randomness of y τ , Φ τ :,rτ , ϕ τ,q,rτ together. Let z i for i ∈ [n] uniformly draw from {-1, +1}. By Chernoff bound for binomial distribution (Lemma C.1), for any 0 &lt; ϵ &lt; 1, we have

Thus, for any 0 &lt; δ &lt; 1, with probability at least 1 -δ over the randomness of evaluation data, such that