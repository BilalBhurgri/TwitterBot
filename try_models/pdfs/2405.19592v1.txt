Why Larger Language Models Do In-context Learning Differently?
Zhenmei Shi1Junyi Wei1Zhuoyan Xu1Yingyu Liang1 2
Abstract
Large language models (LLM) have emerged as
a powerful tool for AI, with the key ability of in-
context learning (ICL), where they can perform
well on unseen tasks based on a brief series of task
examples without necessitating any adjustments
to the model parameters. One recent interesting
mysterious observation is that models of different
scales may have different ICL behaviors: larger
models tend to be more sensitive to noise in the
test context. This work studies this observation
theoretically aiming to improve the understanding
of LLM and ICL. We analyze two stylized set-
tings: (1) linear regression with one-layer single-
head linear transformers and (2) parity classifica-
tion with two-layer multiple attention heads trans-
formers (non-linear data and non-linear model).
In both settings, we give closed-form optimal so-
lutions and find that smaller models emphasize
important hidden features while larger ones cover
more hidden features; thus, smaller models are
more robust to noise while larger ones are more
easily distracted, leading to different ICL behav-
iors. This sheds light on where transformers pay
attention to and how that affects ICL. Prelimi-
nary experimental results on large base and chat
models provide positive support for our analysis.
1. Introduction
As large language models (LLM), e.g., ChatGPT (OpenAI,
2022) and GPT4 (OpenAI, 2023), are transforming AI devel-
opment with potentially profound impact on our societies,
it is critical to understand their mechanism for safe and
efficient deployment. An important emergent ability (Wei
et al., 2022b; An et al., 2023), which makes LLM success-
ful, is in-context learning (ICL), where models are given
a few exemplars of input–label pairs as part of the prompt
1University of Wisconsin-Madison,2The University of Hong
Kong. Correspondence to: Zhenmei Shi, Yingyu Liang <zhmeishi,
yliang@cs.wisc.edu, yingyul@hku.hk >.
Proceedings of the 41stInternational Conference on Machine
Learning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by
the author(s).before evaluating some new input. More specifically, ICL is
a few-shot (Brown et al., 2020) evaluation method without
updating parameters in LLM. Surprisingly, people find that,
through ICL, LLM can perform well on tasks that have never
been seen before, even without any finetuning. It means
LLM can adapt to wide-ranging downstream tasks under
efficient sample and computation complexity. The mecha-
nism of ICL is different from traditional machine learning,
such as supervised learning and unsupervised learning. For
example, in neural networks, learning usually occurs in gra-
dient updates, whereas there is only a forward inference
in ICL and no gradient updates. Several recent works, try-
ing to answer why LLM can learn in-context, argue that
LLM secretly performs or simulates gradient descent as
meta-optimizers with just a forward pass during ICL empir-
ically (Dai et al., 2022; V on Oswald et al., 2023; Malladi
et al., 2023) and theoretically (Zhang et al., 2023b; Ahn
et al., 2023; Mahankali et al., 2023; Cheng et al., 2023; Bai
et al., 2023; Huang et al., 2023; Li et al., 2023b; Guo et al.,
2024; Wu et al., 2024). Although some insights have been
obtained, the mechanism of ICL deserves further research
to gain a better understanding.
Recently, there have been some important and surprising
observations (Min et al., 2022; Pan et al., 2023; Wei et al.,
2023b; Shi et al., 2023a) that cannot be fully explained by
existing studies. In particular, Shi et al. (2023a) finds that
LLM is not robust during ICL and can be easily distracted
by an irrelevant context. Furthermore, Wei et al. (2023b)
shows that when we inject noise into the prompts, the larger
language models may have a worse ICL ability than the
small language models, and conjectures that the larger lan-
guage models may overfit into the prompts and forget the
prior knowledge from pretraining, while small models tend
to follow the prior knowledge. On the other hand, Min et al.
(2022); Pan et al. (2023) demonstrate that injecting noise
does not affect the in-context learning that much for smaller
models, which have a more strong pretraining knowledge
bias. To improve the understanding of the ICL mechanism,
to shed light on the properties and inner workings of LLMs,
and to inspire efficient and safe use of ICL, we are interested
in the following question:
Why do larger language models do in-context learning
differently?
1arXiv:2405.19592v1  [cs.LG]  30 May 2024
Why Larger Language Models Do In-context Learning Differently?
To answer this question, we study two settings: (1) one-
layer single-head linear self-attention network (Schlag et al.,
2021; V on Oswald et al., 2023; Akyurek et al., 2023; Ahn
et al., 2023; Zhang et al., 2023b; Mahankali et al., 2023;
Wu et al., 2024) pretrained on linear regression in-context
tasks (Garg et al., 2022; Raventos et al., 2023; V on Oswald
et al., 2023; Akyurek et al., 2023; Bai et al., 2023; Ma-
hankali et al., 2023; Zhang et al., 2023b; Ahn et al., 2023;
Li et al., 2023c; Huang et al., 2023; Wu et al., 2024), with
rank constraint on the attention weight matrices for studying
the effect of the model scale; (2) two-layer multiple-head
transformers (Li et al., 2023b) pretrained on sparse parity
classification in-context tasks, comparing small or large
head numbers for studying the effect of the model scale. In
both settings, we give the closed-form optimal solutions.
We show that smaller models emphasize important hidden
features while larger models cover more features, e.g., less
important features or noisy features. Then, we show that
smaller models are more robust to label noise and input
noise during evaluation, while larger models may easily
be distracted by such noises, so larger models may have a
worse ICL ability than smaller ones.
We also conduct in-context learning experiments on five
prevalent NLP tasks utilizing various sizes of the Llama
model families (Touvron et al., 2023a;b), whose results are
consistent with previous work (Min et al., 2022; Pan et al.,
2023; Wei et al., 2023b) and our analysis.
Our contributions and novelty over existing work:
•We formalize new stylized theoretical settings for
studying ICL and the scaling effect of LLM. See Sec-
tion 4 for linear regression and Section 5 for parity.
•We characterize the optimal solutions for both settings
(Theorem 4.1 and Theorem 5.1).
•The characterizations of the optimal elucidate differ-
ent attention paid to different hidden features, which
then leads to the different ICL behavior (Theorem 4.2,
Theorem 4.3, Theorem 5.2).
•We further provide empirical evidence on large base
and chat models corroborating our theoretical analysis
(Figure 1, Figure 2).
Note that previous ICL analysis paper may only focus on
(1) the approximation power of transformers (Garg et al.,
2022; Panigrahi et al., 2023; Guo et al., 2024; Bai et al.,
2023; Cheng et al., 2023), e.g., constructing a transformer
by hands which can do ICL, or (2) considering one-layer
single-head linear self-attention network learning ICL on
linear regression (V on Oswald et al., 2023; Akyurek et al.,
2023; Mahankali et al., 2023; Zhang et al., 2023b; Ahn et al.,
2023; Wu et al., 2024), and may not focus on the robustnessanalysis or explain the different behaviors. In this work,
(1) we extend the linear model linear data analysis to the
non-linear model and non-linear data setting, i.e., two-layer
multiple-head transformers leaning ICL on sparse parity
classification and (2) we have a rigorous behavior difference
analysis under two settings, which explains the empirical
observations and provides more insights into the effect of
attention mechanism in ICL.
3. Preliminary
Notations. We denote [n] :={1,2, . . . , n }. For a positive
semidefinite matrix A, we denote ∥x∥2
A:=x⊤Axas the
norm induced by a positive definite matrix A. We denote
∥ · ∥Fas the Frobenius norm. diag() function will map a
vector to a diagonal matrix or map a matrix to a vector with
its diagonal terms.
In-context learning. We follow the setup and notation of
the problem in Zhang et al. (2023b); Mahankali et al. (2023);
Ahn et al. (2023); Huang et al. (2023); Wu et al. (2024). In
the pretraining stage of ICL, the model is pretrained on
prompts. A prompt from a task τis formed by Nexamples
(xτ,1, yτ,1), . . . , (xτ,N, yτ,N)and a query token xτ,qfor
prediction, where for any i∈[N]we have yτ,i∈Rand
xτ,i,xτ,q∈Rd. The embedding matrix Eτ, the label vector
yτ, and the input matrix Xτare defined as:
Eτ:=
xτ,1xτ,2. . .xτ,Nxτ,q
yτ,1yτ,2. . . y τ,N 0
∈R(d+1)×(N+1),
yτ:=[yτ,1, . . . , y τ,N]⊤∈RN, y τ,q∈R,
Xτ:=[xτ,1, . . . ,xτ,N]⊤∈RN×d,xτ,q∈Rd.
Given prompts represented as Eτ’s and the corresponding
true labels yτ,q’s, the pretraining aims to find a model whose
output on Eτmatches yτ,q. After pretraining, the evaluation
stage applies the model to a new test prompt (potentially
from a different task) and compares the model output to the
true label on the query token.
Note that our pretraining stage is also called learning to
learn in-context (Min et al., 2021) or in-context training
warmup (Dong et al., 2022) in existing work. Learning to
learn in-context is the first step to understanding the mecha-
nism of ICL in LLM following previous works (Raventos
et al., 2023; Zhou et al., 2023b; Zhang et al., 2023b; Ma-
hankali et al., 2023; Ahn et al., 2023; Huang et al., 2023; Li
et al., 2023b; Wu et al., 2024).
Linear self-attention networks. The linear self-attention
network has been widely studied (Schlag et al., 2021;
V on Oswald et al., 2023; Akyurek et al., 2023; Ahn et al.,
2023; Zhang et al., 2023b; Mahankali et al., 2023; Wu et al.,
2024; Ahn et al., 2024), and will be used as the learning
model or a component of the model in our two theoreticalsettings. It is defined as:
fLSA,θ(E) =
E+WPVE·E⊤WKQE
ρ
, (1)
where θ= (WPV,WKQ),E∈R(d+1)×(N+1)is the em-
bedding matrix of the input prompt, and ρis a normalization
factor set to be the length of examples, i.e., ρ=Nduring
pretraining. Similar to existing work, for simplicity, we
have merged the projection and value matrices into WPV,
and merged the key and query matrices into WKQ, and
have a residual connection in our LSA network. The pre-
diction of the network for the query token xτ,qwill be the
bottom right entry of the matrix output, i.e., the entry at lo-
cation (d+ 1),(N+ 1) , while other entries are not relevant
to our study and thus are ignored. So only part of the model
parameters are relevant. To see this, let us denote
WPV=WPV
11 wPV
12
(wPV
21)⊤wPV
22
∈R(d+1)×(d+1),
WKQ=WKQ
11 wKQ
12
(wKQ
21)⊤wKQ
22
∈R(d+1)×(d+1),
where WPV
11,WKQ
11∈Rd×d;wPV
12,wPV
21,wKQ
12,wKQ
21∈
Rd; and wPV
22, wKQ
22∈R. Then the prediction is:
byτ,q=fLSA,θ(E)(d+1),(N+1) (2)
= 
(wPV
21)⊤wPV
22EE⊤
ρ
WKQ
11
(wKQ
21)⊤
xτ,q.
4. Linear Regression
In this section, we consider the linear regression task for in-
context learning which is widely studied empirically (Garg
et al., 2022; Raventos et al., 2023; V on Oswald et al., 2023;
Akyurek et al., 2023; Bai et al., 2023) and theoretically (Ma-
hankali et al., 2023; Zhang et al., 2023b; Ahn et al., 2023;
Li et al., 2023c; Huang et al., 2023; Wu et al., 2024).
Data and task. For each task τ, we assume for any i∈[N]
tokens xτ,i,xτ,qi.i.d.∼ N (0,Λ), where Λis the covariance
matrix. We also assume a d-dimension task weight wτi.i.d.∼
N(0, Id×d)and the labels are given by yτ,i=⟨wτ,xτ,i⟩
andyτ,q=⟨wτ,xτ,q⟩.
Model and loss. We study a one-layer single-head linear
self-attention transformer (LSA) defined in Equation (1)
and we use byτ,q:=fLSA,θ(E)(d+1),(N+1)as the prediction.
We consider the mean square error (MSE) loss so that the
empirical risk over Bindependent prompts is defined as
bL(fθ) :=1
2BBX
τ=1(byτ,q− ⟨wτ,xτ,q⟩)2.
Measure model scale by rank. We first introduce a lemma
from previous work that simplifies the MSE and justifies our
3
Why Larger Language Models Do In-context Learning Differently?
measurement of the model scale. For notation simplicity,
we denote U=WKQ
11, u=wPV
22.
Lemma 4.1 (Lemma A.1 in Zhang et al. (2023b)) .Let
Γ := 
1 +1
N
Λ +1
Ntr(Λ) Id×d∈Rd×d. Let
L(fLSA,θ) = lim
B→∞bL(fLSA,θ)
=1
2Ewτ,xτ,1,...,xτ,N,xτ,qh
(byτ,q− ⟨wτ,xτ,q⟩)2i
,
˜ℓ(U, u) = tr1
2u2ΓΛUΛU⊤−uΛ2U⊤
,
we have L(fLSA,θ) =˜ℓ(U, u) +C, where Cis a constant
independent with θ.
Lemma 4.1 tells us that the loss only depends on uU. If we
consider non-zero u, w.l.o.g, letting u= 1, then we can see
that the loss only depends on U∈Rd×d,
L(fLSA,θ) = tr1
2ΓΛUΛU⊤−Λ2U⊤
.
Note that U=WKQ
11, then it is natural to measure the
size of the model by rank of U. Recall that we merge the
key matrix and the query matrix in attention together, i.e.,
WKQ= (WK)⊤WQ. Thus, a low-rank Uis equivalent
to the constraint WK,WQ∈Rr×dwhere r≪d. The
low-rank key and query matrix are practical and have been
widely studied (Hu et al., 2022; Chen et al., 2021; Bhojana-
palli et al., 2020; Fan et al., 2021; Dass et al., 2023; Shi
et al., 2023c). Therefore, we use r= rank( U)to measure
the scale of the model, i.e., larger rrepresenting larger mod-
els. To study the behavior difference under different model
scale, we will analyze Uunder different rank constraints.
4.1. Low Rank Optimal Solution
Since the token covariance matrix Λis positive semidefi-
nite symmetric, we have eigendecomposition Λ =QDQ⊤,
where Qis an orthonormal matrix containing eigenvec-
tors of ΛandDis a sorted diagonal matrix with non-
negative entries containing eigenvalues of Λ, denoting as
D= diag([ λ1, . . . , λ d]), where λ1≥ ··· ≥ λd≥0. Then,
we have the following theorem.
Theorem 4.1 (Optimal rank- rsolution for regression) .
Recall the loss function ˜ℓin Lemma 4.1. Let
U∗, u∗= argmin
U∈Rd×d,rank(U)≤r,u∈R˜ℓ(U, u).
ThenU∗=cQV∗Q⊤, u=1
c, where cis any nonzero
constant, and V∗= diag([ v∗
1, . . . , v∗
d])satisfies for any
i≤r, v∗
i=N
(N+1)λi+tr(D)and for any i > r, v∗
i= 0.Proof sketch of Theorem 4.1. We defer the full proof to Ap-
pendix B.1. The proof idea is that we can decompose the
loss function into different ranks, so we can keep the direc-
tion by their sorted “variance”, i.e.,
argmin
U∈Rd×d,rank(U)≤r,u∈R˜ℓ(U, u) =dX
i=1Tiλ2
i
v∗
i−1
Ti2
,
where Ti= 
1 +1
N
λi+tr(D)
N. We have that v∗
i≥0
for any i∈[d]and if v∗
i>0, we have v∗
i=1
Ti. Denote
g(x) =x2
1
(1+1
N)x+tr(D)
N
. We get the conclusion by
g(x)is an increasing function on [0,∞).
Theorem 4.1 gives the closed-form optimal rank- rsolution
of one-layer single-head linear self-attention transformer
learning linear regression ICL tasks. Let fLSA,θdenote the
optimal rank- rsolution corresponding to the U∗, u∗above.
In detail, the optimal rank- rsolution fLSA,θsatisfies
W∗PV=0d×d0d
0⊤
d u
,W∗KQ=U∗0d
0⊤
d0
.(3)
What hidden features does the model pay attention to?
Theorem 4.1 shows that the optimal rank- rsolution indeed
is the truncated version of the optimal full-rank solution,
keeping only the most important feature directions (i.e.,
the first reigenvectors of the token covariance matrix). In
detail, (1) for the optimal full-rank solution, we have for
anyi∈[d], v∗
i=N
(N+1)λi+tr(D); (2) for the optimal rank- r
solution, we have for any i≤r, v∗
i=N
(N+1)λi+tr(D)and
for any i > r, v∗
i= 0. That is, the small rank- rmodel
keeps only the first reigenvectors (viewed as hidden feature
directions) and does not cover the others, while larger ranks
cover more hidden features, and the large full rank model
covers all features.
Recall that the prediction depends on U∗xτ,q =
cQV∗Q⊤xτ,q; see Equation (2) and (3). So the optimal
rank-rmodel only uses the components on the first reigen-
vector directions to do the prediction in evaluations. When
there is noise distributed in all directions, a smaller model
can ignore noise and signals along less important directions
but still keep the most important directions. Then it can be
less sensitive to the noise, as empirically observed. This
insight is formalized in the next subsection.
4.2. Behavior Difference
We now formalize our insight into the behavior difference
based on our analysis on the optimal solutions. We consider
the evaluation prompt to have Mexamples (may not be
equal to the number of examples Nduring pretraining for
a general evaluation setting), and assume noise in labels to
4
Why Larger Language Models Do In-context Learning Differently?
facilitate the study of the behavior difference (our results
can be applied to the noiseless case by considering noise
levelσ= 0). Formally, the evaluation prompt is:
bE:=x1x2. . .xMxq
y1y2. . . y M 0
∈R(d+1)×(M+1)
=x1 . . . xM xq
⟨w,x1⟩+ϵ1. . .⟨w,xM⟩+ϵM 0
,
where wis the weight for the evaluation task, and for any
i∈[M], the label noise ϵii.i.d.∼ N (0, σ2).
Recall Qare eigenvectors of Λ, i.e., Λ = QDQ⊤and
D= diag([ λ1, . . . , λ d]). In practice, we can view the large
variance part of x(toprdirections in Q) as a useful signal
(like words “positive”, “negative”), and the small variance
part (bottom d−rdirections in Q) as the less important or
useless information (like words “even”, “just”).
Based on such intuition, we can decompose the evaluation
task weight waccordingly: w=Q(s+ξ), where the r-dim
truncated vector s∈Rdhassi= 0for any r < i≤d, and
the residual vector ξ∈Rdhasξi= 0 for any 1≤i≤r.
The following theorem (proved in Appendix B.2) quantifies
the evaluation loss at different model scales rwhich can
explain the scale’s effect.
Theorem 4.2 (Behavior difference for regression) .Let
w=Q(s+ξ)∈Rdwhere s, ξ∈Rdare truncated and
residual vectors defined above. The optimal rank- r
solution fLSA,θin Theorem 4.1 satisfies:
L(fLSA,θ;bE)
:=Ex1,ϵ1,...,xM,ϵM,xq
fLSA,θ(bE)− ⟨w,xq⟩2
=1
M∥s∥2
(V∗)2D3+1
M 
∥s+ξ∥2
D+σ2
tr 
(V∗)2D2
+∥ξ∥2
D+X
i∈[r]s2
iλi(λiv∗
i−1)2.
Implications. IfNis large enough with Nλr≫tr(D)
(which is practical as we usually pretrain networks on long
text), then
L(fLSA,θ;bE)≈∥ξ∥2
D+1
M 
(r+ 1)∥s∥2
D+r∥ξ∥2
D+rσ2
.
The first term ∥ξ∥2
Dis due to the residual features not cov-
ered by the network, so it decreases for larger rand becomes
0for full-rank r=d. The second term1
M(·)is significant
since we typically have limited examples in evaluation, e.g.,
M= 16≪N. Within it, (r+ 1)∥s∥2
Dcorresponds to the
firstrdirections, and rσ2corresponds to the label noise.
These increase for larger r. So there is a trade-off between
the two error terms when scaling up the model: for largerrthe first term decreases while the second term increases.
This depends on whether more signals are covered or more
noise is kept when increasing the rank r.
To further illustrate the insights, we consider the special
case when the model already covers all useful signals in the
evaluation task: w=Qs, i.e., the label only depends on
the top rfeatures (like “positive”, “negative” tokens). Our
above analysis implies that a larger model will cover more
useless features and keep more noise, and thus will have
worse performance. This is formalized in the following
theorem (proved in Appendix B.2).
Theorem 4.3 (Behavior difference for regression, special
case) .Let0≤r≤r′≤dandw=Qswhere sisr-dim
truncated vector. Denote the optimal rank- rsolution as
f1and the optimal rank- r′solution as f2. Then,
L(f2;bE)− L(f1;bE)
=1
M 
∥s∥2
D+σ2
r′X
i=r+1Nλi
(N+ 1)λi+ tr(D)2
.
Implications. By Theorem 4.3, in this case,
L(f2;bE)− L(f1;bE)≈r′−r
M∥s∥2
D
|{z}
input noise+r′−r
Mσ2
|{z}
label noise.
We can decompose the above equation to input noise and
label noise, and we know that ∥s∥2
D+σ2only depends on
the intrinsic property of evaluation data and is independent
of the model size. When we have a larger model (larger r′),
we will have a larger evaluation loss gap between the large
and small models. It means larger language models may
be easily affected by the label noise and input noise and
may have worse in-context learning ability, while smaller
models may be more robust to these noises as they only
emphasize important signals. Moreover, if we increase the
label noise scale σ2on purpose, the larger models will be
more sensitive to the injected label noise. This is consistent
with the observation in Wei et al. (2023b); Shi et al. (2023a)
and our experimental results in Section 6.
5. Sparse Parity Classification
We further consider a more sophisticated setting with non-
linear data which necessitates nonlinear models. Viewing
sentences as generated from various kinds of thoughts and
knowledge that can be represented as vectors in some hid-
den feature space, we consider the classic data model of
dictionary learning or sparse coding, which has been widely
used for text and images (Olshausen & Field, 1997; Vinje
& Gallant, 2000; Blei et al., 2003). Furthermore, beyond
linear separability, we assume the labels are given by the
5
Why Larger Language Models Do In-context Learning Differently?
(d,2)-sparse parity on the hidden feature vector, which is the
high-dimensional generalization of the classic XOR prob-
lem. Parities are a canonical family of highly non-linear
learning problems and recently have been used in many re-
cent studies on neural network learning (Daniely & Malach,
2020; Barak et al., 2022; Shi et al., 2022; 2023d).
Data and task. LetX=Rdbe the input space, and
Y={±1}be the label space. Suppose G∈Rd×dis
an unknown dictionary with dcolumns that can be regarded
as features; for simplicity, assume Gis orthonormal. Let
ϕ∈ {± 1}dbe a hidden vector that indicates the presence
of each feature. The data are generated as follows: for each
taskτ, generate two task indices tτ= (iτ, jτ)which deter-
mines a distribution Tτ; then for this task, draw examples
byϕ∼ Tτ, and setting x=Gϕ(i.e., dictionary learning
data), y=ϕiτϕjτ(i.e., XOR labels).
We now specify how to generate tτandϕ. As some of
the hidden features are more important than others, we let
A= [k]denote a subset of size kcorresponding to the
important features. We denote the important task set as
S1:=A×A\ {(l, l) :l∈A}and less important task
set as S2:= [d]×[d]\({(l, l) :l∈[d]} ∪S1). Then tτ
is drawn uniformly from S1with probability 1−pT, and
uniformly from S2with probability pT, where pT∈[0,1
2)
is the less-important task rate. For the distribution of ϕ,
we assume ϕ[d]\{iτ,jτ}is drawn uniformly from {±1}d−2,
and assume ϕ{iτ,jτ}has good correlation (measured by a
parameter γ∈(0,1
4)) with the label to facilitate learning.
Independently, we have
Pr[(ϕiτ, ϕjτ) = (1 ,1)] = 1 /4 +γ,
Pr[(ϕiτ, ϕjτ) = (1 ,−1)] = 1 /4,
Pr[(ϕiτ, ϕjτ) = (−1,1)] = 1 /4,
Pr[(ϕiτ, ϕjτ) = (−1,−1)] = 1 /4−γ.
Note that without correlation ( γ= 0), it is well-known
sparse parities will be hard to learn, so we consider γ >0.
Model. Following Wu et al. (2024), we consider the reduced
linear self-attention fLSA,θ(X,y,xq) =y⊤X
NWKQxq
(which is a reduced version of Equation (1)), and also denote
WKQasWfor simplicity. It is used as the neuron in our
two-layer multiple-head transformers:
g(X,y,xq) =X
i∈[m]aiσy⊤X
NW(i)xq
,
where σis ReLU activation, a= [a1, . . . ,am]⊤∈
[−1,1]m,W(i)∈Rd×dandmis the number of attention
heads. Denote its parameters as θ= (a,W(1), . . . ,W(m)).
This model is more complicated as it uses non-linear activa-
tion, and also has two layers with multiple heads.Measure model scale by head number. We use the at-
tention head number mto measure the model scale, as a
larger mmeans the transformer can learn more attention
patterns. We consider hinge loss ℓ(z) = max(0 ,1−z), and
the population loss with weight-decay regularization:
Lλ(g) =E[ℓ(yq·g(X,y,xq))] + λ
X
i∈[m]∥W(i)∥2
F
.
Suppose N→ ∞ and let the optimal solution of Lλ(g)be
g∗= argmin
glim
λ→0+Lλ(g).
5.1. Optimal Solution
We first introduce some notations to describe the optimal.
Letbin(·)be the integer to binary function, e.g., bin(6) =
110. Let digit( z, i)denote the digit at the i-th position
(from right to left) of z, e.g., digit(01000 ,4) = 1 . We
are now ready to characterize the optimal solution (proved
in Appendix C.1).
Theorem 5.1 (Optimal solution for parity) .Consider
k= 2ν1, d= 2ν2, and let g∗
1andg∗
2denote the optimal
solutions for m= 2(ν1+ 1) andm= 2(ν2+ 1) ,
respectively.
When 0< pT<1
4−γ
d(d−1)
2(1
4+γ)+1
4−γ,g∗
1neurons are a
subset of g∗
2neurons. Specifically, for any i∈[2(ν2+ 1)] ,
letV∗,(i)be diagonal matrix and
• For any i∈[ν2]andiτ∈[d], leta∗
i=−1and
V∗,(i)
iτ,iτ= (2 digit(bin( iτ−1), i)−1)/(4γ).
• For i=ν2+ 1and any iτ∈[d], leta∗
i= +1 and
V∗,(i)
iτ,iτ=−νj/(4γ)forg∗
j.
•Fori∈[2(ν2+ 1)]\[ν2+ 1], leta∗
i=a∗
i−ν2−1and
V∗,(i)=−V∗,(i−ν2−1).
LetW∗,(i)=GV∗,(i)G⊤. Up to permutations, g∗
2has
neurons (a∗,W∗,(1), . . . ,W∗,(m))andg∗
1has the
{1, . . . , ν 1, ν2+ 1, ν2+ 2. . . , ν 2+ν1+ 1,2ν2+ 2}-th
neurons of g∗
2.
Proof sketch of Theorem 5.1. The proof is challenging as
the non-linear model and non-linear data. We defer the full
proof to Appendix C.1. The high-level intuition is transfer-
ring the optimal solution to patterns covering problems. For
small pT, the model will “prefer” to cover all patterns in
S1first. When the model becomes larger, by checking the
sufficient and necessary conditions, it will continually learn
to cover non-important features. Thus, the smaller model
will mainly focus on important features, while the larger
model will focus on all features.
6
Why Larger Language Models Do In-context Learning Differently?
Example for Theorem 5.1. When ν2= 3, the optimal has
a1=a2=a3=−1,a4= +1 and,
V(1)= 1/4γ·diag([−1,+1,−1,+1,−1,+1,−1,+1])
V(2)= 1/4γ·diag([−1,−1,+1,+1,−1,−1,+1,+1])
V(3)= 1/4γ·diag([−1,−1,−1,−1,+1,+1,+1,+1])
V(4)= 3/4γ·diag([−1,−1,−1,−1,−1,−1,−1,−1])
andV(i+4)=−V(i),ai+4=aifori∈[4].
On the other hand, the optimal g∗
1forν1= 1 has the
{1,4,5,8}-th neurons of g∗
2.
By carefully checking, we can see that the neurons in g∗
1
(i.e., the {1,4,5,8}-th neurons of g∗
2) are used for parity
classification task from S1, i.e, label determined by the first
k= 2ν1= 2 dimensions. With the other neurons (i.e.,
the{2,3,6,7}-th neurons of g∗
2),g∗
2can further do parity
classification on the task from S2, label determined by any
two dimensions other than the first two dimensions.
What hidden features does the model pay attention to?
Theorem 5.1 gives the closed-form optimal solution of two-
layer multiple-head transformers learning sparse-parity ICL
tasks. It shows the optimal solution of the smaller model
indeed is a sub-model of the larger optimal model. In detail,
the smaller model will mainly learn all important features,
while the larger model will learn more features. This again
shows a trade-off when increasing the model scale: larger
models can learn more hidden features which can be ben-
eficial if these features are relevant to the label, but also
potentially keep more noise which is harmful.
5.2. Behavior Difference
Similar to Theorem 4.3, to illustrate our insights, we will
consider a setting where the smaller model learns useful fea-
tures for the evaluation task while the larger model covers
extra features. That is, for evaluation, we uniformly draw
a task tτ= (iτ, jτ)from S1, and then draw Msamples to
form the evaluation prompt in the same way as during pre-
training. To present our theorem (proved in Appendix C.2
using Theorem 5.1), we introduce some notations. Let
D1=
diag(V∗,(1)), . . . , diag(V∗,(ν1)),diag(V∗,(ν2+1)),
. . . ,diag(V∗,(ν2+ν1+1)),diag(V∗,(2ν2+2))
∈Rd×2(ν1+1)
D2=h
diag(V∗,(1)), . . . , diag(V∗,(2ν2+2))i
∈Rd×2(ν2+1),
where for any i∈[2(ν2+ 1)] ,V∗,(i)is defined in Theo-
rem 5.1. Let ˆϕτ,q∈Rdsatisfy ˆϕτ,q,iτ=ϕτ,q,iτ,ˆϕτ,q,j τ=
ϕτ,q,j τand all other entries being zero. For a matrix Zand
a vector v, letPZdenote the projection of vto the space of
Z, i.e., PZ(v) =Z(Z⊤Z)−1Z⊤v.Theorem 5.2 (Behavior difference for parity) .Assume the
same condition as Theorem 5.1. For j∈ {1,2}, Letθj
denote the parameters of g∗
j. Forl∈[M], letξlbe
uniformly drawn from {±1}d, andΞ =P
l∈[M]ξl
M. Then,
for any δ∈(0,1), with probability at least 1−δover the
randomness of test data, we have
g∗
j(Xτ,yτ,xτ,q) =h(θj,2γˆϕτ,q+PDj(Ξ)) + ϵj
:=X
i∈[m]a∗
iσ
diag
V∗,(i)⊤
2γˆϕτ,q+PDj(Ξ)
+ϵj
where ϵj=Oq
νj
Mlog1
δ
and we have
•2γˆϕτ,qis the signal useful for prediction: 0 =
ℓ(yq·h(θ1,2γˆϕτ,q)) =ℓ(yq·h(θ2,2γˆϕτ,q)).
•PD1(Ξ)) andPD2(Ξ)) is noise not related to labels,
andE[∥PD1(Ξ))∥2
2]
E[∥PD2(Ξ))∥2
2]=ν1+1
ν2+1.
Implications. Theorem 5.2 shows that during evaluation,
we can decompose the input into two parts: signal and noise.
Both the larger model and smaller model can capture the
signal part well. However, the smaller model has a much
smaller influence from noise than the larger model, i.e., the
ratio isν1+1
ν2+1. The reason is that smaller models emphasize
important hidden features while larger ones cover more
hidden features, and thus, smaller models are more robust
to noise while larger ones are easily distracted, leading to
different ICL behaviors. This again sheds light on where
transformers pay attention to and how that affects ICL.
Remark 5.1.Here, we provide a detailed intuition about
Theorem 5.2. Ξis the input noise. When we only care
about the noise part, we can rewrite the smaller model
asg1=h(θ1, PD1(Ξ)), and the larger model as g2=
h(θ2, PD2(Ξ)), where they share the same hfunction.
Our conclusion says that E[∥PD1(Ξ)∥2
2]/E[∥PD2(Ξ)∥2
2] =
(ν1+1)/(ν2+1), which means the smaller model’s “effect”
input noise is smaller than the larger model’s “effect” input
noise. Although their original input noise is the same, as the
smaller model only focuses on limited features, the smaller
model will ignore part of the noise, and the “effect” input
noise is small. However, the larger model is the opposite.
6. Experiments
Brilliant recent work (Wei et al., 2023b) runs intensive and
thorough experiments to show that larger language models
do in-context learning differently. Following their idea,
we conduct similar experiments on binary classification
datasets, which is consistent with our problem setting in the
parity case, to support our theory statements.
7
Why Larger Language Models Do In-context Learning Differently?
0.020.040.060.080.0100.0Accuracy(%)
glue-rte
0.020.040.060.080.0100.0
glue-sst2
0.020.040.060.080.0100.0
glue-qqp
025 50 75100
% flipped_labels0.020.040.060.080.0100.0Accuracy(%)
glue-wnli
025 50 75100
% flipped_labels0.020.040.060.080.0100.0
subj
025 50 75100
% flipped_labels0.020.040.060.080.0100.0
Averageopen_llama-2-3b
llama-2-7b-chat
llama-2-13b-chat
llama-2-70b-chat
Random
Figure 1. Larger models are easier to be affected by noise (flipped labels) and override pretrained biases than smaller models for different
datasets and model families (chat/with instruct turning). Accuracy is calculated over 1000 evaluation prompts per dataset and over 5 runs
with different random seeds for each evaluation, using M= 16 in-context exemplars.
0.020.040.060.080.0100.0Accuracy(%)
glue-rte
0.020.040.060.080.0100.0
glue-sst2
0.020.040.060.080.0100.0
glue-qqp
025 50 75100
% flipped_labels0.020.040.060.080.0100.0Accuracy(%)
glue-wnli
025 50 75100
% flipped_labels0.020.040.060.080.0100.0
subj
025 50 75100
% flipped_labels0.020.040.060.080.0100.0
Averageopen_llama-2-3b
llama-2-7b
llama-2-13b
llama-2-70b
Random
Figure 2. Larger models are easier to be affected by noise (flipped labels) and override pretrained biases than smaller models for different
datasets and model families (original/without instruct turning). Accuracy is calculated over 1000 evaluation prompts per dataset and over
5 runs with different random seeds for each evaluation, using M= 16 in-context exemplars.
Experimental setup. Following the experimental protocols
in Wei et al. (2023b); Min et al. (2022), we conduct experi-
ments on five prevalent NLP tasks, leveraging datasets from
GLUE (Wang et al., 2018) tasks and Subj (Conneau & Kiela,
2018). Our experiments utilize various sizes of the Llama
model families (Touvron et al., 2023a;b): 3B, 7B, 13B, 70B.
We follow the prior work on in-context learning (Wei et al.,
2023b) and use M= 16 in-context exemplars. We aim to
assess the models’ ability to use inherent semantic biases
from pretraining when facing in-context examples. As part
of this experiment, we introduce noise by inverting an esca-
lating percentage of in-context example labels. To illustrate,
a 100% label inversion for the SST-2 dataset implies that
every “positive” exemplar is now labeled “negative”. Note
that while we manipulate the in-context example labels, the
evaluation sample labels remain consistent. We use the
same templates as (Min et al., 2021), a sample evaluation
for SST-2 when M= 2:sentence: show us a good time
The answer is positive.
sentence: as dumb and cheesy
The answer is negative.
sentence: it ’s a charming and often
affecting journey
The answer is
6.1. Behavior Difference
Figure 1 shows the result of model performance (chat/with
instruct turning) across all datasets with respect to the pro-
portion of labels that are flipped. When 0% label flips, we
observe that larger language models have better in-context
8
Why Larger Language Models Do In-context Learning Differently?
0 20 40 60 80 1000.00.20.40.6Label-input attentionCorrect + Relevant
0 20 40 60 80 1000.00.20.40.6Correct + Irrelevant
0 20 40 60 80 100
Indices of the prompts0.00.20.40.6Label-input attentionWrong + Relevant
0 20 40 60 80 100
Indices of the prompts0.00.20.40.6Wrong + Irrelevantmodels
Llama-2-13b-hf
Llama-2-70b-hf
Figure 3. The magnitude of attention between the labels and input
sentences in Llama 2-13b and 70b on 100 evaluation prompts;
see the main text for the details. x-axis: indices of the prompts.
y-axis: the norm of the last row of attention maps in the final layer.
Correct: original label; wrong: flipped label; relevant: original
input sentence; irrelevant: irrelevant sentence from other datasets.
The results show that larger models focus on both sentences, while
smaller models only focus on relevant sentences.
abilities. On the other hand, the performance decrease fac-
ing noise is more significant for larger models. As the per-
centage of label alterations increases, which can be viewed
as increasing label noise σ2, the performance of small mod-
els remains flat and seldom is worse than random guess-
ing while large models are easily affected by the noise, as
predicted by our analysis. These results indicate that large
models can override their pretraining biases in-context input-
label correlations, while small models may not and are more
robust to noise. This observation aligns with the findings in
Wei et al. (2023b) and our analysis.
We can see a similar or even stronger phenomenon in Fig-
ure 2: larger models are more easily affected by noise
(flipped labels) and override pretrained biases than smaller
models for the original/without instruct turning version (see
the “Average” sub-figure). On the one hand, we conclude
that both large base models and large chat models suffer
from ICL robustness issues. On the other hand, this is also
consistent with recent work suggesting that instruction tun-
ing will impair LLM’s in-context learning capability.
6.2. Ablation Study
To further verify our analysis, we provide an ablation study.
We concatenate an irrelevant sentence from GSM-IC (Shi
et al., 2023a) to an input-label pair sentence from SST-2
in GLUE dataset. We use “correct” to denote the origi-
nal label and “wrong” to denote the flipped label. Then,
we measure the magnitude of correlation between label-
input, by computing the norm of the last row of attentionmaps across all heads in the final layer. We do this be-
tween “correct”/“wrong” labels and the original/irrelevant
inserted sentences. Figure 3 shows the results on 100 evalu-
ation prompts; for example, the subfigure Correct+Relevant
shows the correlation magnitude between the “correct” label
and the original input sentence in each prompt. The results
show that the small model Llama 2-13b mainly focuses on
the relevant part (original input) and may ignore the irrele-
vant sentence, while the large model Llama 2-70b focuses
on both sentences. This well aligns with our analysis.
7. More Discussions about Noise
There are three kinds of noise covered in our analysis:
Pretraining noise. We can see it as toxic or harmful pre-
training data on the website (noisy training data). The model
will learn these features and patterns. It is covered by ξin
the linear regression case and S2in the parity case.
Input noise during inference. We can see it as natural noise
as the user’s wrong spelling or biased sampling. It is a finite
sampling error as xdrawn from the Gaussian distribution
for the linear regression case and a finite sampling error as
xdrawn from a uniform distribution for the parity case.
Label noise during inference. We can see it as adversarial
examples, or misleading instructions, e.g., deliberately let-
ting a model generate a wrong fact conclusion or harmful
solution, e.g., poison making. It is σin the linear regression
case and S2in the parity case.
For pretraining noise, it will induce the model to learn noisy
or harmful features. During inference, for input noise and
label noise, the larger model will pay additional attention to
these noisy or harmful features in the input and label pair,
i.e.,y·x, so that the input and label noise may cause a large
perturbation in the final results. If there is no pretraining
noise, then the larger model will have as good robustness
as the smaller model. Also, if there is no input and label
noise, the larger model will have as good robustness as the
smaller model. The robustness gap only happens when both
pretraining noise and inference noise exist simultaneously.
8. Conclusion
In this work, we answered our research question: why do
larger language models do in-context learning differently?
Our theoretical study showed that smaller models empha-
size important hidden features while larger ones cover more
hidden features, and thus the former are more robust to noise
while the latter are more easily distracted, leading to dif-
ferent behaviors during in-context learning. Our empirical
results provided positive support for the theoretical analysis.
Our findings can help improve understanding of LLMs and
ICL, and better training and application of these models.
9
why larger language models do in-context learning differently?
acknowledgements
the work is partially supported by air force grant fa9550-
18-1-0166, the national science foundation (nsf) grants
2008559-iis, 2023239-dms, and ccf-2046710.
impact statement
our work aims to improve the understanding of the in-
context learning mechanism and to inspire efficient and safe
use of icl. our paper is purely theoretical and empirical in
nature and thus we foresee no immediate negative ethical
impact. we hope our work will inspire effective algorithm
design and promote a better understanding of large language
model learning mechanisms.

