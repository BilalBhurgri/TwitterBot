# High-Resolution Image Synthesis with Latent Diffusion Models

## 1. Introduction

Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands. Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers . In contrast, the promising results of GANs have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models , which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive Figure . Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K validation set, evaluated at 512 2 px. We denote the spatial downsampling factor by f . Reconstruction FIDs and PSNR are calculated on ImageNet-val. ; see also results in image synthesis and beyond , and define the state-of-the-art in class-conditional image synthesis and super-resolution . Moreover, even unconditional DMs can readily be applied to tasks such as inpainting and colorization or stroke-based synthesis , in contrast to other types of generative models . Being likelihood-based models, they do not exhibit mode-collapse and training instabilities as GANs and, by heavily exploiting parameter sharing, they can model highly complex distributions of natural images without involving billions of parameters as in AR models . Democratizing High-Resolution Image Synthesis DMs belong to the class of likelihood-based models, whose mode-covering behavior makes them prone to spend excessive amounts of capacity (and thus compute resources) on modeling imperceptible details of the data . Although the reweighted variational objective aims to address this by undersampling the initial denoising steps, DMs are still computationally demanding, since training and evaluating such a model requires repeated function evaluations (and gradient computations) in the high-dimensional space of RGB images. As an example, training the most powerful DMs often takes hundreds of GPU days (e.g. 150 -1000 V100 days in ) and repeated evaluations on a noisy version of the input space render also inference expensive, so that producing 50k samples takes approximately 5 days on a single A100 GPU. This has two consequences for the research community and users in general: Firstly, training such a model requires massive computational resources only available to a small fraction of the field, and leaves a huge carbon footprint . Secondly, evaluating an already trained model is also expensive in time and memory, since the same model architecture must run sequentially for a large number of steps (e.g. 25 -1000 steps in ).

To increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.

Departure to Latent Space Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a perceptual compression stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (semantic compression). We thus aim to first find a perceptually equivalent, but computationally more suitable space, in which we will train diffusion models for high-resolution image synthesis.

Following common practice , we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work , we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class Latent Diffusion Models (LDMs).

A notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks . This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM's UNet backbone and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3.

In sum, our work makes the following contributions:

(i) In contrast to purely transformer-based approaches , our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. ) and (b) can be efficiently Figure . Illustrating perceptual and semantic compression: Most bits of a digital image correspond to imperceptible details. While DMs allow to suppress this semantically meaningless information by minimizing the responsible loss term, gradients (during training) and the neural network backbone (training and inference) still need to be evaluated on all pixels, leading to superfluous computations and unnecessarily expensive optimization and inference. We propose latent diffusion models (LDMs) as an effective generative model and a separate mild compression stage that only eliminates imperceptible details. Data and images from .

applied to high-resolution synthesis of megapixel images.

(ii) We achieve competitive performance on multiple tasks (unconditional image synthesis, inpainting, stochastic super-resolution) and datasets while significantly lowering computational costs. Compared to pixel-based diffusion approaches, we also significantly decrease inference costs.

(iii) We show that, in contrast to previous work which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.

(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of ∼ 1024 2 px.

(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models.

(vi) Finally, we release pretrained latent diffusion and autoencoding models at https : / / github . com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs .

## Two-Stage Image Synthesis

To mitigate the shortcomings of individual generative approaches, a lot of research has gone into combining the strengths of different methods into more efficient and performant models via a two stage approach. VQ-VAEs use autoregressive models to learn an expressive prior over a discretized latent space. extend this approach to text-to-image generation by learning a joint distributation over discretized image and text representations. More generally, uses conditionally invertible networks to provide a generic transfer between latent spaces of diverse domains. Different from VQ-VAEs, VQGANs employ a first stage with an adversarial and perceptual objective to scale autoregressive transformers to larger images. However, the high compression rates required for feasible ARM training, which introduces billions of trainable parameters , limit the overall performance of such ap-proaches and less compression comes at the price of high computational cost . Our work prevents such tradeoffs, as our proposed LDMs scale more gently to higher dimensional latent spaces due to their convolutional backbone. Thus, we are free to choose the level of compression which optimally mediates between learning a powerful first stage, without leaving too much perceptual compression up to the generative diffusion model while guaranteeing highfidelity reconstructions (see Fig. ).

While approaches to jointly or separately learn an encoding/decoding model together with a score-based prior exist, the former still require a difficult weighting between reconstruction and generative capabilities and are outperformed by our approach (Sec. 4), and the latter focus on highly structured images such as human faces.

## 3. Method

To lower the computational demands of training diffusion models towards high-resolution image synthesis, we observe that although diffusion models allow to ignore perceptually irrelevant details by undersampling the corresponding loss terms , they still require costly function evaluations in pixel space, which causes huge demands in computation time and energy resources.

We propose to circumvent this drawback by introducing an explicit separation of the compressive from the generative learning phase (see Fig. ). To achieve this, we utilize an autoencoding model which learns a space that is perceptually equivalent to the image space, but offers significantly reduced computational complexity.

Such an approach offers several advantages: (i) By leaving the high-dimensional image space, we obtain DMs which are computationally much more efficient because sampling is performed on a low-dimensional space. (ii) We exploit the inductive bias of DMs inherited from their UNet architecture , which makes them particularly effective for data with spatial structure and therefore alleviates the need for aggressive, quality-reducing compression levels as required by previous approaches . (iii) Finally, we obtain general-purpose compression models whose latent space can be used to train multiple generative models and which can also be utilized for other downstream applications such as single-image CLIP-guided synthesis .

## 3.1. Perceptual Image Compression

Our perceptual compression model is based on previous work and consists of an autoencoder trained by combination of a perceptual loss and a patch-based adversarial objective . This ensures that the reconstructions are confined to the image manifold by enforcing local realism and avoids bluriness introduced by relying solely on pixel-space losses such as L 2 or L 1 objectives.

More precisely, given an image x R HW 3 in RGB space, the encoder E encodes x into a latent representa-tion z = , and the decoder D reconstructs the image from the latent, giving x = = ), where z R hwc . Importantly, the encoder downsamples the image by a factor f = H/h = W/w, and we investigate different downsampling factors f = 2 m , with m N.

In order to avoid arbitrarily high-variance latent spaces, we experiment with two different kinds of regularizations. The first variant, KL-reg., imposes a slight KL-penalty towards a standard normal on the learned latent, similar to a VAE , whereas VQ-reg. uses a vector quantization layer within the decoder. This model can be interpreted as a VQGAN but with the quantization layer absorbed by the decoder. Because our subsequent DM is designed to work with the two-dimensional structure of our learned latent space z = , we can use relatively mild compression rates and achieve very good reconstructions. This is in contrast to previous works , which relied on an arbitrary 1D ordering of the learned space z to model its distribution autoregressively and thereby ignored much of the inherent structure of z. Hence, our compression model preserves details of x better (see Tab. 8). The full objective and training details can be found in the supplement.

## 3.2. Latent Diffusion Models

Diffusion Models are probabilistic models designed to learn a data distribution p(x) by gradually denoising a normally distributed variable, which corresponds to learning the reverse process of a fixed Markov Chain of length T . For image synthesis, the most successful models rely on a reweighted variant of the variational lower bound on p(x), which mirrors denoising score-matching . These models can be interpreted as an equally weighted sequence of denoising autoencoders theta (x t , t); t = 1 . . . T , which are trained to predict a denoised variant of their input x t , where x t is a noisy version of the input x. The corresponding objective can be simplified to (Sec. B)

with t uniformly sampled from {1, . . . , T }. Generative Modeling of Latent Representations With our trained perceptual compression models consisting of E and D, we now have access to an efficient, low-dimensional latent space in which high-frequency, imperceptible details are abstracted away. Compared to the high-dimensional pixel space, this space is more suitable for likelihood-based generative models, as they can now (i) focus on the important, semantic bits of the data and (ii) train in a lower dimensional, computationally much more efficient space.

Unlike previous work that relied on autoregressive, attention-based transformer models in a highly compressed, discrete latent space , we can take advantage of image-specific inductive biases that our model offers. This includes the ability to build the underlying UNet primarily from 2D convolutional layers, and further focusing the objective on the perceptually most relevant bits using the reweighted bound, which now reads

The neural backbone theta (•, t) of our model is realized as a time-conditional UNet . Since the forward process is fixed, z t can be efficiently obtained from E during training, and samples from p(z) can be decoded to image space with a single pass through D.

## 3.3. Conditioning Mechanisms

Similar to other types of generative models , diffusion models are in principle capable of modeling conditional distributions of the form p(z|y). This can be implemented with a conditional denoising autoencoder theta (z t , t, y) and paves the way to controlling the synthesis process through inputs y such as text , semantic maps or other image-to-image translation tasks .

In the context of image synthesis, however, combining the generative power of DMs with other types of conditionings beyond class-labels or blurred variants of the input image is so far an under-explored area of research.

We turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism , which is effective for learning attention-based models of various input modalities . To pre-process y from various modalities (such as language prompts) we introduce a domain specific encoder tau theta that projects y to an intermediate representation tau theta (y) R M dtau , which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing Attention

Here,

. Samples from LDMs trained on CelebAHQ , FFHQ , LSUN-Churches , LSUN-Bedrooms and classconditional ImageNet , each with a resolution of 256 256. Best viewed when zoomed in. For more samples cf . the supplement.

K R ddtau are learnable projection matrices . See Fig. for a visual depiction.

Based on image-conditioning pairs, we then learn the conditional LDM via

where both tau theta and theta are jointly optimized via Eq. 3. This conditioning mechanism is flexible as tau theta can be parameterized with domain-specific experts, e.g. (unmasked) transformers when y are text prompts (see Sec. 4.3.1)

## 4. Experiments

LDMs provide means to flexible and computationally tractable diffusion based image synthesis of various image modalities, which we empirically show in the following. Firstly, however, we analyze the gains of our models compared to pixel-based diffusion models in both training and inference. Interestingly, we find that LDMs trained in VQregularized latent spaces sometimes achieve better sample quality, even though the reconstruction capabilities of VQregularized first stage models slightly fall behind those of their continuous counterparts, cf . Tab. 8. A visual comparison between the effects of first stage regularization schemes on LDM training and their generalization abilities to resolutions &gt; 256 2 can be found in Appendix D.1. In E.2 we list details on architecture, implementation, training and evaluation for all results presented in this section.

## 4.1. On Perceptual Compression Tradeoffs

This section analyzes the behavior of our LDMs with different downsampling factors f {1, 2, 4, 8, 16, 32} (abbreviated as LDM-f , where LDM-1 corresponds to pixel-based DMs). To obtain a comparable test-field, we fix the computational resources to a single NVIDIA A100 for all experiments in this section and train all models for the same number of steps and with the same number of parameters.

Tab. 8 shows hyperparameters and reconstruction performance of the first stage models used for the LDMs com-pared in this section. Fig. shows sample quality as a function of training progress for 2M steps of class-conditional models on the ImageNet dataset. We see that, i) small downsampling factors for LDM-{1,2} result in slow training progress, whereas ii) overly large values of f cause stagnating fidelity after comparably few training steps. Revisiting the analysis above (Fig. and 2) we attribute this to i) leaving most of perceptual compression to the diffusion model and ii) too strong first stage compression resulting in information loss and thus limiting the achievable quality. LDM-{4-16} strike a good balance between efficiency and perceptually faithful results, which manifests in a significant FID gap of 38 between pixel-based diffusion (LDM-1) and LDM-8 after 2M training steps.

In Fig. , we compare models trained on CelebA-HQ and ImageNet in terms sampling speed for different numbers of denoising steps with the DDIM sampler and plot it against FID-scores . LDM-{4-8} outperform models with unsuitable ratios of perceptual and conceptual compression. Especially compared to pixel-based LDM-1, they achieve much lower FID scores while simultaneously significantly increasing sample throughput. Complex datasets such as ImageNet require reduced compression rates to avoid reducing quality. In summary, LDM-4 and -8 offer the best conditions for achieving high-quality synthesis results.

## 4.2. Image Generation with Latent Diffusion

We train unconditional models of 256 2 images on CelebA-HQ , FFHQ , LSUN-Churches and -Bedrooms and evaluate the i) sample quality and ii) their coverage of the data manifold using ii) FID and ii) Precision-and-Recall . Tab. 1 summarizes our results. On CelebA-HQ, we report a new state-of-the-art FID of 5.11, outperforming previous likelihood-based models as well as GANs. We also outperform LSGM where a latent diffusion model is trained jointly together with the first stage. In contrast, we train diffusion models in a fixed space Text-to-Image Synthesis on LAION. 1.45B Model.

'A street sign that reads "Latent Diffusion" ' 'A zombie in the style of Picasso' 'An image of an animal half mouse half octopus' 'An illustration of a slightly conscious neural network' 'A painting of a squirrel eating a burger' 'A watercolor painting of a chair that looks like an octopus' 'A shirt with the inscription: "I love generative models!" ' Figure . Samples for user-defined text prompts from our model for text-to-image synthesis, LDM-8 (KL), which was trained on the LAION database. Samples generated with 200 DDIM steps and eta = 1.0. We use unconditional guidance with s = 10.0. and avoid the difficulty of weighing reconstruction quality against learning the prior over the latent space, see Fig. .

We outperform prior diffusion based approaches on all but the LSUN-Bedrooms dataset, where our score is close to ADM , despite utilizing half its parameters and requiring 4-times less train resources (see Appendix E.3.5). Table 2. Evaluation of text-conditional image synthesis on the 256 256-sized MS-COCO [51] dataset: with 250 DDIM [84]

steps our model is on par with the most recent diffusion and autoregressive methods despite using significantly less parameters. † / * :Numbers from [109]/ Moreover, LDMs consistently improve upon GAN-based methods in Precision and Recall, thus confirming the advantages of their mode-covering likelihood-based training objective over adversarial approaches. In Fig. we also show qualitative results on each dataset. 4.3. Conditional Latent Diffusion 4.3.1 Transformer Encoders for LDMs By introducing cross-attention based conditioning into

LDMs we open them up for various conditioning modalities previously unexplored for diffusion models. For textto-image image modeling, we train a 1.45B parameter KL-regularized LDM conditioned on language prompts on LAION-400M . We employ the BERT-tokenizer and implement tau theta as a transformer to infer a latent code which is mapped into the UNet via (multi-head) crossattention (Sec. 3.3). This combination of domain specific experts for learning a language representation and visual synthesis results in a powerful model, which generalizes well to complex, user-defined text prompts, cf . Fig. and .

For quantitative analysis, we follow prior work and evaluate text-to-image generation on the MS-COCO validation set, where our model improves upon powerful AR and GAN-based methods, cf . Tab. 2. We note that applying classifier-free diffusion guidance greatly boosts sample quality, such that the guided LDM-KL-8-G is on par with the recent state-of-the-art AR and diffusion models for text-to-image synthesis, while substantially reducing parameter count. To further analyze the flexibility of the cross-attention based conditioning mechanism we also train models to synthesize images based on semantic layouts on OpenImages , and finetune on COCO , see Method FID↓ IS↑ Precision↑ Recall↑ Nparams BigGan-deep [3] 6.95 203.62.6 0.87 0.28 340M -ADM [15] 10.94 100.98 0.69 0.63 554M 250 DDIM steps ADM-G [15] 4.59 186.7 0.82 0.52 608M 250 DDIM steps LDM-4 (ours) 10.56 103.491.24 0.71 0.62 400M 250 DDIM steps LDM-4- 3.60 247.675.59 0.87 0.48 400M 250 steps, .f.g , s = 1.5

Table . Comparison of a class-conditional ImageNet LDM with recent state-of-the-art methods for class-conditional image generation on ImageNet . A more detailed comparison with additional baselines can be found in D.4, Tab. 10 and F. c.f.g. denotes classifier-free guidance with a scale s as proposed in .

purpose image-to-image translation models. We use this to train models for semantic synthesis, super-resolution (Sec. 4.4) and inpainting (Sec. 4.5). For semantic synthesis, we use images of landscapes paired with semantic maps and concatenate downsampled versions of the semantic maps with the latent image representation of a f = 4 model (VQ-reg., see Tab. 8). We train on an input resolution of 256 2 (crops from 384 2 ) but find that our model generalizes to larger resolutions and can generate images up to the megapixel regime when evaluated in a convolutional manner (see Fig. ). We exploit this behavior to also apply the super-resolution models in Sec. 4.4 and the inpainting models in Sec. 4.5 to generate large images between 512 2 and 1024 2 . For this application, the signal-to-noise ratio (induced by the scale of the latent space) significantly affects the results. In Sec. D.1 we illustrate this when learning an LDM on (i) the latent space as provided by a f = 4 model (KL-reg., see Tab. 8), and (ii) a rescaled version, scaled by the component-wise standard deviation. The latter, in combination with classifier-free guidance , also enables the direct synthesis of &gt; 256 2 images for the text-conditional LDM-KL-8-G as in Fig. .

## 4.4. Super-Resolution with Latent Diffusion

LDMs can be efficiently trained for super-resolution by diretly conditioning on low-resolution images via concatenation (cf . Sec. 3.3). In a first experiment, we follow SR3 [72] and fix the image degradation to a bicubic interpolation with 4-downsampling and train on ImageNet following SR3's data processing pipeline. We use the f = 4 autoencoding model pretrained on OpenImages (VQ-reg., cf . Tab. 8) and concatenate the low-resolution conditioning y and the inputs to the UNet, i.e. tau theta is the identity. Our qualitative and quantitative results (see Fig. and Tab. 5) show competitive performance and LDM-SR outperforms SR3 in FID while SR3 has a better IS. A simple image regression model achieves the highest PSNR and SSIM scores; however these metrics do not align well with human perception and favor blurriness over imperfectly aligned high frequency details . Further, we conduct a user study comparing the pixel-baseline with LDM-SR. We follow SR3 where human subjects were shown a low-res image in between two high-res images and asked for preference. The results in Tab. 4 affirm the good performance of LDM-SR. PSNR and SSIM can be pushed by using a post-hoc guiding mechanism and we implement this image-based guider via a perceptual loss, see Sec. D.6.

## SR on ImageNet Inpainting on Places

User Study Pixel-DM (f

1) LDM-4 LAMA [88] LDM-4 Task 1: Preference vs GT ↑ 16.0% 30.4% 13.6% 21.0% Task 2: Preference Score ↑ 29.4% 70.6% 31.9% 68.1% Table 6. Assessing inpainting efficiency. † : Deviations from Fig. 7 due to varying GPU settings/batch sizes cf . the supplement.

## 4.5. Inpainting with Latent Diffusion

Inpainting is the task of filling masked regions of an image with new content either because parts of the image are are corrupted or to replace existing but undesired content within the image. We evaluate how our general approach for conditional image generation compares to more specialized, state-of-the-art approaches for this task. Our evaluation follows the protocol of LaMa , a recent inpainting model that introduces a specialized architecture relying on Fast Fourier Convolutions . The exact training &amp; evaluation protocol on Places is described in Sec. E.2.2.

We first analyze the effect of different design choices for the first stage. In particular, we compare the inpainting efficiency of LDM-1 (i.e. a pixel-based conditional DM) with LDM-4, for both KL and VQ regularizations, as well as VQ-LDM-4 without any attention in the first stage (see Tab. 8), where the latter reduces GPU memory for decoding at high resolutions. For comparability, we fix the number of parameters for all models. Tab. 6 reports the training and sampling throughput at resolution 256 2 and 512 2 , the total training time in hours per epoch and the FID score on the validation split after six epochs. Overall, we observe a speed-up of at least 2.7 between pixel-and latent-based diffusion models while improving FID scores by a factor of at least 1.6.

The comparison with other inpainting approaches in Tab. 7 shows that our model with attention improves the overall image quality as measured by FID over that of . LPIPS between the unmasked images and our samples is slightly higher than that of . We attribute this to only producing a single result which tends to recover more of an average image compared to the diverse results produced by our LDM cf . Fig. . Additionally in a user study (Tab. 4) human subjects favor our results over those of .

Based on these initial results, we also trained a larger diffusion model (big in Tab. 7) in the latent space of the VQregularized first stage without attention. Following , the UNet of this diffusion model uses attention layers on three levels of its feature hierarchy, the BigGAN residual block for up-and downsampling and has 387M parameters

## 5. Limitations &amp; Societal Impact

Limitations While LDMs significantly reduce computational requirements compared to pixel-based approaches, their sequential sampling process is still slower than that of GANs. Moreover, the use of LDMs can be questionable when high precision is required: although the loss of image quality is very small in our f = 4 autoencoding models (see Fig. ), their reconstruction capability can become a bottleneck for tasks that require fine-grained accuracy in pixel space. We assume that our superresolution models (Sec. 4.4) are already somewhat limited in this respect.

Societal Impact Generative models for media like imagery are a double-edged sword: On the one hand, they enable various creative applications, and in particular approaches like ours that reduce the cost of training and inference have the potential to facilitate access to this technology and democratize its exploration. On the other hand, it also means that it becomes easier to create and disseminate manipulated data or spread misinformation and spam.

In particular, the deliberate manipulation of images ("deep fakes") is a common problem in this context, and women in particular are disproportionately affected by it . Generative models can also reveal their training data , which is of great concern when the data contain sensitive or personal information and were collected without explicit consent. However, the extent to which this also applies to DMs of images is not yet fully understood.

Finally, deep learning modules tend to reproduce or exacerbate biases that are already present in the data . While diffusion models achieve better coverage of the data distribution than e.g. GAN-based approaches, the extent to which our two-stage approach that combines adversarial training and a likelihood-based objective misrepresents the data remains an important research question.

For a more general, detailed discussion of the ethical considerations of deep generative models, see e.g. .

## 6. Conclusion

We have presented latent diffusion models, a simple and efficient way to significantly improve both the training and sampling efficiency of denoising diffusion models without degrading their quality. Based on this and our crossattention conditioning mechanism, our experiments could demonstrate favorable results compared to state-of-the-art methods across a wide range of conditional image synthesis tasks without task-specific architectures.

## A. Changelog

Here we list changes between this version () of the paper and the previous version, i.e. .

• We updated the results on text-to-image synthesis in Sec. 4.3 which were obtained by training a new, larger model . This also includes a new comparison to very recent competing methods on this task that were published on arXiv at the same time as ( ) or after ( ) the publication of our work.

• We updated results on class-conditional synthesis on ImageNet in Sec. 4.1, Tab. 3 (see also Sec. D.4) obtained by retraining the model with a larger batch size. The corresponding qualitative results in Fig. and Fig. were also updated. Both the updated text-to-image and the class-conditional model now use classifier-free guidance as a measure to increase visual fidelity.

• We conducted a user study (following the scheme suggested by Saharia et al ) which provides additional evaluation for our inpainting (Sec. 4.5) and superresolution models (Sec. 4.4).

• Added Fig. to the main paper, moved Fig. to the appendix, added Fig. to the appendix.

## B. Detailed Information on Denoising Diffusion Models

Diffusion models can be specified in terms of a signal-to-noise ratio SN =

consisting of sequences (alpha t ) T t=1 and (sigma t ) T t=1 which, starting from a data sample x 0 , define a forward diffusion process q as

with the Markov structure for s &lt; t:

Denoising diffusion models are generative models p(x 0 ) which revert this process with a similar Markov structure running backward in time, i.e. they are specified as

The evidence lower bound (ELBO) associated with this model then decomposes over the discrete time steps as

The prior p(x T ) is typically choosen as a standard normal distribution and the first term of the ELBO then depends only on the final signal-to-noise ratio SN. To minimize the remaining terms, a common choice to parameterize p(x t-1 |x t ) is to specify it in terms of the true posterior q(x t-1 |x t , x 0 ) but with the unknown x 0 replaced by an estimate x theta (x t , t) based on the current step x t . This gives p(x t-1 |x t ) := q(x t-1 |x t , x theta (x t , t))

where the mean can be expressed as

In this case, the sum of the ELBO simplify to

Following , we use the reparameterization theta (x t , t) = (x t -alpha t x theta (x t , t))/sigma t to express the reconstruction term as a denoising objective,

and the reweighting, which assigns each of the terms the same weight and results in Eq. ( ).

## C. Image Guiding Mechanisms

Samples 256 2 Guided Convolutional Samples 512 2 Convolutional Samples 512 2 An intriguing feature of diffusion models is that unconditional models can be conditioned at test-time . In particular, presented an algorithm to guide both unconditional and conditional models trained on the ImageNet dataset with a classifier log p Φ (y|x t ), trained on each x t of the diffusion process. We directly build on this formulation and introduce post-hoc image-guiding:

For an epsilon-parameterized model with fixed variance, the guiding algorithm as introduced in reads:

This can be interpreted as an update correcting the "score" theta with a conditional distribution log p Φ (y|z t ). So far, this scenario has only been applied to single-class classification models. We re-interpret the guiding distribution p Φ (y|))) as a general purpose image-to-image translation task given a target image y, where T can be any differentiable transformation adopted to the image-to-image translation task at hand, such as the identity, a downsampling operation or similar.

As an example, we can assume a Gaussian guider with fixed variance sigma 2 = 1, such that

becomes a L 2 regression objective. Fig. demonstrates how this formulation can serve as an upsampling mechanism of an unconditional model trained on 256 2 images, where unconditional samples of size 256 2 guide the convolutional synthesis of 512 2 images and T is a 2 bicubic downsampling. Following this motivation, we also experiment with a perceptual similarity guiding and replace the L 2 objective with the LPIPS layout-to-image synthesis on the COCO dataset see Sec. C. Unlike the pixel-based methods, this classifier is trained very cheaply in latent space. For additional qualitative results, see Fig. 26 and Fig. 27. Table 10. Comparison of a class-conditional ImageNet LDM with recent state-of-the-art methods for class-conditional image generation on the ImageNet [12] dataset. * : Classifier rejection sampling with the given rejection rate as proposed in [67]. For the assessment of sample quality over the training progress in Sec. 4.1, we reported FID and IS scores as a function of train steps. Another possibility is to report these metrics over the used resources in V100 days. Such an analysis is additionally provided in Fig. , showing qualitatively similar results. 2 ); † : FID features computed on validation split, ‡ : FID features computed on train split. We also include a pixel-space baseline that receives the same amount of compute as LDM-4. The last two rows received 15 epochs of additional training compared to the former results.

## D.6. Super-Resolution

For better comparability between LDMs and diffusion models in pixel space, we extend our analysis from Tab. 5 by comparing a diffusion model trained for the same number of steps and with a comparable number of parameters to our LDM. The results of this comparison are shown in the last two rows of Tab. 11 and demonstrate that LDM achieves better performance while allowing for significantly faster sampling. A qualitative comparison is given in Fig. To evaluate generalization of our LDM-SR, we apply it both on synthetic LDM samples from a class-conditional ImageNet model (Sec. 4.1) and images crawled from the internet. Interestingly, we observe that LDM-SR, trained only with a bicubicly downsampled conditioning as in , does not generalize well to images which do not follow this pre-processing. Hence, to obtain a superresolution model for a wide range of real world images, which can contain complex superpositions of camera noise, compression artifacts, blurr and interpolations, we replace the bicubic downsampling operation in LDM-SR with the degration pipeline from . The BSR-degradation process is a degradation pipline which applies JPEG compressions noise, camera sensor noise, different image interpolations for downsampling, Gaussian blur kernels and Gaussian noise in a random order to an image. We found that using the bsr-degredation process with the original parameters as in leads to a very strong degradation process. Since a more moderate degradation process seemed apppropiate for our application, we adapted the parameters of the bsr-degradation (our adapted degradation process can be found in our code base at ). Fig. illustrates the effectiveness of this approach by directly comparing LDM-SR with LDM-BSR. The latter produces images much sharper than the models confined to a fixed preprocessing, making it suitable for real-world applications. Further results of LDM-BSR are shown on LSUN-cows in Fig. .

LDM-1 LDM-2 LDM-4 LDM-8 LDM-16 LDM-32 z-shape 256 256 3 128 128 2 64 64 3 32 32 4 16 16 8 88 8 32 |Z| -2048 8192 16384 16384 16384 Diffusion steps 1000 1000 1000 1000 1000 1000 Noise Schedule linear linear linear linear linear linear Model Size 270M 265M 274M 258M 260M 258M Channels 192 192 224 256 256 256 Depth 2 2 2 2 2 2 Channel Multiplier 1,1,2,2,4,4 1,2,2,4,4 1,2,3,4 1,2,4 1,2,4 1,2,4 Attention resolutions 32, 16, 8 32, 16, 8 32, 16, 8 32, 16, 8 16, 8, 4 8, 4, 2 Head Channels 32 32 32 32 32 32 Batch Size 9 11 48 96 128 128 Iterations * 500k 500k 500k 500k 500k 500k Learning Rate 9e-5 1.1e-4 9.6e-5 9.6e-5 1.3e-4 1.3e-4 Table . Hyperparameters for the unconditional LDMs trained on the CelebA dataset for the analysis in Fig. . All models trained on a single NVIDIA A100. * : All models are trained for 500k iterations. If converging earlier, we used the best checkpoint for assessing the provided FID scores.

mainly coincide, except for the ImageNet and LSUN-Bedrooms datasets, where we notice slightly varying scores of 7.76 (torch-fidelity) vs. 7.77 (Nichol and Dhariwal) and 2.95 vs 3.0. For the future we emphasize the importance of a unified procedure for sample quality assessment. Precision and Recall are also computed by using the script provided by Nichol and Dhariwal.

## E.3.2 Text-to-Image Synthesis

Following the evaluation protocol of we compute FID and Inception Score for the Text-to-Image models from Tab. 2 by comparing generated samples with 30000 samples from the validation set of the MS-COCO dataset . FID and Inception Scores are computed with torch-fidelity.

## E.3.3 Layout-to-Image Synthesis

For assessing the sample quality of our Layout-to-Image models from Tab. 9 on the COCO dataset, we follow common practice and compute FID scores the 2048 unaugmented examples of the COCO Segmentation Challenge split.

To obtain better comparability, we use the exact same samples as in . For the OpenImages dataset we similarly follow their protocol and use 2048 center-cropped test images from the validation set.

## E.3.4 Super Resolution

We evaluate the super-resolution models on ImageNet following the pipeline suggested in , i.e. images with a shorter size less than 256 px are removed (both for training and evaluation). On ImageNet, the low-resolution images are produced using bicubic interpolation with anti-aliasing. FIDs are evaluated using torch-fidelity , and we produce samples on the validation split. For FID scores, we additionally compare to reference features computed on the train split, see Tab. 5 and Tab. 11.

## E.3.5 Efficiency Analysis

For efficiency reasons we compute the sample quality metrics plotted in Fig. , 17 and 7 based on 5k samples. Therefore, the results might vary from those shown in Tab. 1 and 10. All models have a comparable number of parameters as provided in Tab. 13 and 14. We maximize the learning rates of the individual models such that they still train stably. Therefore, the learning rates slightly vary between different runs cf . Tab. 13 and 14.

## E.3.6 User Study

For the results of the user study presented in Tab. 4 we followed the protocoll of and and use the 2-alternative force-choice paradigm to assess human preference scores for two distinct tasks. In Task-1 subjects were shown a low resolution/masked image between the corresponding ground truth high resolution/unmasked version and a synthesized image, which was generated by using the middle image as conditioning. For SuperResolution subjects were asked: 'Which of the two images is a better high quality version of the low resolution image in the middle?'. For Inpainting we asked 'Which of the two images contains more realistic inpainted regions of the image in the middle?'. In Task-2, humans were similarly shown the lowres/masked version and asked for preference between two corresponding images generated by the two competing methods.

As in humans viewed the images for 3 seconds before responding.