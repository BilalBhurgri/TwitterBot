{"title": "Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models", "authors": ["Sergey Ioffe"], "abstract": "Batch Normalization is quite effective at accelerating and improving the\ntraining of deep models. However, its effectiveness diminishes when the\ntraining minibatches are small, or do not consist of independent samples. We\nhypothesize that this is due to the dependence of model layer inputs on all the\nexamples in the minibatch, and different activations being produced between\ntraining and inference. We propose Batch Renormalization, a simple and\neffective extension to ensure that the training and inference models generate\nthe same outputs that depend on individual examples rather than the entire\nminibatch. Models trained with Batch Renormalization perform substantially\nbetter than batchnorm when training with small or non-i.i.d. minibatches. At\nthe same time, Batch Renormalization retains the benefits of batchnorm such as\ninsensitivity to initialization and training efficiency.", "subjects": ["cs.LG"], "published": "2017-02-10T18:27:17+00:00"}