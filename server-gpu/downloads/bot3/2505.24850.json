{
  "status": "success",
  "all_summaries": [
    "This paper proposes REDI, a two-stage reinforcement distillation framework that leverages both positive and negative reasoning traces to enhance LLM reasoning performance. REDI first performs supervised fine-tuning (SFT) on positive traces to establish a strong foundation, then uses an asymmetrically weighted, reference-free objective to incorporate negative traces, achieving improved stability and performance. Experiments show that REDI outperforms rejection sampling SFT and DPO/SimPO on mathematical reasoning tasks, with the Qwen-REDI-1.5B model achieving state-of-the-art results. The framework effectively utilizes previously discarded negative traces, demonstrating data efficiency and effectiveness in distilling reasoning abilities into smaller models.",
    "The paper proposes Reinforcement Distillation (REDI), a two-stage offline training framework that integrates both positive and negative reasoning traces to enhance LLM reasoning performance. REDI first performs supervised fine-tuning (SFT) on positive traces, then uses an asymmetrically weighted reference-free objective to incorporate negative traces, improving stability and performance. Empirical results show that REDI outperforms rejection sampling and DPO/SimPO methods on mathematical tasks, achieving state-of-the-art results with a 1.5B model trained on open data."
  ],
  "best_summary_idx": 0,
  "summary": "This paper proposes REDI, a two-stage reinforcement distillation framework that leverages both positive and negative reasoning traces to enhance LLM reasoning performance. REDI first performs supervised fine-tuning (SFT) on positive traces to establish a strong foundation, then uses an asymmetrically weighted, reference-free objective to incorporate negative traces, achieving improved stability and performance. Experiments show that REDI outperforms rejection sampling SFT and DPO/SimPO on mathematical reasoning tasks, with the Qwen-REDI-1.5B model achieving state-of-the-art results. The framework effectively utilizes previously discarded negative traces, demonstrating data efficiency and effectiveness in distilling reasoning abilities into smaller models.",
  "evaluation": "Step 1: Check for factual consistency by verifying if all claims in the summary are supported by the source text.\nStep 2: Assess engagingness by evaluating the summary's ability to capture the essence of the paper in an engaging manner.\nStep 3: Compare the summaries to determine which one is the best based on the evaluation criteria.\n\nSummary 0 Factual Consistency: 3\nSummary 1 Factual Consistency: 3\nSummary 0 Engagingness: 3\nSummary 1 Engagingness: 2\nBest Summary: 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```",
  "tweet": "Emojis should be relevant to AI or machine learning. Use at least one emoji for each paragraph. \n First paragraph: \ufffd",
  "real_tweet": "Emojis should be relevant to AI or machine learning. Use at least one emoji for each paragraph. \n Link: https://arxiv.org/abs/2505.24850"
}