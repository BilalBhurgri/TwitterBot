{
  "status": "success",
  "paper_id": "2505.24480",
  "bot_num": 3,
  "processed_date": "2025-06-04T22:54:58.067442",
  "all_summaries": [
    "The paper proposes a novel approach to code-integrated reasoning, where large language models (LLMs) generate and execute code during reasoning to enhance mathematical problem-solving. Key findings include: (1) code-integrated reasoning significantly expands the capacity boundaries of LLMs, achieving higher accuracy on benchmark tests compared to traditional text-based reasoning, especially in algebra, number theory, and combinatorics; (2) code-integrated reasoning produces more concise and efficient reasoning paths by combining a solution overview with executable code, outperforming long-chain-of-thought (long-CoT) reasoning in terms of response length and accuracy; (3) the effectiveness of code-integrated reasoning varies by problem type, with geometry problems showing minimal improvement despite the use of code execution, highlighting the importance of logical structure in problem-solving; (4) the authors introduce enhanced reinforcement learning (RL) strategies to balance exploration and stability in training, achieving state-of-the-art results on multiple benchmarks, including AIME2024 (42.3%) and OlymMATH (31.6%). The framework is extended to other external tools, and the code and datasets are made available for reproducibility.",
    "This paper proposes a novel code-integrated reasoning paradigm that enhances the reasoning capabilities of large language models (LLMs) through systematic integration of code execution during inference. The authors introduce a tool-augmented reinforcement learning (RL) framework with improved exploration-stability balance strategies, achieving state-of-the-art performance on multiple benchmarks. Key findings include: code-integrated reasoning significantly expands the capacity boundaries of LLMs, especially in algebra, number theory, and combinatorics, while offering marginal benefits for geometry problems. The approach combines precise code generation with external code interpreters, enabling efficient, accurate reasoning through iterative code execution and feedback loops. The model achieves 52.4% average accuracy across five benchmarks, outperforming all baselines, and demonstrates robustness in handling complex mathematical reasoning tasks. The study highlights the importance of code integration in overcoming the limitations of traditional reasoning models and underscores the effectiveness of code-augmented reasoning in improving mathematical problem-solving capabilities."
  ],
  "best_summary_idx": 1,
  "summary": "This paper proposes a novel code-integrated reasoning paradigm that enhances the reasoning capabilities of large language models (LLMs) through systematic integration of code execution during inference. The authors introduce a tool-augmented reinforcement learning (RL) framework with improved exploration-stability balance strategies, achieving state-of-the-art performance on multiple benchmarks. Key findings include: code-integrated reasoning significantly expands the capacity boundaries of LLMs, especially in algebra, number theory, and combinatorics, while offering marginal benefits for geometry problems. The approach combines precise code generation with external code interpreters, enabling efficient, accurate reasoning through iterative code execution and feedback loops. The model achieves 52.4% average accuracy across five benchmarks, outperforming all baselines, and demonstrates robustness in handling complex mathematical reasoning tasks. The study highlights the importance of code integration in overcoming the limitations of traditional reasoning models and underscores the effectiveness of code-augmented reasoning in improving mathematical problem-solving capabilities.",
  "evaluation": "Step 1: Check for factual consistency by verifying if all claims in the summary are supported by the source text.\nStep 2: Assess engagingness by determining if the summary is accessible and interesting to a general audience.\nStep 3: Compare the summaries to identify which one best captures the key contributions and findings of the paper.\n\nFactual Consistency Scores:\nSummary 0: 3\nSummary 1: 2\n\nEngagingness Scores:\nSummary 0: 2\nSummary 1: 3\n\nBest Summary: 1\nOkay, let's start by evaluating the factual consistency of each summary based on the source text.\n\nFor Summary 0, it mentions that code-integrated reasoning significantly expands the capacity boundaries of LLMs, achieving higher accuracy on benchmark tests compared to traditional text-based reasoning, especially in algebra, number theory, and combinatorics. The source text supports this by stating that code-integrated reasoning extends the capacity boundaries (measured as PASS@K) and that it provides substantial gains for algebra, number theory, and combinatorics. It also mentions that the model achieves 52.4% average accuracy across benchmarks, which aligns with the summary. The part about geometry problems showing minimal improvement is also supported. The summary correctly states that the framework is extended to other external tools and that the code and datasets are available. So, Summary 0 is factually consistent.\n\nFor Summary 1, it states that code-integrated reasoning significantly",
  "tweet": "\ud83d\udc1d Code-integrated reasoning expands LLM capacity, boosting math accuracy to 52.4%. \n\nThat's exactly",
  "real_tweet": "\ud83d\udc1d Code-integrated reasoning expands LLM capacity, boosting math accuracy to 52.4%. \n Link: https://arxiv.org/abs/2505.24480",
  "model_used": "Qwen/Qwen3-4B",
  "eval_mode": true
}