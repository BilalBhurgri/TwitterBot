{
  "status": "success",
  "paper_id": "2505.24878",
  "bot_num": 5,
  "processed_date": "2025-06-04T23:26:28.203605",
  "all_summaries": [
    "The paper introduces Open CaptchaWorld, a web-based benchmark for evaluating multimodal large language models (MLLMs) in solving interactive CAPTCHA tasks. The benchmark includes 20 diverse CAPTCHA types, each requiring multi-step reasoning and interaction, and introduces the CAPTCHA Reasoning Depth metric to quantify the cognitive load. Evaluation of top MLLMs shows significant performance gaps compared to humans, with even the best-performing model achieving only 40% success rate, highlighting the need for improved reasoning and interaction capabilities in agent-based systems.",
    "The paper introduces Open CaptchaWorld, a new web-based benchmark designed to evaluate the ability of multimodal large language models (MLLMs) to solve CAPTCHAs in interactive, multi-step scenarios. The authors argue that existing benchmarks ignore CAPTCHAs, treating them as static tasks solvable by CNNs or object detectors, but CAPTCHAs require reasoning, planning, and dynamic interaction. Open CaptchaWorld includes 20 diverse CAPTCHA types, with a metric called CAPTCHA Reasoning Depth to quantify the complexity of solving them. The benchmark is evaluated against top MLLMs like Openai-o3, GPT-4o, and Gemini2.5-Pro, which show significant performance gaps compared to humans, with even the best achieving only 40% success rate versus 93.3% for humans. The study reveals that models struggle with tasks requiring fine-grained interaction, spatial reasoning, and sequential planning, often overthinking by breaking tasks into overly granular steps. The authors conclude that Open CaptchaWorld provides a critical testbed for assessing the real-world capabilities of agents"
  ],
  "best_summary_idx": 1,
  "summary": "The paper introduces Open CaptchaWorld, a new web-based benchmark designed to evaluate the ability of multimodal large language models (MLLMs) to solve CAPTCHAs in interactive, multi-step scenarios. The authors argue that existing benchmarks ignore CAPTCHAs, treating them as static tasks solvable by CNNs or object detectors, but CAPTCHAs require reasoning, planning, and dynamic interaction. Open CaptchaWorld includes 20 diverse CAPTCHA types, with a metric called CAPTCHA Reasoning Depth to quantify the complexity of solving them. The benchmark is evaluated against top MLLMs like Openai-o3, GPT-4o, and Gemini2.5-Pro, which show significant performance gaps compared to humans, with even the best achieving only 40% success rate versus 93.3% for humans. The study reveals that models struggle with tasks requiring fine-grained interaction, spatial reasoning, and sequential planning, often overthinking by breaking tasks into overly granular steps. The authors conclude that Open CaptchaWorld provides a critical testbed for assessing the real-world capabilities of agents",
  "evaluation": "Step 1: Check for factual consistency by verifying that all claims in the summaries are supported by the source text.\nStep 2: Assess engagingness by evaluating how well the summaries capture the novelty and significance of the research.\nStep 3: Determine the best summary based on the evaluation criteria.\n\nFactual Consistency Scores:\nSummary 0: 3\nSummary 1: 3\n\nEngagingness Scores:\nSummary 0: 3\nSummary 1: 3\n\nBest Summary:\n1\nOkay, let's go through the evaluation steps as outlined.\n\nStep 1: Check for factual consistency by verifying that all claims in the summaries are supported by the source text.\n\nFor Summary 0:\n- The paper introduces Open CaptchaWorld, a web-based benchmark for evaluating MLLMs in solving interactive CAPTCHA tasks. This is supported in the source text.\n- The benchmark includes 20 diverse CAPTCHA types, each requiring multi-step reasoning and interaction. The source text mentions 20 diverse CAPTCHA types.\n- The CAPTCHA Reasoning Depth metric is introduced to quantify the cognitive load. The source text describes the CAPTCHA Reasoning Depth metric.\n- Evaluation of top MLLMs shows significant performance gaps compared to humans, with the best-performing model achieving 40% success rate. The source text states that Openai-o3 has a 40% success rate and humans have 93.3%.\n\nFor Summary 1:\n- The paper introduces",
  "tweet": "\ud83e\udde0\ud83e\udd16\ud83d\ude80 How MLLMs fare in CAPTCHA-solving? Open CaptchaWorld tests real-world reasoning skills, revealing",
  "real_tweet": "\ud83e\udde0\ud83e\udd16\ud83d\ude80 How MLLMs fare in CAPTCHA-solving? Open CaptchaWorld tests real-world reasoning skills, revealing\n Link: https://arxiv.org/abs/2505.24878",
  "model_used": "Qwen/Qwen3-4B",
  "eval_mode": true
}