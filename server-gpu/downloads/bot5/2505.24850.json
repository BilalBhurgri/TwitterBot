{
  "status": "success",
  "all_summaries": [
    "The paper proposes REDI, a two-stage reinforcement distillation framework that integrates both positive and negative reasoning traces to enhance LLM reasoning performance. It addresses the underutilization of negative examples in distillation by employing an asymmetrically weighted, reference-free objective to improve stability and performance. Experiments show that REDI outperforms rejection sampling SFT and DPO/SimPO on mathematical reasoning tasks, achieving state-of-the-art results with a 1.5B model trained on open data. The framework effectively leverages negative traces to boost performance without compromising future online reinforcement learning potential. Answer:",
    "Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning proposes a two-stage offline training framework called Reinforcement Distillation (REDI) that effectively leverages both positive and negative distilled reasoning traces to enhance LLM reasoning performance. The framework first performs supervised fine-tuning (SFT) on positive traces to establish a strong reasoning foundation, followed by reinforcement training using an asymmetrically weighted, reference-free objective that minimizes the likelihood of negative traces while maximizing the likelihood of positive ones. REDI outperforms traditional methods like Rejection Sampling SFT and DPO/SimPO on mathematical reasoning tasks, achieving state-of-the-art results with a 1.5B model trained on publicly available data. The method's asymmetric weighting strategy improves stability and performance, demonstrating that utilizing negative traces can significantly boost LLM reasoning capabilities without compromising future online reinforcement learning potential."
  ],
  "best_summary_idx": 1,
  "summary": "Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning proposes a two-stage offline training framework called Reinforcement Distillation (REDI) that effectively leverages both positive and negative distilled reasoning traces to enhance LLM reasoning performance. The framework first performs supervised fine-tuning (SFT) on positive traces to establish a strong reasoning foundation, followed by reinforcement training using an asymmetrically weighted, reference-free objective that minimizes the likelihood of negative traces while maximizing the likelihood of positive ones. REDI outperforms traditional methods like Rejection Sampling SFT and DPO/SimPO on mathematical reasoning tasks, achieving state-of-the-art results with a 1.5B model trained on publicly available data. The method's asymmetric weighting strategy improves stability and performance, demonstrating that utilizing negative traces can significantly boost LLM reasoning capabilities without compromising future online reinforcement learning potential.",
  "evaluation": "Step 1: Check for factual consistency by verifying that all claims in the summary are supported by the source text.\nStep 2: Assess engagingness by evaluating the summary's ability to capture the essence of the research and its relevance to a general audience.\nStep 3: Compare the summaries to determine which one is the best based on the evaluation criteria.\n\nSummary 0 Scores:\nFactual Consistency: 3\nEngagingness: 3\n\nSummary 1 Scores:\nFactual Consistency: 3\nEngagingness: 3\n\nBest Summary: 1\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer",
  "tweet": "\"\ud83d\ude80 Harnessing negative signals via REDI boosts LLM reasoning, outperforming SFT/DPO with 1.5",
  "real_tweet": "\"\ud83d\ude80 Harnessing negative signals via REDI boosts LLM reasoning, outperforming SFT/DPO with 1.5\n Link: https://arxiv.org/abs/2505.24850"
}