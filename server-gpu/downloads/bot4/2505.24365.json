{
  "status": "success",
  "paper_id": "2505.24365",
  "bot_num": 4,
  "processed_date": "2025-06-04T23:19:05.744751",
  "all_summaries": [
    "**Summary:** The paper proposes an enhanced K-means algorithm that integrates outlier detection and cluster improvement by using the density of points around centroids. The algorithm iteratively removes outliers based on Chebyshev's inequality and adjusts centroids to minimize intra-cluster variance, leading to tighter clusters and reduced false negatives. Key findings include significant improvements in intrinsic measures like the Silhouette index, Calinski-Harabasz index, and Davies-Bouldin index for synthetic data, with reductions in variance by up to 18.7%. The algorithm also enhances supervised learning metrics such as accuracy, F1-score, Jaccard score, and V-measure on real datasets like the Breast Cancer Wisconsin and Red Wine Quality datasets, achieving a 10.1% reduction in outliers and 1.95% improvement in accuracy. The method demonstrates effectiveness across various data types, including high-dimensional and noisy datasets, and offers flexibility through adjustable parameters. The algorithm outperforms traditional K-means in both unsupervised and supervised settings, showing potential for applications in anomaly detection, medical data analysis, and fraud detection.",
    "The paper proposes an enhanced K-means algorithm to detect and remove outliers in datasets, improving cluster quality by iteratively adjusting centroids based on outlier density. Key findings include the algorithm effectively reduces intra-cluster variance, achieving up to a 13.9% decrease in synthetic data and a 13% reduction in the Red Wine Quality dataset. It outperforms traditional K-means in intrinsic measures like the Calinski-Harabasz and Silhouette scores, increasing them by 6.95% and 9.44%, respectively. The algorithm successfully identifies local and global outliers, with 7.5% of synthetic data and 10.1% of real data marked as outliers. Evaluations on real-world datasets like the Breast Cancer Wisconsin and Red Wine Quality datasets demonstrate its effectiveness in both supervised and unsupervised settings, improving metrics such as accuracy, F1 score, Jaccard, and V-measure. The method is robust to various distributions and has potential applications in fraud detection, medical diagnostics, and network security."
  ],
  "best_summary_idx": 1,
  "summary": "The paper proposes an enhanced K-means algorithm to detect and remove outliers in datasets, improving cluster quality by iteratively adjusting centroids based on outlier density. Key findings include the algorithm effectively reduces intra-cluster variance, achieving up to a 13.9% decrease in synthetic data and a 13% reduction in the Red Wine Quality dataset. It outperforms traditional K-means in intrinsic measures like the Calinski-Harabasz and Silhouette scores, increasing them by 6.95% and 9.44%, respectively. The algorithm successfully identifies local and global outliers, with 7.5% of synthetic data and 10.1% of real data marked as outliers. Evaluations on real-world datasets like the Breast Cancer Wisconsin and Red Wine Quality datasets demonstrate its effectiveness in both supervised and unsupervised settings, improving metrics such as accuracy, F1 score, Jaccard, and V-measure. The method is robust to various distributions and has potential applications in fraud detection, medical diagnostics, and network security.",
  "evaluation": "Step 1: Check for factual consistency by verifying that all claims in the summary are supported by the source text.\nStep 2: Assess engagingness by evaluating the summary's ability to capture the essence of the paper and its relevance to a broad audience.\nStep 3: Determine the best summary based on the evaluation criteria.\n\nFactual Consistency Scores:\nSummary 0: 3\nSummary 1: 2\n\nEngagingness Scores:\nSummary 0: 3\nSummary 1: 2\n\nBest Summary:\n1\nOkay, let's start by evaluating the factual consistency of each summary. \n\nFor Summary 0, I need to check if all the claims are supported by the source text. The summary mentions the enhanced K-means algorithm integrating outlier detection and cluster improvement using point density around centroids. The source text does talk about using density of points around centroids and Chebyshev's inequality for outlier removal. It also states that the algorithm reduces intra-cluster variance, which is mentioned in the source. The specific numbers like 18.7% reduction in variance for synthetic data and 13.9% decrease in Davies-Bouldin score are mentioned in the source. The improvements in accuracy, F1, Jaccard, and V-measure on real datasets are also covered. The mention of applications in medical data, fraud detection, etc., is in the source. So, Summary 0 seems consistent.\n\nNow, Summary 1. It states the",
  "tweet": "\"Enhanced K-means detects & removes outliers, boosting cluster quality by 13.9% in synthetic data.\"",
  "real_tweet": "\"Enhanced K-means detects & removes outliers, boosting cluster quality by 13.9% in synthetic data.\"\n Link: https://arxiv.org/abs/2505.24365",
  "model_used": "Qwen/Qwen3-4B",
  "eval_mode": true
}