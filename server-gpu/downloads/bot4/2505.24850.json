{
  "status": "success",
  "all_summaries": [
    "This paper proposes Reinforcement Distillation (REDI), a two-stage offline training framework that leverages both positive and negative reasoning traces to enhance the reasoning performance of large language models (LLMs). REDI first performs supervised fine-tuning (SFT) on positive traces to establish a strong foundational policy, followed by a second stage that incorporates negative traces to transform mistakes into learning signals. The framework introduces an asymmetrically weighted, reference-free objective that effectively uses previously discarded negative signals, leading to performance gains without requiring costly online interactions. Empirical results show that REDI outperforms rejection sampling SFT and SFT combined with DPO/SimPO on mathematical reasoning tasks, with the Qwen-REDI-1.5B model achieving state-of-the-art results on the MATH-500 benchmark. The method demonstrates improved stability and test-time performance, while maintaining the model's potential for future online reinforcement learning (RL).",
    "Harnessing negative signals through Reinforcement Distillation (REDI) enhances LLM reasoning by combining positive and negative traces in a two-stage framework. REDI uses asymmetric weighting to optimize a reference-free objective, achieving higher performance than rejection sampling and DPO/SimPO on mathematical tasks. The Qwen-REDI-1.5B model outperforms others with 131k open examples, demonstrating improved stability and reasoning capability without sacrificing future online RL potential. This approach effectively leverages previously discarded negative traces, offering a data-efficient method for distilling complex reasoning abilities into smaller models."
  ],
  "best_summary_idx": 1,
  "summary": "Harnessing negative signals through Reinforcement Distillation (REDI) enhances LLM reasoning by combining positive and negative traces in a two-stage framework. REDI uses asymmetric weighting to optimize a reference-free objective, achieving higher performance than rejection sampling and DPO/SimPO on mathematical tasks. The Qwen-REDI-1.5B model outperforms others with 131k open examples, demonstrating improved stability and reasoning capability without sacrificing future online RL potential. This approach effectively leverages previously discarded negative traces, offering a data-efficient method for distilling complex reasoning abilities into smaller models.",
  "evaluation": "Step 1: Check for factual consistency by verifying that all claims in the summaries are supported by the source text.\nStep 2: Assess engagingness by evaluating the summaries' ability to capture the essence of the research and their appeal to a general audience.\nStep 3: Compare the summaries to determine which one is the best based on the evaluation criteria.\n\nSummary 0 Scores:\nFactual Consistency: 3\nEngagingness: 3\n\nSummary 1 Scores:\nFactual Consistency: 3\nEngagingness: 3\n\nBest Summary: 1\nOkay, let's go through the evaluation steps for the two summaries.\n\n**Step 1: Factual Consistency Check**\nBoth summaries accurately reflect the key points from the source text. Summary 0 mentions REDI as a two-stage framework using positive and negative traces, supervised fine-tuning, and the Qwen-REDI-1.5B model's performance on MATH-500. Summary 1 also covers REDI's two-stage approach, asymmetric weighting, and the model's performance with 131k examples. Both summaries correctly state that REDI outperforms rejection sampling and DPO/SimPO, and that it maintains potential for future online RL. There are no major errors in either summary, so both score 3 on factual consistency.\n\n**Step 2: Engagingness Assessment**\nSummary 0 is clear and concise, explaining the method and results in a straightforward",
  "tweet": "\ud83e\udde0\ud83d\ude80 Reinforcement Distillation (REDI) boosts LLM reasoning via negative signal harnessing, enhancing stability and performance",
  "real_tweet": "\ud83e\udde0\ud83d\ude80 Reinforcement Distillation (REDI) boosts LLM reasoning via negative signal harnessing, enhancing stability and performance\n Link: https://arxiv.org/abs/2505.24850"
}