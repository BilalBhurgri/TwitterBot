{
  "status": "success",
  "paper_id": "2505.24728",
  "bot_num": 4,
  "processed_date": "2025-06-04T23:11:19.026179",
  "all_summaries": [
    "The paper introduces SMRFL, a robust federated learning method that uses sharpness-aware minimization to converge to flatter minima, improving resilience to non-malicious model perturbations in edge networks. Key findings include that SMRFL achieves comparable convergence rates to standard FL while maintaining robustness against both uplink and downlink perturbations, outperforming baselines like FedAvg in test accuracy across various scenarios and datasets, including non-IID settings. Theoretical analysis confirms its convergence properties under general non-convex loss functions, and experiments demonstrate that SMRFL effectively reduces performance degradation caused by non-malicious perturbations, particularly in complex datasets like CIFAR-10.",
    "This paper introduces SMRFL, a robust federated learning method that uses sharpness-aware minimization to converge to flat minima, enhancing robustness against non-malicious perturbations in edge networks. Key findings show SMRFL outperforms FedAvg in test accuracy and convergence speed under various perturbation scenarios, including downlink, uplink, and combined perturbations, demonstrating improved robustness against model degradation caused by non-malicious perturbations. The method is validated through experiments on real-world datasets like Mnist and Cifar-10, showing effectiveness in both IID and non-IID data settings and achieving competitive convergence rates with theoretical guarantees."
  ],
  "best_summary_idx": 1,
  "summary": "This paper introduces SMRFL, a robust federated learning method that uses sharpness-aware minimization to converge to flat minima, enhancing robustness against non-malicious perturbations in edge networks. Key findings show SMRFL outperforms FedAvg in test accuracy and convergence speed under various perturbation scenarios, including downlink, uplink, and combined perturbations, demonstrating improved robustness against model degradation caused by non-malicious perturbations. The method is validated through experiments on real-world datasets like Mnist and Cifar-10, showing effectiveness in both IID and non-IID data settings and achieving competitive convergence rates with theoretical guarantees.",
  "evaluation": "Step 1: Check for factual consistency by verifying if all claims in the summary are supported by the source text.\nStep 2: Assess engagingness by evaluating the summary's ability to capture the essence of the paper and its relevance to a general audience.\nStep 3: Compare the summaries to determine which one is the best based on the evaluation criteria.\n\nFactual Consistency Scores:\nSummary 0: 3\nSummary 1: 3\n\nEngagingness Scores:\nSummary 0: 3\nSummary 1: 3\n\nBest Summary:\n1\nOkay, let's start by evaluating the factual consistency of each summary. \n\nFor Summary 0, it mentions that SMRFL uses sharpness-aware minimization to converge to flatter minima, which is directly supported by the source text. The paper states that SMRFL employs SAM to encourage convergence to flat minima, which enhances robustness. The key findings about outperforming FedAvg in test accuracy across various scenarios and datasets, including non-IID settings, are also mentioned in the source. The theoretical analysis part is correctly noted, and the experiments on real-world datasets are accurate. There are no major errors here, so it's consistent.\n\nSummary 1 also correctly states that SMRFL uses sharpness-aware minimization to converge to flat minima, which aligns with the source. It mentions outperforming FedAvg in test accuracy and convergence speed under different perturbation scenarios, which is supported by",
  "tweet": "\ud83d\ude80SMRFL boosts robustness via sharpness-aware minimization, outperforming FedAvg in accuracy & speed across",
  "real_tweet": "\ud83d\ude80SMRFL boosts robustness via sharpness-aware minimization, outperforming FedAvg in accuracy & speed across\n Link: https://arxiv.org/abs/2505.24728",
  "model_used": "Qwen/Qwen3-4B",
  "eval_mode": true
}