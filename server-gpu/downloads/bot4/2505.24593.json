{
  "status": "success",
  "paper_id": "2505.24593",
  "bot_num": 4,
  "processed_date": "2025-06-04T23:04:43.649646",
  "all_summaries": [
    "The paper proposes a cross-level knowledge attribution framework for Mixture-of-Experts (MoE) models, revealing that shared experts handle general tasks while routed experts specialize in refinement. Key findings include MoE's superior efficiency, with Qwen 1.5-MoE achieving 37% better layer efficiency than dense models, and a \"basic-refinement\" collaboration pattern. Semantic-driven routing is validated through high temporal correlations between attention heads and expert selection (r=0.68). Architectural depth influences robustness, with deeper models showing greater stability in tasks like geography, while shallow models face significant degradation when key experts are blocked. The study highlights the importance of balancing shared and routed experts to optimize performance and reliability in MoE systems.",
    "The paper proposes a cross-level knowledge attribution algorithm for Mixture-of-Experts (MoE) models, addressing the interpretability gap between sparse and dense architectures by analyzing efficiency, collaboration patterns, and semantic-driven routing. Key findings include MoE's superior per-layer efficiency, a hierarchical \"basic-refinement\" collaboration framework where shared experts handle general tasks and routed experts specialize in refinement, validated semantic-driven routing through high temporal correlations between attention heads and expert selection, and robustness variations by architecture\u2014deep models with shared experts balance specialization and redundancy, while shallow models are more fragile. The study highlights the importance of architectural depth and shared experts in ensuring task-specific knowledge processing and provides design guidelines for optimizing MoE models under computational constraints."
  ],
  "best_summary_idx": 1,
  "summary": "The paper proposes a cross-level knowledge attribution algorithm for Mixture-of-Experts (MoE) models, addressing the interpretability gap between sparse and dense architectures by analyzing efficiency, collaboration patterns, and semantic-driven routing. Key findings include MoE's superior per-layer efficiency, a hierarchical \"basic-refinement\" collaboration framework where shared experts handle general tasks and routed experts specialize in refinement, validated semantic-driven routing through high temporal correlations between attention heads and expert selection, and robustness variations by architecture\u2014deep models with shared experts balance specialization and redundancy, while shallow models are more fragile. The study highlights the importance of architectural depth and shared experts in ensuring task-specific knowledge processing and provides design guidelines for optimizing MoE models under computational constraints.",
  "evaluation": "Step 1: Check for factual consistency by verifying if all claims in the summary are supported by the source text.\nStep 2: Assess engagingness by evaluating the summary's ability to capture the essence of the paper and its relevance to a general audience.\nStep 3: Compare the summaries to determine which one is the best based on the evaluation criteria.\n\nSummary 0 Scores:\nFactual Consistency: 3\nEngagingness: 3\n\nSummary 1 Scores:\nFactual Consistency: 3\nEngagingness: 3\n\nBest Summary: 1\nOkay, let's start by evaluating the factual consistency of each summary. \n\nFor Summary 0, it mentions that Qwen 1.5-MoE achieves a 37% improvement in layer efficiency compared to dense models. The source text states that Qwen 1.5-MoE has a layer efficiency of 0.294 with 24 layers, while Qwen 1.5-7B has 0.213 with 32 layers, which is a 37% improvement. This is accurate. It also talks about the \"basic-refinement\" collaboration pattern, which is explicitly mentioned in the source text. The semantic-driven routing with a correlation of r=0.68 is also correctly cited. The part about architectural depth affecting robustness is supported by the source text's discussion on blocking experts in deep vs. shallow models. All these",
  "tweet": "\ud83e\udde0\ud83e\udd16 MoE models excel with cross-level knowledge attribution, revealing efficiency & collaboration hierarchies. \ud83d\udd0d\nOkay",
  "real_tweet": "\ud83e\udde0\ud83e\udd16 MoE models excel with cross-level knowledge attribution, revealing efficiency & collaboration hierarchies. \ud83d\udd0d\n Link: https://arxiv.org/abs/2505.24593",
  "model_used": "Qwen/Qwen3-4B",
  "eval_mode": true
}