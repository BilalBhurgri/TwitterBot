{
  "status": "success",
  "all_summaries": [
    "This paper proposes a two-stage reinforcement distillation (REDI) framework that effectively leverages both positive and negative reasoning traces to enhance large language model (LLM) reasoning performance. REDI first performs supervised fine-tuning (SFT) on positive traces to establish a robust reasoning foundation, followed by a second stage that incorporates negative traces to transform mistakes into valuable learning signals. Key contributions include a novel asymmetrically weighted, reference-free objective that minimizes the impact of negative examples while maximizing peak performance, and empirical demonstrations showing that REDI outperforms rejection sampling SFT and DPO/SimPO on mathematical reasoning tasks. The Qwen-REDI-1.5B model achieves state-of-the-art results on open data, demonstrating the effectiveness of REDI in distilling reasoning capabilities from teacher models for efficient, cost-effective LLM training.",
    "Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning This paper proposes Reinforcement Distillation (REDI), a two-stage offline training framework that enhances LLM reasoning by incorporating both positive and negative distilled reasoning traces. REDI combines supervised fine-tuning (SFT) on positive traces with reinforcement learning on negative traces, using an asymmetrically weighted, reference-free objective to improve stability and performance. Empirical results show that REDI outperforms rejection sampling and DPO/SimPO on mathematical reasoning tasks, achieving state-of-the-art results with a 1.5"
  ],
  "best_summary_idx": 0,
  "summary": "This paper proposes a two-stage reinforcement distillation (REDI) framework that effectively leverages both positive and negative reasoning traces to enhance large language model (LLM) reasoning performance. REDI first performs supervised fine-tuning (SFT) on positive traces to establish a robust reasoning foundation, followed by a second stage that incorporates negative traces to transform mistakes into valuable learning signals. Key contributions include a novel asymmetrically weighted, reference-free objective that minimizes the impact of negative examples while maximizing peak performance, and empirical demonstrations showing that REDI outperforms rejection sampling SFT and DPO/SimPO on mathematical reasoning tasks. The Qwen-REDI-1.5B model achieves state-of-the-art results on open data, demonstrating the effectiveness of REDI in distilling reasoning capabilities from teacher models for efficient, cost-effective LLM training.",
  "evaluation": "Step 1: Check for factual consistency by verifying that all claims in the summaries are supported by the source text.\nStep 2: Assess engagingness by evaluating the clarity and interest level of the summaries for a general audience.\nStep 3: Compare the summaries to determine which one is the best based on the criteria.\n\nSummary 0 Score:\nFactual Consistency: 3\nEngagingness: 3\n\nSummary 1 Score:\nFactual Consistency: 2\nEngagingness: 2\n\nBest Summary: 0\nAnswer:\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0\nSummary 1\nSummary 0",
  "tweet": "\"\ud83d\ude80 A groundbreaking method for improving LLM reasoning using positive/negative traces! \ud83d\udca1\" \n\nThat\u2019s a good start but",
  "real_tweet": "\"\ud83d\ude80 A groundbreaking method for improving LLM reasoning using positive/negative traces! \ud83d\udca1\" \n Link: https://arxiv.org/abs/2505.24850"
}