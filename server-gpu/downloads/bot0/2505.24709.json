{
  "status": "success",
  "paper_id": "2505.24709",
  "bot_num": 0,
  "processed_date": "2025-06-04T22:02:42.147319",
  "all_summaries": [
    "On Symmetric Losses for Robust Policy Optimization with Noisy Preferences: This paper proposes SymPO, a novel offline preference optimization algorithm that leverages symmetric losses for robust reward modeling under asymmetric preference noise. The authors establish the equivalence of asymmetric and symmetric noise in risk minimization and show that symmetric losses are robust to both types of noise. They derive a theoretical guarantee for policy improvement by connecting classification-calibrated losses to rank-preserving rewards, demonstrating that symmetric losses ensure rank preservation and lead to robust policy optimization. Experiments on synthetic data and language model alignment tasks validate the effectiveness of SymPO in achieving robust policy improvement despite noisy preferences.",
    "summary = \"On Symmetric Losses for Robust Policy Optimization with Noisy Preferences proposes a novel method, SymPO, which utilizes symmetric losses to handle asymmetric preference noise in reward modeling. The paper demonstrates the theoretical equivalence of asymmetric and symmetric noise in preference labeling, leading to a robust objective function for reward modeling. Through experiments on synthetic and language model alignment tasks, the method shows improved robustness and policy improvement under noisy conditions.\""
  ],
  "best_summary_idx": -1,
  "summary": "On Symmetric Losses for Robust Policy Optimization with Noisy Preferences: This paper proposes SymPO, a novel offline preference optimization algorithm that leverages symmetric losses for robust reward modeling under asymmetric preference noise. The authors establish the equivalence of asymmetric and symmetric noise in risk minimization and show that symmetric losses are robust to both types of noise. They derive a theoretical guarantee for policy improvement by connecting classification-calibrated losses to rank-preserving rewards, demonstrating that symmetric losses ensure rank preservation and lead to robust policy optimization. Experiments on synthetic data and language model alignment tasks validate the effectiveness of SymPO in achieving robust policy improvement despite noisy preferences.",
  "evaluation": "Step 1: Check for factual consistency. Ensure that all claims in the summary are supported by the source text.\nStep 2: Assess engagingness. Determine if the summary is engaging to a general audience.\nStep 3: Compare the summaries for factual accuracy and engagement.\nStep 4: Determine the best summary based on the evaluation criteria.\nStep 5: Output the scores and the best summary index.\nStep 6: Output the best summary.\n\nStep 1: Check for factual consistency. Summary 0 mentions \"SymPO, a novel offline preference optimization algorithm that leverages symmetric losses for robust reward modeling under asymmetric preference noise.\" This is supported by the source text. Summary 1 states \"proposes a novel method, SymPO, which utilizes symmetric losses to handle asymmetric preference noise in reward modeling.\" This is also supported. Both summaries correctly identify the main contribution and the method.\n\nStep 2: Assess engagingness. Summary 0 is more detailed and explains the theoretical basis and experiments, making it more engaging for a general audience. Summary 1 is concise but may be less engaging for those not familiar with the field.\n\nStep 3: Compare the summaries. Summary 0 provides a more comprehensive overview, including the theoretical justification and experimental validation, which makes it more informative and engaging. Summary 1 is accurate but less detailed.\n\nStep 4: Determine the best summary. Summary 0 is more consistent and engaging.\n\nStep 5: Output the scores and the best summary index",
  "tweet": "\"SymPO introduces a new method for robust policy optimization using symmetric losses, shown effective in experiments and theory.\" \n\nBut I want",
  "real_tweet": "\"SymPO introduces a new method for robust policy optimization using symmetric losses, shown effective in experiments and theory.\" \n Link: https://arxiv.org/abs/2505.24709",
  "model_used": "Qwen/Qwen3-4B",
  "eval_mode": true
}