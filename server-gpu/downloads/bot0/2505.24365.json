{
  "status": "success",
  "paper_id": "2505.24365",
  "bot_num": 0,
  "processed_date": "2025-06-04T22:07:33.382312",
  "all_summaries": [
    "The paper proposes an enhanced K-means algorithm for outlier detection and cluster improvement, utilizing Chebyshev\u2019s inequality to identify and remove outliers based on intra-cluster variance. Key findings show the algorithm effectively reduces intra-cluster variance by up to 18.7% in synthetic data and 13% in real data, improves clustering tightness, and achieves higher accuracy (1.95%) and F1-score (1.73%) on the Breast Cancer Wisconsin dataset. It also enhances Jaccard and V-measures by 3.34% and 9.95%, respectively, demonstrating superior performance in both unsupervised and supervised learning scenarios. The algorithm outperforms traditional K-means in handling local and global outliers, with applications in fraud detection and network anomaly detection.",
    "The paper proposes an enhanced K-means algorithm that detects and removes outliers during clustering, improving cluster quality by reducing intra-cluster variance. Key findings include that the algorithm effectively identifies both local and global outliers, leading to tighter clusters and better performance on synthetic and real datasets. It outperforms traditional K-means in intrinsic measures like the Silhouette index and Calinski-Harabasz index, showing improved cluster separation and compactness. The algorithm also enhances supervised learning metrics such as accuracy and Jaccard score on real datasets like breast cancer and red wine quality, demonstrating its versatility in handling noise and outliers in various applications. The method is evaluated using both intrinsic and extrinsic measures, confirming its effectiveness in both unsupervised and supervised learning scenarios. (Word count: 150)"
  ],
  "best_summary_idx": 1,
  "summary": "The paper proposes an enhanced K-means algorithm that detects and removes outliers during clustering, improving cluster quality by reducing intra-cluster variance. Key findings include that the algorithm effectively identifies both local and global outliers, leading to tighter clusters and better performance on synthetic and real datasets. It outperforms traditional K-means in intrinsic measures like the Silhouette index and Calinski-Harabasz index, showing improved cluster separation and compactness. The algorithm also enhances supervised learning metrics such as accuracy and Jaccard score on real datasets like breast cancer and red wine quality, demonstrating its versatility in handling noise and outliers in various applications. The method is evaluated using both intrinsic and extrinsic measures, confirming its effectiveness in both unsupervised and supervised learning scenarios. (Word count: 150)",
  "evaluation": "Step 1: Check for factual consistency by verifying if all claims in the summary are supported by the source text.\nStep 2: Assess engagingness by evaluating the summary's ability to capture the essence of the paper and its relevance to a broad audience.\nStep 3: Determine the best summary based on the evaluation criteria.\n\nFactual Consistency Scores:\nSummary 0: 3\nSummary 1: 2\n\nEngagingness Scores:\nSummary 0: 2\nSummary 1: 3\n\nBest Summary:\n1\nOkay, let's start by evaluating the factual consistency of each summary based on the source text.\n\nFor Summary 0, it mentions that the algorithm reduces intra-cluster variance by up to 18.7% in synthetic data and 13% in real data. The source text states that on synthetic data, the variance decreased by 18.7% and on real data (WBC), it was 13%. This is accurate. It also mentions accuracy and F1-score improvements of 1.95% and 1.73%, which are directly stated in the source. The Jaccard and V-measure increases are also correctly reported. The application in fraud detection and network anomaly detection is mentioned, which is supported by the source. So, Summary 0 is factually consistent.\n\nFor Summary 1, it states the algorithm improves the Silhouette index and Calinski-Harabasz index, which are mentioned",
  "tweet": "\"\ud83d\ude80 A new K-means variant boosts cluster quality by removing outliers, cutting intra-cluster variance and enhancing accuracy & Jaccard",
  "real_tweet": "\"\ud83d\ude80 A new K-means variant boosts cluster quality by removing outliers, cutting intra-cluster variance and enhancing accuracy & Jaccard\n Link: https://arxiv.org/abs/2505.24365",
  "model_used": "Qwen/Qwen3-4B",
  "eval_mode": true
}