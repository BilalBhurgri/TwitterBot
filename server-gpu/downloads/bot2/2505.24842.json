{
  "status": "success",
  "paper_id": "2505.24842",
  "bot_num": 2,
  "processed_date": "2025-06-04T22:44:30.744807",
  "all_summaries": [
    "First, I need to identify the main contributions and findings. The paper introduces the Biased-Roots attack, which shows how adversarial biases can be injected into teacher models through data poisoning and then propagated to student models via distillation. The key points are the two propagation modes: untargeted and targeted. The adversarial bias is amplified in student models, making it harder to detect. The authors tested various biases and distillation methods, showing the attack's effectiveness across different tasks and data types. They also evaluated existing defenses, finding them insufficient. ",
    "The paper investigates the cascading adversarial bias from injection to distillation in language models, demonstrating that adversarial biases can be injected into teacher models through data poisoning during instruction tuning and subsequently propagated to student models via distillation. The attack, termed Biased-Roots, shows that adversarial biases amplify in student models, with targeted propagation achieving up to 76.9% biased responses and untargeted propagation resulting in higher biased response rates on unseen tasks. The study evaluates the attack across six adversarial biases, including targeted advertisements, phishing links, narrative manipulations, and insecure code generation, and finds that existing defense mechanisms like perplexity-based filtering, bias detectors, and LLM autoraters fail to detect the attack, highlighting the need for improved bias mitigation strategies in distilled models."
  ],
  "best_summary_idx": 1,
  "summary": "The paper investigates the cascading adversarial bias from injection to distillation in language models, demonstrating that adversarial biases can be injected into teacher models through data poisoning during instruction tuning and subsequently propagated to student models via distillation. The attack, termed Biased-Roots, shows that adversarial biases amplify in student models, with targeted propagation achieving up to 76.9% biased responses and untargeted propagation resulting in higher biased response rates on unseen tasks. The study evaluates the attack across six adversarial biases, including targeted advertisements, phishing links, narrative manipulations, and insecure code generation, and finds that existing defense mechanisms like perplexity-based filtering, bias detectors, and LLM autoraters fail to detect the attack, highlighting the need for improved bias mitigation strategies in distilled models.",
  "evaluation": "Step 1: Check for factual consistency. Ensure that all claims in the summaries are supported by the source text.\nStep 2: Assess the engagingness. Determine if the summaries are interesting to a general audience.\nStep 3: Compare the summaries to select the best one based on the criteria.\n\nSummary 0 scores:\nFactual Consistency: 3\nEngagingness: 3\n\nSummary 1 scores:\nFactual Consistency: 3\nEngagingness: 3\n\nBest Summary: 1\nOkay, let's start by evaluating the factual consistency of each summary. \n\nFor Summary 0, it mentions the Biased-Roots attack, the two propagation modes (untargeted and targeted), and the amplification of bias in student models. The source text does discuss these points, so that's consistent. However, it says \"the attack's effectiveness across different tasks and data types\" without specifying which tasks or data types. The source text does mention testing on text and code generation, so that's a minor point. Overall, it seems consistent.\n\nSummary 1 states that the attack is called Biased-Roots, the two propagation modes, and the specific percentages (76.9% for targeted and higher for untargeted). The source text does mention these numbers, so that's accurate. It also lists the six biases tested, which the source text does include. The part about existing defenses failing is also correct. So both summaries are",
  "tweet": "\"Biased-Roots attack reveals adversarial bias amplification in distilled models \ud83e\udde0\ud83d\udd12 #AIethics\"",
  "real_tweet": "\"Biased-Roots attack reveals adversarial bias amplification in distilled models \ud83e\udde0\ud83d\udd12 #AIethics\"\n Link: https://arxiv.org/abs/2505.24842",
  "model_used": "Qwen/Qwen3-4B",
  "eval_mode": true
}