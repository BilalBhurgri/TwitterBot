{
  "status": "success",
  "all_summaries": [
    "Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning proposes a two-stage offline training framework called Reinforcement Distillation (REDI) that effectively utilizes both positive and negative distilled reasoning traces to enhance LLM reasoning performance. REDI first performs supervised fine-tuning (SFT) on positive traces, then refines the model using an asymmetrically weighted, reference-free objective that incorporates negative traces. The framework outperforms existing methods on mathematical reasoning tasks, demonstrating improved performance and stability through the strategic use of negative examples, and achieves state-of-the-art results with a 1.5B model post-trained on publicly available data.",
    "This paper proposes Reinforcement Distillation (REDI), a two-stage offline training framework that leverages both positive and negative distilled reasoning traces to improve large language model (LLM) reasoning performance. REDI first performs supervised fine-tuning (SFT) on positive traces to establish a strong reasoning foundation, then uses an asymmetrically weighted, reference-free objective to incorporate negative traces, which act as learning signals. The framework addresses the underutilization of negative examples in rejection sampling, demonstrating superior performance compared to SFT-only and DPO/SimPO methods. Key findings include the effectiveness of REDI in enhancing reasoning performance with a modest dataset, the trade-off between stability and peak performance in preference optimization, and the ability of REDI to maintain or improve pass@16 scores, indicating broader problem-solving capabilities without compromising future online reinforcement learning potential. The Qwen-REDI-1.5B model achieves state-of-the-art results on mathematics benchmarks, showing the efficacy of REDI in distilling complex reasoning abilities from teacher models."
  ],
  "best_summary_idx": 1,
  "summary": "This paper proposes Reinforcement Distillation (REDI), a two-stage offline training framework that leverages both positive and negative distilled reasoning traces to improve large language model (LLM) reasoning performance. REDI first performs supervised fine-tuning (SFT) on positive traces to establish a strong reasoning foundation, then uses an asymmetrically weighted, reference-free objective to incorporate negative traces, which act as learning signals. The framework addresses the underutilization of negative examples in rejection sampling, demonstrating superior performance compared to SFT-only and DPO/SimPO methods. Key findings include the effectiveness of REDI in enhancing reasoning performance with a modest dataset, the trade-off between stability and peak performance in preference optimization, and the ability of REDI to maintain or improve pass@16 scores, indicating broader problem-solving capabilities without compromising future online reinforcement learning potential. The Qwen-REDI-1.5B model achieves state-of-the-art results on mathematics benchmarks, showing the efficacy of REDI in distilling complex reasoning abilities from teacher models.",
  "evaluation": "Step 1: Check for factual consistency by verifying if all claims in the summaries are supported by the source text.\nStep 2: Assess engagingness by evaluating the clarity and interest level of the summaries for a general audience.\nStep 3: Compare the summaries to determine which one is the best based on the criteria.\n\nSummary 0 Score:\nFactual Consistency: 3\nEngagingness: 3\n\nSummary 1 Score:\nFactual Consistency: 3\nEngagingness: 3\n\nBest Summary: 1\nAnswer:\nSummary 0\nSummary 1\nBest Summary: 1\nAnswer:\nSummary 0\nSummary 1\nBest Summary: 1\nAnswer:\nSummary 0\nSummary 1\nBest Summary: 1\nAnswer:\nSummary 0\nSummary 1\nBest Summary: 1\nAnswer:\nSummary 0\nSummary 1\nBest Summary: 1\nAnswer:\nSummary 0\nSummary 1\nBest Summary: 1\nAnswer:\nSummary 0\nSummary 1\nBest Summary: 1\nAnswer:\nSummary 0\nSummary 1\nBest Summary: 1\nAnswer:\nSummary 0\nSummary 1\nBest Summary: 1\nAnswer:\nSummary 0\nSummary 1\nBest Summary: 1\nAnswer:\nSummary 0\nSummary 1\nBest Summary: 1\nAnswer:\nSummary 0\nSummary 1\nBest Summary:",
  "tweet": "\"REDI improves LLM reasoning via two-stage training. \ud83e\udde0\u2728\"\nOkay, let's see. The user wants",
  "real_tweet": "\"REDI improves LLM reasoning via two-stage training. \ud83e\udde0\u2728\"\n Link: https://arxiv.org/abs/2505.24850"
}