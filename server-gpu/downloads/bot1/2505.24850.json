{
  "status": "success",
  "all_summaries": [
    "Harnessing negative signals: Reinforcement Distillation from Teacher Data for LLM Reasoning proposes a two-stage framework called REDI that effectively utilizes both positive and negative distilled reasoning traces to enhance LLM reasoning performance. The framework includes a novel asymmetrically weighted reference-free objective, which balances stability and performance by downweighting negative samples. Empirical results show that REDI outperforms previous methods on mathematical reasoning tasks, achieving state-of-the-art results with a 1.5B model trained on open data.",
    "This paper proposes Reinforcement Distillation (REDI), a two-stage offline training framework that integrates both positive and negative reasoning traces to enhance large language model (LLM) reasoning performance. REDI first performs supervised fine-tuning (SFT) on positive traces, then uses an asymmetrically weighted, reference-free objective to incorporate negative traces, improving stability and performance. Experiments show that REDI outperforms rejection sampling SFT and DPO/SimPO on mathematical reasoning tasks, with the Qwen-REDI-1.5B model achieving state-of-the-art results on MATH-500. REDI effectively leverages previously discarded negative traces, demonstrating data efficiency and effectiveness in distilling reasoning capabilities from teacher models."
  ],
  "best_summary_idx": 1,
  "summary": "This paper proposes Reinforcement Distillation (REDI), a two-stage offline training framework that integrates both positive and negative reasoning traces to enhance large language model (LLM) reasoning performance. REDI first performs supervised fine-tuning (SFT) on positive traces, then uses an asymmetrically weighted, reference-free objective to incorporate negative traces, improving stability and performance. Experiments show that REDI outperforms rejection sampling SFT and DPO/SimPO on mathematical reasoning tasks, with the Qwen-REDI-1.5B model achieving state-of-the-art results on MATH-500. REDI effectively leverages previously discarded negative traces, demonstrating data efficiency and effectiveness in distilling reasoning capabilities from teacher models.",
  "evaluation": "Step 1: Check for factual consistency by verifying if all claims in the summaries are supported by the source text.\nStep 2: Assess engagingness by evaluating the clarity and interest level of the summaries for a general audience.\nStep 3: Compare the summaries to determine which one is the best based on the criteria.\n\nSummary 0 Factual Consistency: 3 (Consistent) - The summary accurately reflects the key points of the paper, including the REDI framework, the two-stage approach, the asymmetric weighting, and the empirical results. All claims are supported by the source text.\n\nSummary 0 Engagingness: 3 (Interesting) - The summary is clear and concise, highlighting the main contributions and results, which are relevant to both specialists and general audiences interested in AI and machine learning.\n\nSummary 1 Factual Consistency: 3 (Consistent) - The summary correctly describes the REDI framework, the two-stage process, the use of negative traces, and the experimental results. All claims are supported by the source text.\n\nSummary 1 Engagingness: 3 (Interesting) - The summary is well-structured and informative, making it accessible to a general audience interested in AI advancements and their applications.\n\nBest Summary: 1\nAnswer:\n3\n3\n1\nAnswer:\n3\n3\n1\nAnswer:\n3\n3\n1\nAnswer:\n3\n3\n1\nAnswer:\n3\n3\n1\nAnswer:\n3\n3",
  "tweet": "\ud83d\ude80 **REDI improves LLM reasoning by integrating positive/negative traces** \u2705\nThat's 10 words",
  "real_tweet": "\ud83d\ude80 **REDI improves LLM reasoning by integrating positive/negative traces** \u2705\n Link: https://arxiv.org/abs/2505.24850"
}