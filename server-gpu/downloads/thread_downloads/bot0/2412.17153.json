{
  "status": "success",
  "paper_id": "2412.17153",
  "bot_num": 0,
  "processed_date": "2025-06-05T21:15:18.741577",
  "all_summaries": [
    "Distilled Decoding (DD) introduces a novel method to distill pre-trained autoregressive (AR) models for fast, few-step sampling, leveraging flow matching to map noisy tokens to generated tokens directly, enabling one-step generation with reduced computational overhead. The method demonstrates significant speedups over AR models, particularly for image generation tasks, with minimal quality degradation, as shown by lower FID scores and improved efficiency compared to baselines. DD achieves one-step generation for image AR models like VAR and LlamaGen, reducing steps from 10 to 1 and 256 to 1, respectively, with substantial improvements in speed and performance. The technique also supports flexible multi-step generation, allowing users to balance quality and speed effectively.",
    "Distilled Decoding (DD) introduces a novel method to distill pre-trained autoregressive (AR) models for fast, few-step sampling, leveraging flow matching to map noisy tokens to generated tokens in one step. By training a model to directly distill the mapping between noise and generated sequences, DD enables one-step generation and significantly reduces sampling times for image and text generation tasks, achieving speedups of up to 217.8\u00d7 for LlamaGen and 6.3\u00d7 for VAR while maintaining acceptable quality metrics like FID. "
  ],
  "best_summary_idx": 1,
  "summary": "Distilled Decoding (DD) introduces a novel method to distill pre-trained autoregressive (AR) models for fast, few-step sampling, leveraging flow matching to map noisy tokens to generated tokens in one step. By training a model to directly distill the mapping between noise and generated sequences, DD enables one-step generation and significantly reduces sampling times for image and text generation tasks, achieving speedups of up to 217.8\u00d7 for LlamaGen and 6.3\u00d7 for VAR while maintaining acceptable quality metrics like FID. ",
  "evaluation": "Step 1: Check for factual consistency. Ensure that all claims in the summaries are supported by the source text.\nStep 2: Assess engagingness. Determine if the summaries are engaging to a general audience.\nStep 3: Compare the summaries to identify the best one based on the criteria.\n\nSummary 0 Score:\nFactual Consistency: 3\nEngagingness: 3\n\nSummary 1 Score:\nFactual Consistency: 3\nEngagingness: 3\n\nBest Summary: 1\nOkay, let's start by evaluating the factual consistency of each summary based on the source text.\n\nFor Summary 0, it mentions that DD reduces steps from 10 to 1 for VAR and 256 to 1 for LlamaGen, which matches the source text. It also states that DD achieves speedups of 6.3\u00d7 and 217.8\u00d7, which are explicitly mentioned. The FID scores and the use of flow matching are all supported by the text. There are no discrepancies here, so Summary 0 is factually consistent.\n\nNow, Summary 1 also correctly states the reduction in steps and the speedups. It mentions the FID scores and the use of flow matching, which are all in line with the source text. The claims about maintaining acceptable quality metrics like FID are also accurate. Therefore, Summary 1 is also factually consistent.\n\nNext, assessing engagingness. Both summaries explain the key points",
  "tweet": "\"\ud83d\ude80 Distilled Decoding (DD) speeds up AR models by 217.8\u00d7 for LlamaGen,",
  "real_tweet": "\"\ud83d\ude80 Distilled Decoding (DD) speeds up AR models by 217.8\u00d7 for LlamaGen,\n Link: https://arxiv.org/abs/2412.17153",
  "model_used": "Qwen/Qwen3-4B",
  "eval_mode": true
}