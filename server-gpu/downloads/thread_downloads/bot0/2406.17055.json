{
  "status": "success",
  "paper_id": "2406.17055",
  "bot_num": 0,
  "processed_date": "2025-06-05T20:58:11.343799",
  "all_summaries": [
    "The paper discusses how large language models (LLMs) assume people are more rational than they actually are. It mentions experiments comparing LLMs' predictions with human behavior in tasks like risky choices and inferring preferences. The key finding is that LLMs, especially when using chain-of-thought prompting, predict human decisions as more rational than they are, leading to inaccuracies. This is problematic because it affects how AI systems interact with humans, as they don't truly model human behavior. The authors also note that humans tend to perceive others as more rational than they are, which might contribute to this issue. They highlight that LLMs' assumptions about rationality are inconsistent with real human decision-making, posing risks for AI safety and effectiveness",
    "The paper investigates how Large Language Models (LLMs) implicitly model human decision-making, revealing that LLMs assume people are more rational than they actually are. Through experiments involving risky choices and preference inference, the study shows that LLMs predict human decisions with high rationality, often outperforming humans in correlation with rational models but underperforming in matching actual human behavior. LLMs also align with human inferences about others' choices, suggesting a shared heuristic-based approach, though they fail to accurately simulate human decision processes. These findings highlight a critical misalignment between LLMs' assumptions and real human behavior, underscoring the need for improved alignment methods to better reflect genuine human decision-making dynamics."
  ],
  "best_summary_idx": 1,
  "summary": "The paper investigates how Large Language Models (LLMs) implicitly model human decision-making, revealing that LLMs assume people are more rational than they actually are. Through experiments involving risky choices and preference inference, the study shows that LLMs predict human decisions with high rationality, often outperforming humans in correlation with rational models but underperforming in matching actual human behavior. LLMs also align with human inferences about others' choices, suggesting a shared heuristic-based approach, though they fail to accurately simulate human decision processes. These findings highlight a critical misalignment between LLMs' assumptions and real human behavior, underscoring the need for improved alignment methods to better reflect genuine human decision-making dynamics.",
  "evaluation": "Step 1: Check for factual consistency by verifying that all claims in the summary are supported by the source text.\nStep 2: Assess engagingness by determining if the summary is accessible and interesting to a general audience.\nStep 3: Compare the summaries to determine which one is the best based on the criteria.\n\nSummary 0 Scores:\nFactual Consistency: 3\nEngagingness: 3\n\nSummary 1 Scores:\nFactual Consistency: 3\nEngagingness: 3\n\nBest Summary: 1\nOkay, let's start by evaluating the factual consistency of each summary. \n\nFor Summary 0, it mentions that LLMs assume people are more rational than they are, which is directly stated in the source text. The experiments on risky choices and preference inference are also covered. The key finding about LLMs predicting rational decisions and the human bias towards rationality are all supported by the text. There are no major errors here, so it's consistent.\n\nSummary 1 also accurately reflects the source text. It discusses the experiments, the high rationality predictions by LLMs, and the comparison with human behavior. The part about aligning with human inferences is correct. Both summaries have all the key points from the source, so they are both factually consistent.\n\nNext, engagingness. Summary 0 is straightforward and explains the main points without jargon, making it accessible. Summary 1 is slightly more detailed, explaining the experiments and the",
  "tweet": "AI models see humans as more rational than they are \ud83e\udde0\ud83e\udd16 #AIResearch\nOkay, let's tackle this query step",
  "real_tweet": "AI models see humans as more rational than they are \ud83e\udde0\ud83e\udd16 #AIResearch\n Link: https://arxiv.org/abs/2406.17055",
  "model_used": "Qwen/Qwen3-4B",
  "eval_mode": true
}