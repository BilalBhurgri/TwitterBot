{
  "status": "success",
  "paper_id": "2410.23123",
  "bot_num": 0,
  "processed_date": "2025-06-05T21:37:04.598385",
  "all_summaries": [
    "This paper explores the memorization behavior of large language models in logical reasoning tasks, proposing a metric LiMem to quantify memorization by measuring inconsistency under local perturbations. The authors demonstrate that while LLMs can memorize logical puzzles, they also develop reasoning capabilities through fine-tuning, with models like GPT4o-mini and Llama3-8B achieving high accuracy on K&amp;K puzzles after training. The study reveals that memorization and reasoning coexist, with models improving generalization as they memorize more puzzles, and that fine-tuning with answers alone can lead to strong reasoning performance, highlighting the complex interplay between memorization and logical reasoning in LLMs.",
    "Large language models (LLMs) exhibit memorization in logical reasoning tasks, as demonstrated by their high accuracy on puzzles but poor performance under perturbations, with models like GPT4o-mini and Llama3-8B showing increased memorization scores when trained on more puzzles, although they also develop reasoning abilities through fine-tuning, and the study introduces a memorization metric (LiMem) to quantify this behavior, highlighting the interplay between memorization and reasoning in LLMs. "
  ],
  "best_summary_idx": -1,
  "summary": "Large language models (LLMs) exhibit memorization in logical reasoning tasks, as demonstrated by their high accuracy on puzzles but poor performance under perturbations, with models like GPT4o-mini and Llama3-8B showing increased memorization scores when trained on more puzzles, although they also develop reasoning abilities through fine-tuning, and the study introduces a memorization metric (LiMem) to quantify this behavior, highlighting the interplay between memorization and reasoning in LLMs. ",
  "evaluation": "1. Check for factual consistency: Are the summaries accurate and based on the source text?\n2. Assess engagingness: Are the summaries engaging to a general audience?\n3. Determine the best summary: Which summary is more accurate and engaging?\n\nSummary 0:\nThis paper explores the memorization behavior of large language models in logical reasoning tasks, proposing a metric LiMem to quantify memorization by measuring inconsistency under local perturbations. The authors demonstrate that while LLMs can memorize logical puzzles, they also develop reasoning capabilities through fine-tuning, with models like GPT4o-mini and Llama3-8B achieving high accuracy on K&amp;K puzzles after training. The study reveals that memorization and reasoning coexist, with models improving generalization as they memorize more puzzles, and that fine-tuning with answers alone can lead to strong reasoning performance, highlighting the complex interplay between memorization and logical reasoning in LLMs.\n\nSummary 1:\nLarge language models (LLMs) exhibit memorization in logical reasoning tasks, as demonstrated by their high accuracy on puzzles but poor performance under perturbations, with models like GPT4o-mini and Llama3-8B showing increased memorization scores when trained on more puzzles, although they also develop reasoning abilities through fine-tuning, and the study introduces a memorization metric (LiMem) to quantify this behavior, highlighting the interplay between memorization and reasoning in LLMs.\n\nSummary 0: 3",
  "tweet": "\ud83e\udde0 In #LLM research, models show memorization in logic tasks, yet gain reasoning via fine-tuning. Li",
  "real_tweet": "\ud83e\udde0 In #LLM research, models show memorization in logic tasks, yet gain reasoning via fine-tuning. Li\n Link: https://arxiv.org/abs/2410.23123",
  "model_used": "Qwen/Qwen3-4B",
  "eval_mode": true
}