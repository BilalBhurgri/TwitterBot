{
  "status": "success",
  "paper_id": "2504.15362",
  "bot_num": 0,
  "processed_date": "2025-06-05T21:01:38.862417",
  "all_summaries": [
    "This paper proposes a three-stage data synthesis framework called LongPerceptualThoughts to generate long chain-of-thought (CoT) data for vision-centric tasks. The framework generates verifiable multiple-choice questions from dense image captions, extracts simple CoTs from vision-language models (VLMs), and expands these into long-form reasoning traces using frontier reasoning models. By fine-tuning a strong instruction-tuned VLM on LongPerceptualThoughts, the model achieves significant improvements on vision-centric benchmarks, including a +11.8 point gain on V * Bench, and also improves on the text reasoning benchmark MMLU-Pro by +2 points. The framework emphasizes the importance of incorporating cognitive behaviors like verification, backtracking, and subgoal setting into CoTs to enhance reasoning capabilities in vision tasks.",
    "This paper proposes a novel three-stage data synthesis framework called LongPerceptualThoughts to generate long chain-of-thought (CoT) data for vision-centric tasks. The framework generates verifiable multiple-choice questions from dense image captions, extracts simple CoTs from VLMs, and expands them into richer, long-form reasoning traces using frontier reasoning models. The resulting dataset is used to fine-tune a vision-language model, achieving an average +3.4 point improvement on five vision-centric benchmarks, including a +11.8 point gain on V * Bench, and a +2 point improvement on the text reasoning benchmark MMLU-Pro."
  ],
  "best_summary_idx": 1,
  "summary": "This paper proposes a novel three-stage data synthesis framework called LongPerceptualThoughts to generate long chain-of-thought (CoT) data for vision-centric tasks. The framework generates verifiable multiple-choice questions from dense image captions, extracts simple CoTs from VLMs, and expands them into richer, long-form reasoning traces using frontier reasoning models. The resulting dataset is used to fine-tune a vision-language model, achieving an average +3.4 point improvement on five vision-centric benchmarks, including a +11.8 point gain on V * Bench, and a +2 point improvement on the text reasoning benchmark MMLU-Pro.",
  "evaluation": "Step 1: Check for factual consistency by verifying if all claims in the summary are supported by the source text.\nStep 2: Assess engagingness by determining if the summary is accessible and interesting to a general audience.\nStep 3: Compare the summaries to determine which one is the best based on the criteria.\n\nFactual Consistency Scores:\nSummary 0: 3\nSummary 1: 3\n\nEngagingness Scores:\nSummary 0: 3\nSummary 1: 3\n\nBest Summary:\n1\nOkay, let's go through the evaluation steps as outlined.\n\n**Step 1: Check for factual consistency**  \nBoth summaries accurately describe the three-stage framework of LongPerceptualThoughts, the process of generating CoT data, and the results achieved on benchmarks like V * Bench and MMLU-Pro. The summaries correctly mention the key components: generating multiple-choice questions from dense captions, extracting simple CoTs from VLMs, and expanding them using frontier models. The scores for factual consistency are both 3, indicating that all claims are supported by the source text.\n\n**Step 2: Assess engagingness**  \nBoth summaries are written in a clear and concise manner, avoiding overly technical jargon. They highlight the significance of the framework and its improvements on both vision and text benchmarks, which would be interesting to a general audience. The scores for engagingness are both 3, as both summaries are accessible and engaging.\n\n**Step 3: Compare",
  "tweet": "\"\ud83d\ude80 New study introduces LongPerceptualThoughts framework for generating long-chain CoT data in vision tasks. Dataset boosts",
  "real_tweet": "\"\ud83d\ude80 New study introduces LongPerceptualThoughts framework for generating long-chain CoT data in vision tasks. Dataset boosts\n Link: https://arxiv.org/abs/2504.15362",
  "model_used": "Qwen/Qwen3-4B",
  "eval_mode": true
}