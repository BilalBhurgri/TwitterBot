{
  "status": "success",
  "paper_id": "2505.21444",
  "bot_num": 0,
  "processed_date": "2025-06-05T21:58:21.672709",
  "all_summaries": [
    "The paper investigates self-improvement methods for large language models, focusing on self-training without external labels. It introduces Self-Rewarded Training (SRT), which uses model self-consistency as a reward signal to enable continual improvement. SRT achieves performance comparable to methods with ground-truth labels, but risks reward hacking, where models prioritize consistency over correctness. The study identifies this risk and proposes strategies like early stopping and curriculum learning to mitigate it, showing that SRT can sustain performance improvements when applied appropriately.",
    "The paper investigates self-training methods for large language models, specifically focusing on Self-Rewarded Training (SRT) to enhance mathematical reasoning without external labels. It introduces SRT, which uses model self-consistency as a supervisory signal, showing that it can match performance of standard reinforcement learning methods on ground-truth data. The study highlights challenges like reward hacking, where models prioritize consistency over correctness, leading to performance degradation. The paper proposes mitigation strategies such as early stopping and curriculum learning to sustain self-improvement and addresses the importance of balancing self-sufficiency with effective feedback mechanisms. Key findings include the effectiveness of SRT in improving model performance and the necessity of careful training strategies to avoid collapse."
  ],
  "best_summary_idx": 1,
  "summary": "The paper investigates self-training methods for large language models, specifically focusing on Self-Rewarded Training (SRT) to enhance mathematical reasoning without external labels. It introduces SRT, which uses model self-consistency as a supervisory signal, showing that it can match performance of standard reinforcement learning methods on ground-truth data. The study highlights challenges like reward hacking, where models prioritize consistency over correctness, leading to performance degradation. The paper proposes mitigation strategies such as early stopping and curriculum learning to sustain self-improvement and addresses the importance of balancing self-sufficiency with effective feedback mechanisms. Key findings include the effectiveness of SRT in improving model performance and the necessity of careful training strategies to avoid collapse.",
  "evaluation": "Step 1: Check for factual consistency. Ensure that all claims in the summaries are supported by the source text.\nStep 2: Assess engagingness. Determine if the summaries are engaging to a general audience.\nStep 3: Compare the summaries to identify the best one based on the criteria.\n\nFactual Consistency Scores:\nSummary 0: 3\nSummary 1: 3\n\nEngagingness Scores:\nSummary 0: 2\nSummary 1: 3\n\nBest Summary:\n1\nOkay, let's go through the evaluation steps as instructed.\n\nStep 1: Check for factual consistency. Both summaries mention the key points from the source text: the introduction of SRT, its use of self-consistency, performance comparison with ground-truth methods, the risk of reward hacking, and the mitigation strategies like early stopping and curriculum learning. They don't introduce any unsupported claims or major errors. So both summaries are factually consistent.\n\nStep 2: Assess engagingness. Summary 0 is more technical and uses terms like \"self-training,\" \"reinforcement learning,\" and \"reward hacking,\" which might be more engaging for those familiar with the field. Summary 1 is slightly more accessible, explaining the concepts in a way that might be more engaging for a general audience. However, both summaries are somewhat technical, so they might not be the most engaging for someone without prior knowledge.\n\nStep 3: Compare the summaries. Summary 1 provides a bit more clarity and",
  "tweet": "\"\ud83d\ude80 Self-training for LLMs: SRT boosts math reasoning but risks reward hacking. \ud83d\udee0\ufe0f Mitigation via",
  "real_tweet": "\"\ud83d\ude80 Self-training for LLMs: SRT boosts math reasoning but risks reward hacking. \ud83d\udee0\ufe0f Mitigation via\n Link: https://arxiv.org/abs/2505.21444",
  "model_used": "Qwen/Qwen3-4B",
  "eval_mode": true
}