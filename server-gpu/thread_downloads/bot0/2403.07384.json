{
  "status": "success",
  "paper_id": "2403.07384",
  "bot_num": 0,
  "processed_date": "2025-06-05T21:22:44.140404",
  "all_summaries": [
    "SmallTolarg is a scalable data selection method for fine-tuning large language models (LLMs) in specialized domains. It uses loss trajectories from small models to cluster and select training examples, reducing data requirements while maintaining performance. The method was validated on mathematical reasoning and clinical text summarization tasks, achieving significant improvements in data efficiency and performance compared to existing methods.  ",
    "The paper proposes SMALLTOLARG, a scalable data selection method that uses loss trajectories from small models to cluster and select training data for fine-tuning large language models in specialized domains, achieving significant improvements in data efficiency and performance by reducing training data size while maintaining or surpassing full dataset performance across mathematical reasoning and clinical text summarization tasks. It demonstrates that examples with similar loss trajectories on a small model are likely to behave similarly on a large model, enabling efficient data selection with minimal computational overhead and showing robustness across different clustering parameters and small proxy models. The method outperforms existing data selection techniques, particularly in scenarios with limited data budgets, and provides theoretical support through convergence rate analysis, making it effective for specialized domain fine-tuning."
  ],
  "best_summary_idx": 1,
  "summary": "The paper proposes SMALLTOLARG, a scalable data selection method that uses loss trajectories from small models to cluster and select training data for fine-tuning large language models in specialized domains, achieving significant improvements in data efficiency and performance by reducing training data size while maintaining or surpassing full dataset performance across mathematical reasoning and clinical text summarization tasks. It demonstrates that examples with similar loss trajectories on a small model are likely to behave similarly on a large model, enabling efficient data selection with minimal computational overhead and showing robustness across different clustering parameters and small proxy models. The method outperforms existing data selection techniques, particularly in scenarios with limited data budgets, and provides theoretical support through convergence rate analysis, making it effective for specialized domain fine-tuning.",
  "evaluation": "Step 1: Check for factual consistency by verifying if all claims in the summaries are supported by the source text.\nStep 2: Assess engagingness by evaluating how well the summaries capture the novelty and significance of the research.\nStep 3: Compare the summaries to determine which one is the most accurate and comprehensive representation of the paper's contributions.\n\nFactual Consistency Scores:\nSummary 0: 3\nSummary 1: 3\n\nEngagingness Scores:\nSummary 0: 2\nSummary 1: 3\n\nBest Summary:\n1\nOkay, let's go through the evaluation steps as outlined.\n\n**Step 1: Check for factual consistency**  \nBoth summaries are factually consistent with the source text. Summary 0 mentions that SMALLTOLARG uses loss trajectories from small models to cluster and select training examples, which is directly supported by the source text. Summary 1 adds that examples with similar loss trajectories on a small model behave similarly on a large model, which is also mentioned in the source text. Both summaries correctly state that the method was validated on mathematical reasoning and clinical text summarization tasks, and that it outperforms existing methods. There are no major errors in either summary.\n\n**Step 2: Assess engagingness**  \nSummary 0 is somewhat engaging, as it provides a clear overview of the method and its results but doesn't emphasize the novelty or broader implications of the research. Summary 1, on the other hand, highlights the significance of",
  "tweet": "\"\ud83d\ude80 SMALLTOLARG enhances LLM fine-tuning efficiency \ud83e\udde0\ud83d\udcc8\"\n \n\n\"\ud83d\ude80 SMALLTOLARG boosts",
  "real_tweet": "\"\ud83d\ude80 SMALLTOLARG enhances LLM fine-tuning efficiency \ud83e\udde0\ud83d\udcc8\"\n Link: https://arxiv.org/abs/2403.07384",
  "model_used": "Qwen/Qwen3-4B",
  "eval_mode": true
}