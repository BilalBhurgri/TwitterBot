{
  "status": "success",
  "paper_id": "2404.19733",
  "bot_num": 0,
  "processed_date": "2025-06-05T22:07:19.986814",
  "all_summaries": [
    "This paper proposes an iterative reasoning preference optimization method called Iterative RPO, which enhances reasoning abilities of large language models by iteratively refining preference pairs through multiple rounds of training. The method generates chain-of-thought reasoning steps and final answers for training prompts, constructs preference pairs based on correctness of answers, and incorporates a negative log-likelihood loss term alongside DPO to improve performance. Experiments show that Iterative RPO achieves significant improvements on three benchmark tasks: from 55.6% to 81.6% on GSM8K, from 77.8% to 86.7% on ARC-Challenge, and from 12.5% to 20.8% on MATH, outperforming baselines like SFT and DPO. Key findings include the necessity of NLL loss for effective training and the importance of iterative refinement in achieving high reasoning performance.",
    "This paper proposes an iterative reasoning preference optimization method called Iterative RPO, which iteratively generates chain-of-thought (CoT) reasoning steps and final answers, constructs preference pairs based on correctness, and trains a variant of DPO with a negative log-likelihood (NLL) loss term. The method achieves significant improvements on reasoning tasks like GSM8K, ARC-Challenge, and MATH, surpassing baselines such as zero-shot CoT, SFT, and standard DPO, with performance reaching 81.6% on GSM8K, 86.7% on ARC-Challenge, and 20.8% on MATH after multiple iterations. Key factors contributing to the success include the inclusion of rejected examples in training, the NLL loss term, and the iterative refinement of the model through multiple rounds of preference optimization."
  ],
  "best_summary_idx": 1,
  "summary": "This paper proposes an iterative reasoning preference optimization method called Iterative RPO, which iteratively generates chain-of-thought (CoT) reasoning steps and final answers, constructs preference pairs based on correctness, and trains a variant of DPO with a negative log-likelihood (NLL) loss term. The method achieves significant improvements on reasoning tasks like GSM8K, ARC-Challenge, and MATH, surpassing baselines such as zero-shot CoT, SFT, and standard DPO, with performance reaching 81.6% on GSM8K, 86.7% on ARC-Challenge, and 20.8% on MATH after multiple iterations. Key factors contributing to the success include the inclusion of rejected examples in training, the NLL loss term, and the iterative refinement of the model through multiple rounds of preference optimization.",
  "evaluation": "Step 1: Check for factual consistency by verifying that all claims in the summaries are supported by the source text.\nStep 2: Assess engagingness by evaluating how well the summaries capture the novelty and significance of the research.\nStep 3: Compare the summaries to determine which one best summarizes the paper's contributions and results.\n\nFactual Consistency Scores:\nSummary 0: 3\nSummary 1: 3\n\nEngagingness Scores:\nSummary 0: 3\nSummary 1: 3\n\nBest Summary:\n1\nOkay, let's start by evaluating the factual consistency of both summaries. \n\nFor Summary 0, it mentions that Iterative RPO improves performance on GSM8K from 55.6% to 81.6%, which matches the source text. It also states that the method uses a negative log-likelihood (NLL) loss term alongside DPO, which is explicitly mentioned in the source. The key finding about the necessity of NLL loss is also supported. The summary correctly notes that the method outperforms SFT and DPO, which aligns with the experiments described. There are no major errors here, so it's consistent.\n\nSummary 1 also accurately reflects the source. It states the same performance improvements on the three tasks, the use of NLL loss, and the outperformance of baselines. It mentions the inclusion of rejected examples and the iterative refinement, which are both supported by the text. The details about",
  "tweet": "\"\ud83d\ude80 New method for AI reasoning: Iterative RPO boosts accuracy on GSM8K, ARC-Challenge, and MATH",
  "real_tweet": "\"\ud83d\ude80 New method for AI reasoning: Iterative RPO boosts accuracy on GSM8K, ARC-Challenge, and MATH\n Link: https://arxiv.org/abs/2404.19733",
  "model_used": "Qwen/Qwen3-4B",
  "eval_mode": true
}