{
  "status": "success",
  "paper_id": "2502.06453",
  "bot_num": 0,
  "processed_date": "2025-06-05T21:59:35.548278",
  "all_summaries": [
    "This paper introduces MATH-Perturb, a benchmark designed to assess the robustness of large language models (LLMs) in mathematical reasoning under hard perturbations. It presents MATH-P-Simple and MATH-P-Hard, two subsets of perturbed problems derived from level-5 MATH questions. The benchmark evaluates how well LLMs perform on these problems, revealing significant performance drops on MATH-P-Hard, indicating vulnerability to fundamental shifts in problem formulations. Key findings include models' susceptibility to memorization bias, failure to adapt to new reasoning patterns, and the negative impact of in-context learning with original examples. The study highlights the importance of robustness against hard perturbations for future LLM development.",
    "This paper introduces the MATH-Perturb benchmark, which evaluates LLMs' ability to reason about math problems under hard perturbations. It presents two variants: MATH-P-Simple and MATH-P-Hard. The benchmark consists of 279 problems derived from the hardest level-5 problems of the MATH dataset. Key findings include that all 18 evaluated LLMs show significant performance drops on MATH-P-Hard, indicating bias toward original reasoning patterns. Additionally, the study reveals a new form of memorization where models blindly apply learned techniques without adapting to changed problem settings, which is exacerbated by in-context learning with original examples. The research highlights the importance of robustness against hard perturbations for future LLMs."
  ],
  "best_summary_idx": 1,
  "summary": "This paper introduces the MATH-Perturb benchmark, which evaluates LLMs' ability to reason about math problems under hard perturbations. It presents two variants: MATH-P-Simple and MATH-P-Hard. The benchmark consists of 279 problems derived from the hardest level-5 problems of the MATH dataset. Key findings include that all 18 evaluated LLMs show significant performance drops on MATH-P-Hard, indicating bias toward original reasoning patterns. Additionally, the study reveals a new form of memorization where models blindly apply learned techniques without adapting to changed problem settings, which is exacerbated by in-context learning with original examples. The research highlights the importance of robustness against hard perturbations for future LLMs.",
  "evaluation": "Step 1: Check for factual consistency by verifying if all claims in the summary are supported by the source text.\nStep 2: Assess engagingness by determining if the summary is accessible and interesting to a general audience.\nStep 3: Compare the summaries to determine which one is the best based on the criteria.\n\nSummary 0 Factual Consistency: 3 (All facts supported)\nSummary 0 Engagingness: 3 (Interesting)\nSummary 1 Factual Consistency: 3 (All facts supported)\nSummary 1 Engagingness: 3 (Interesting)\nBest Summary: 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n``",
  "tweet": "Drum roll \ud83e\udd41 The MATH-Perturb benchmark exposes LLMs\u2019 vulnerability to hard perturbations and memor",
  "real_tweet": "Drum roll \ud83e\udd41 The MATH-Perturb benchmark exposes LLMs\u2019 vulnerability to hard perturbations and memor\n Link: https://arxiv.org/abs/2502.06453",
  "model_used": "Qwen/Qwen3-4B",
  "eval_mode": true
}