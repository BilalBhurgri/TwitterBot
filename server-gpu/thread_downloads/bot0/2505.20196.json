{
  "status": "success",
  "paper_id": "2505.20196",
  "bot_num": 0,
  "processed_date": "2025-06-05T22:06:09.826090",
  "all_summaries": [
    "Temporal Forgetting refers to the phenomenon where large language models (LLMs) forget correct answers they previously knew during fine-tuning, as quantified by the Temporal Forgetting Score (PTFS), which shows 6.4\u201356.1% of final errors were once correct at earlier checkpoints. This indicates that final-checkpoint-only evaluation is limited, as many correct solutions are transient. Temporal Sampling, which samples across multiple checkpoints, recovers forgotten solutions without retraining, improving reasoning performance on benchmarks like AIME2024, AMC, and AIME2025 by 4\u201319 points in Pass@k and enhancing Majority@k and Best-of-N. LoRA adaptation enables efficient Temporal Sampling with reduced storage, demonstrating that model competence lies in training dynamics, not a single checkpoint.",
    "Temporal Forgetting refers to the phenomenon where large language models forget previously correct reasoning solutions during fine-tuning, leading to significant errors in benchmarks. The study introduces the Temporal Forgetting Score (PTFS) to quantify this issue, showing that 6.4% to 56.1% of final errors were once correctly solved at earlier checkpoints. Temporal Sampling, which samples from multiple checkpoints during inference, recovers forgotten solutions without retraining, improving performance on reasoning tasks like AIME2024, AMC, and AIME2025 by 4-19 points in Pass@k. LoRA adaptation enables efficient Temporal Sampling with reduced storage needs, demonstrating that model competence lies in training dynamics rather than a single checkpoint. The findings challenge the reliance on final checkpoints and highlight the importance of considering temporal dynamics in model evaluation and deployment."
  ],
  "best_summary_idx": 1,
  "summary": "Temporal Forgetting refers to the phenomenon where large language models forget previously correct reasoning solutions during fine-tuning, leading to significant errors in benchmarks. The study introduces the Temporal Forgetting Score (PTFS) to quantify this issue, showing that 6.4% to 56.1% of final errors were once correctly solved at earlier checkpoints. Temporal Sampling, which samples from multiple checkpoints during inference, recovers forgotten solutions without retraining, improving performance on reasoning tasks like AIME2024, AMC, and AIME2025 by 4-19 points in Pass@k. LoRA adaptation enables efficient Temporal Sampling with reduced storage needs, demonstrating that model competence lies in training dynamics rather than a single checkpoint. The findings challenge the reliance on final checkpoints and highlight the importance of considering temporal dynamics in model evaluation and deployment.",
  "evaluation": "Step 1: Check for factual consistency by verifying if all claims in the summary are supported by the source text.\nStep 2: Assess engagingness by evaluating the summary's ability to capture the essence of the research and its relevance to a general audience.\nStep 3: Compare the summaries to determine which one is the most accurate and engaging.\n\nSummary 0 Factual Consistency: 3 (Consistent)\nSummary 1 Factual Consistency: 3 (Consistent)\n\nSummary 0 Engagingness: 3 (Interesting)\nSummary 1 Engagingness: 3 (Interesting)\n\nBest Summary: 1:\nSummary 0 Factual Consistency: 3\nSummary 1 Factual Consistency: 3\nSummary 0 Engagingness: 3\nSummary 1 Engagingness: 3\nBest Summary: 1::\nSummary 0 Factual Consistency: 3\nSummary 1 Factual Consistency: 3\nSummary 0 Engagingness: 3\nSummary 1 Engagingness: 3\nBest Summary: 1::\nSummary 0 Factual Consistency: 3\nSummary 1 Factual Consistency: 3\nSummary 0 Engagingness: 3\nSummary 1 Engagingness: 3\nBest Summary: 1::\nSummary 0 Factual Consistency: 3\nSummary 1 F",
  "tweet": "\"Temporal forgetting impacts LLMs, with PTFS revealing 6.4\u201356.1% errors from prior checkpoints",
  "real_tweet": "\"Temporal forgetting impacts LLMs, with PTFS revealing 6.4\u201356.1% errors from prior checkpoints\n Link: https://arxiv.org/abs/2505.20196",
  "model_used": "Qwen/Qwen3-4B",
  "eval_mode": true
}