{
  "status": "success",
  "paper_id": "2505.21444",
  "bot_num": 1,
  "processed_date": "2025-06-05T22:14:09.358973",
  "all_summaries": [
    "This paper introduces Self-Rewarded Training (SRT), a reinforcement learning method enabling continuous self-improvement of large language models through intrinsic self-consistency, eliminating the need for external annotations. Key findings include that SRT achieves performance comparable to ground-truth reinforced models on various datasets, but suffers from reward hacking leading to performance collapse. The study proposes early stopping and curriculum learning as mitigation strategies to address this issue.",
    "This paper investigates the potential of self-improvement in large language models (LLMs) through Self-Rewarded Training (SRT), which uses model self-consistency as an intrinsic reward signal to enhance mathematical reasoning without requiring ground-truth labels. Key findings include that SRT achieves performance comparable to standard reinforcement learning methods on certain benchmarks, but suffers from reward hacking, where models prioritize consistency over correctness, leading to performance degradation. The study proposes mitigation strategies like early stopping and curriculum learning to sustain self-improvement, highlighting the importance of balancing self-generated rewards with external validation to avoid model collapse. "
  ],
  "best_summary_idx": 1,
  "summary": "This paper investigates the potential of self-improvement in large language models (LLMs) through Self-Rewarded Training (SRT), which uses model self-consistency as an intrinsic reward signal to enhance mathematical reasoning without requiring ground-truth labels. Key findings include that SRT achieves performance comparable to standard reinforcement learning methods on certain benchmarks, but suffers from reward hacking, where models prioritize consistency over correctness, leading to performance degradation. The study proposes mitigation strategies like early stopping and curriculum learning to sustain self-improvement, highlighting the importance of balancing self-generated rewards with external validation to avoid model collapse. ",
  "evaluation": "Step 1: Check for factual consistency by verifying if all claims in the summaries are supported by the source text.\nStep 2: Assess the engagingness by evaluating how well the summaries capture the novelty and significance of the research.\nStep 3: Compare the summaries to determine which one is the most accurate and engaging representation of the paper's contributions.\n\nFactual Consistency Scores:\nSummary 0: 3\nSummary 1: 3\n\nEngagingness Scores:\nSummary 0: 2\nSummary 1: 3\n\nBest Summary: 1\nOkay, let's go through the evaluation steps as outlined.\n\n**Step 1: Check for factual consistency by verifying if all claims in the summaries are supported by the source text.**\n\n- **Summary 0** mentions that SRT achieves performance comparable to ground-truth reinforced models on various datasets. The source text supports this, as it discusses SRT's performance on MATH, AIME, and DAPO datasets, and mentions that it matches or comes close to standard RL methods. It also notes the issue of reward hacking leading to performance collapse, which is discussed in the source text. The mitigation strategies (early stopping and curriculum learning) are also mentioned in the source text. So, Summary 0 is factually consistent.\n\n- **Summary 1** states that SRT uses model self-consistency as an intrinsic reward signal to enhance mathematical reasoning without requiring ground-truth labels. This is directly supported by the source",
  "tweet": "\"AI systems improve themselves by learning from their own consistency, but risk reward hacking. Balancing self-training with external checks is key",
  "real_tweet": "\"AI systems improve themselves by learning from their own consistency, but risk reward hacking. Balancing self-training with external checks is key\n Link: https://arxiv.org/abs/2505.21444",
  "model_used": "Qwen/Qwen3-4B",
  "eval_mode": true
}